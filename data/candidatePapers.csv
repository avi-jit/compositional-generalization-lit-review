"id","score","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount","url"
"4c430e6c3a72626bd4cb1893960c7c26dfec6c79",5,"Structurally Diverse Sampling Reduces Spurious Correlations in Semantic Parsing Datasets","This work proposes a novel algorithm for sampling a structurally diverse set of instances from a labeled instance pool with structured outputs that leads to better generalization and uses information theory to show that reduction in spurious correlations between substructures may be one reason why diverse training sets improve generalization.","ArXiv",2022,"Shivanshu Gupta,Sameer Singh,Matt Gardner",2,44,0,"https://www.semanticscholar.org/paper/4c430e6c3a72626bd4cb1893960c7c26dfec6c79"
"1bd799cf462f926041dd2fc8fbe4af54bddbf5c5",5,"Translate First Reorder Later: Leveraging Monotonicity in Semantic Parsing","By means of the monotonic translations, TP OL can learn reliable lexico-logical patterns from aligned data, improving compositional generalization both over conventional seq2seq models, as well as over a recently proposed approach that exploits gold alignments.","ArXiv",2022,"Francesco Cazzaro,Davide Locatelli,A. Quattoni,X. Carreras",0,43,0,"https://www.semanticscholar.org/paper/1bd799cf462f926041dd2fc8fbe4af54bddbf5c5"
"fcf25e1affc2f8ee5bb49d156f174e9769234deb",5,"Systematic Generalization with Edge Transformers","The Edge Transformer is a new model that combines inspiration from Transformers and rulebased symbolic AI that outperforms Relation-aware, Universal and classical Transformer baselines on compositional generalization benchmarks in relational reasoning, semantic parsing, and dependency parsing.","NeurIPS",2021,"Leon Bergen,T. O’Donnell,Dzmitry Bahdanau",3,46,0,"https://www.semanticscholar.org/paper/fcf25e1affc2f8ee5bb49d156f174e9769234deb"
"b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347",4,"Sequence-to-Sequence Learning with Latent Neural Grammars","This work develops a neural parameterization of the grammar which enables parameter sharing over the combinatorial space of derivation rules without the need for manual feature engineering, and applies it to a diagnostic language navigation task and to small-scale machine translation.","NeurIPS",2021,"Yoon Kim",11,150,3,"https://www.semanticscholar.org/paper/b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347"
"b1f33e956e36bf25e118c0d537dcc519cfe52e60",4,"CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations","CTL++ is introduced, a new diagnostic dataset based on compositions of unary symbolic functions designed to test systematicity of NNs, that is, their capability to generalize to unseen compositions of known functions.","ArXiv",2022,"R'obert Csord'as,Kazuki Irie,J. Schmidhuber",0,25,0,"https://www.semanticscholar.org/paper/b1f33e956e36bf25e118c0d537dcc519cfe52e60"
"676fa805bd715591f99bb17e36d673a6a14e92fe",4,"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization","This work investigates automatic generation of synthetic utterance-program pairs for improving compositional generalization in semantic parsing and selects a subset of synthetic examples that are structurally-diverse and uses them to improve compositionalgeneralization.","EMNLP",2021,"I. Oren,Jonathan Herzig,Jonathan Berant",10,47,3,"https://www.semanticscholar.org/paper/676fa805bd715591f99bb17e36d673a6a14e92fe"
"c735740b26ceaa4db9d77233116434c0e8b311d8",4,"Learning Adaptive Control Flow in Transformers for Improved Systematic Generalization","The novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the compositional table lookup task, and its attention and gating patterns tend to be interpretable as an intuitive form of neural routing.","",2021,"R. Csordás,Kazuki Irie,J. Schmidhuber",0,36,0,"https://www.semanticscholar.org/paper/c735740b26ceaa4db9d77233116434c0e8b311d8"
"61d56ece2d19f4bfeb322c92085fb28521e169da",4,"Neural-Symbolic Recursive Machine for Systematic Generalization","The proposed Neural-Symbolic Recursive Machine (NSR) demonstrates stronger generalization than pure neural networks due to its symbolic representation and inductive biases, and demonstrates better transferability than existing neural-symbolic approaches due to less domain-speciﬁc knowledge required.","ArXiv",2022,"Qing Li,Yixin Zhu,Yitao Liang,Y. Wu,Song-Chun Zhu,Siyuan Huang",0,48,0,"https://www.semanticscholar.org/paper/61d56ece2d19f4bfeb322c92085fb28521e169da"
"b49ebf36a29cf9734313066129ab0d7092d4041e",4,"Categorizing Semantic Representations for Neural Machine Translation","The main idea is to enhance generalization by reducing sparsity and overfitting, which is achieved by finding prototypes of token representations over the training set and integrating their embeddings into the source encoding.","COLING",2022,"Yongjing Yin,Yafu Li,Fandong Meng,Jie Zhou,Yue Zhang",0,56,0,"https://www.semanticscholar.org/paper/b49ebf36a29cf9734313066129ab0d7092d4041e"
"da09949d0c89aca711de0f00e84138c62df623e1",3,"FROM SCAN TO REAL DATA: SYSTEMATIC GENER-","This paper revisits systematic generalization from the perspective of meaningful learning, an exceptional capability of humans to learn new concepts by connecting them with other previously known knowledge, and proposes to augment a training dataset in either an inductive or deductive manner to build semantic links between new and old concepts.","",2021,"Ning Shi,Boxin Wang,Wei Wang,Xiangyu Liu,Rong Zhang,Hui Xue,Xinbing Wang,Zhouhan Lin",1,49,0,"https://www.semanticscholar.org/paper/da09949d0c89aca711de0f00e84138c62df623e1"
"a406701b5fb05be55244d4f940db7be55fce85c6",3,"Semantic Systematicity in Connectionist Language Production","A novel connectionist model of sentence production that employs rich situation model representations originally proposed for modeling systematicity in comprehension, which provides a sufficient structure from which the neural network can interpret novel inputs.","Inf.",2021,"Jesús Calvillo,Harm Brouwer,M. Crocker",0,59,0,"https://www.semanticscholar.org/paper/a406701b5fb05be55244d4f940db7be55fce85c6"
"eaa88d697f92739f3569564329e9d037aabbe2d7",3,"A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics","Models show a gap toward human-level generalization when tested with new concepts in a few-shot setting, and the results suggest that current models still struggle in extrapolation to long-range syntactic dependency and semantics.","",2021,"Qing Li,Siyuan Huang,Yining Hong,Yixin Zhu,Y. Wu,Song-Chun Zhu",1,105,1,"https://www.semanticscholar.org/paper/eaa88d697f92739f3569564329e9d037aabbe2d7"
"d129841cb2e30e25000dcd9edb83c880fc4babc1",3,"Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention","This work develops a novel modification to the transformer by implementing two separate input streams: a role stream controls the attention distributions at each layer, and a filler stream determines the values.","NAACL",2022,"Ayush K Chakravarthy,Jacob Russin,R. O’Reilly",0,35,0,"https://www.semanticscholar.org/paper/d129841cb2e30e25000dcd9edb83c880fc4babc1"
"69df5b68fbf492341336b39b4cc9fcc74fff4d5f",3,"Improving Systematic Generalization Through Modularity and Augmentation","This work investigates how two well-known modeling principles— modularity and data augmentation—affect systematic generalization of neural networks in grounded language learning and analyzes how large the vocabulary needs to be to achieve system- atic generalization and how similar the augmented data needs toBe to the problem at hand.","ArXiv",2022,"Laura Ruis,B. Lake",1,55,0,"https://www.semanticscholar.org/paper/69df5b68fbf492341336b39b4cc9fcc74fff4d5f"
"66f3f0e8ebc780e570770986f50bf9cb9cd53ec1",3,"WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series Tasks","WOODS: eight challenging open-source time series benchmarks covering a diverse range of data modalities, such as videos, brain recordings, and sensor signals is presented, underscoring the new challenges posed by time series tasks.","ArXiv",2022,"Jean-Christophe Gagnon-Audet,Kartik Ahuja,Mohammad Javad Darvishi Bayazi,G. Dumas,I. Rish",2,128,1,"https://www.semanticscholar.org/paper/66f3f0e8ebc780e570770986f50bf9cb9cd53ec1"
"bc16284f517dd0011dcf64ea1c8fe6d6576494a4",3,"Is the Computation of Abstract Sameness Relations Human-Like in Neural Language Models?","This work explores one facet of the question whether state-of-the-art NLP models exhibit elementary mechanisms known from human cognition by de-signed experimental settings in which each element from the original studies was mapped to a component of language models.","ArXiv",2022,"Lukas Thoma,Benjamin Roth",0,58,0,"https://www.semanticscholar.org/paper/bc16284f517dd0011dcf64ea1c8fe6d6576494a4"
"01b2f7601ab3df0d2982a204e2fb309f6622646f",3,"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models","It is demonstrated that large language models can succeed in extrapolation without modifying their architecture or training procedure, and a limitation of current architectures to effectively generalize without explicit surface form guidance is highlighted.","ArXiv",2022,"M. Bueno,Carlos Gemmel,Jeffrey Stephen Dalton,R. Lotufo,Rodrigo Nogueira",0,63,0,"https://www.semanticscholar.org/paper/01b2f7601ab3df0d2982a204e2fb309f6622646f"
"1ed29beb55b10de8553c926ce6da2625ec2c8776",3,"Benchmarking Long-tail Generalization with Likelihood Splits","This work proposes a method to create challenging benchmarks that require generalizing to the tail of the distribution by re-splitting existing datasets by creating ‘Likeli-hood splits’ where examples that are assigned lower likelihood by a pre-trained language model are placed in the test set, and more likely examples are in the training set.","ArXiv",2022,"Ameya Godbole,Robin Jia",0,56,0,"https://www.semanticscholar.org/paper/1ed29beb55b10de8553c926ce6da2625ec2c8776"
"c6e4518dfd687a2a5bed4e78d5d9f999292a1746",3,"Counterfactual Recipe Generation: Exploring Compositional Generalization in a Realistic Scenario","This paper investigates whether pretrained language models can perform compositional generalization in a realistic setting: recipe generation, and designs the counterfactual recipe generation task, which asks models to modify a base recipe according to the change of an ingredient.","ArXiv",2022,"Xiao Liu,Yansong Feng,Jizhi Tang,Chengang Hu,Dongyan Zhao",0,35,0,"https://www.semanticscholar.org/paper/c6e4518dfd687a2a5bed4e78d5d9f999292a1746"
"1167b3864046b732cf057b8b05db311e726cadab",3,"Measuring Alignment Bias in Neural Seq2seq Semantic Parsers","This work augments the popular Geo semantic parsing dataset with alignment annotations and creates Geo-Aligned, and studies the performance of standard seq2seq models on the examples that can be aligned monotonically versus examples that require more complex alignments.","STARSEM",2022,"Davide Locatelli,A. Quattoni",1,39,0,"https://www.semanticscholar.org/paper/1167b3864046b732cf057b8b05db311e726cadab"
"8008348e87d3904842a2dd230c14b83112e8bf48",3,"Compositional Generalization in Dependency Parsing","This work introduces a gold-standard set of dependency parses for CFQ, and uses this to analyze the behaviour of a state-of-the art dependency parser on the CFQ dataset, finding that increasing compound divergence degrades dependency parsing performance, although not as dramatically as semantic parsing performance.","ACL",2021,"Emily Goodwin,Siva Reddy,T. O’Donnell,Dzmitry Bahdanau",3,18,0,"https://www.semanticscholar.org/paper/8008348e87d3904842a2dd230c14b83112e8bf48"
"2b060b89324c376892a096c84fd14664f7b71710",3,"Understanding Robust Generalization in Learning Regular Languages","The empirical results support the hypothesis that auxiliary tasks can enable robust generalization, and theoretically prove that the compositional strategy generalizes significantly better than the end-to-end strategy.","ICML",2022,"Soham Dan,O. Bastani,D. Roth",1,37,0,"https://www.semanticscholar.org/paper/2b060b89324c376892a096c84fd14664f7b71710"
"03eeff98d24383518ce0dacc0b3c4a38b6f1a514",3,"Recursive Decoding: A Situated Cognition Approach to Compositional Generation in Grounded Language Understanding","Recursive Decoding (RD) is presented, a novel procedure for training and using seq2seq models, targeted towards decode-side generalization, which yields dramatic improvement on two previously neglected generalization tasks in gSCAN.","ArXiv",2022,"Matthew Setzler,Scott Howland,Lauren A. Phillips",0,34,0,"https://www.semanticscholar.org/paper/03eeff98d24383518ce0dacc0b3c4a38b6f1a514"
"3d5699e7f7e085ad72102859b06fa4884d207e77",3,"Iterative Decoding for Compositional Generalization in Transformers","This paper introduces iterative decoding, an alternative toseq2seq that improves transformer compositional generalization in the PCFG and Cartesian product datasets and evidences that, in these datasets, seq2seq transformers do not learn iterations that are not unrolled.","ArXiv",2021,"Luana Ruiz,J. Ainslie,Santiago Ontan'on",3,27,0,"https://www.semanticscholar.org/paper/3d5699e7f7e085ad72102859b06fa4884d207e77"
"a77468f6bd4db7f8d761a0569d9cc29d5a8f0034",3,"L OGIC I NFERENCE : A N EW D ATASET FOR T EACHING L OGICAL I NFERENCE TO SEQ 2 SEQ M ODELS","A new dataset to evaluate the ability of models to perform logical inference using propositional logic and a small subset of ﬁrst-order logic, represented both in semi-formal logical notation, as well as in natural language is presented.","",2022,"Santiago Ontañón,J. Ainslie,V. Cvicek,Zachary Kenneth Fisher",0,19,0,"https://www.semanticscholar.org/paper/a77468f6bd4db7f8d761a0569d9cc29d5a8f0034"
"5021fd710fd17dee53bc7bc7bf334b148ef3d8b6",3,"LogicInference: A New Dataset for Teaching Logical Inference to seq2seq Models","A new dataset to evaluate the ability of models to perform logical inference using propositional logic and a small subset of ﬁrst-order logic, represented both in semi-formal logical notation, as well as in natural language is presented.","ArXiv",2022,"Santiago Ontañón,J. Ainslie,V. Cvicek,Zachary Kenneth Fisher",0,19,0,"https://www.semanticscholar.org/paper/5021fd710fd17dee53bc7bc7bf334b148ef3d8b6"
"837cc9a366c873c84ceec7e84d5cb3d5753757d6",3,"Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks","This work shows that two-layer transformers learn generalizable solutions to multi-level problems and develop signs of systematic task decomposition, and provides key insights into how transformer models may be capable of decomposing complex decisions into reusable, multi- level policies in tasks requiring structured behavior.","ArXiv",2022,"Yuxuan Li,James L. McClelland",0,35,0,"https://www.semanticscholar.org/paper/837cc9a366c873c84ceec7e84d5cb3d5753757d6"
"97833e2aa0da5240e62436373b58af988a4ab6ab",3,"The Curious Case of Absolute Position Embeddings","","",2022,"Koustuv Sinha,Amirhossein Kazemnejad,Siva Reddy,J. Pineau,D. Hupkes,Adina Williams",0,56,0,"https://www.semanticscholar.org/paper/97833e2aa0da5240e62436373b58af988a4ab6ab"
"86589b6286ef3c55b8b4fccfb41a3b30b7afdf61",2,"Going Beyond Linear Transformers with Recurrent Fast Weight Programmers","The novel recurrent FWPs (RFWPs) are evaluated on two synthetic algorithmic tasks (code execution and sequential ListOps), Wikitext-103 language models, and on the Atari 2600 2D game environment, where the models exhibit properties of Transformers and RNNs.","NeurIPS",2021,"Kazuki Irie,Imanol Schlag,R'obert Csord'as,J. Schmidhuber",16,80,0,"https://www.semanticscholar.org/paper/86589b6286ef3c55b8b4fccfb41a3b30b7afdf61"
"79cb080c84da314c2113692585b1e9ee29afa33a",2,"On learning an interpreted language with recurrent models","This work constructs simplified datasets reflecting core properties of natural language as modeled in formal syntax and semantics: recursive syntactic structure and compositionality, and finds LSTM and GRU networks to generalise to compositional interpretation well, but only in the most favorable learning settings.","",2018,"Denis Paperno",0,30,0,"https://www.semanticscholar.org/paper/79cb080c84da314c2113692585b1e9ee29afa33a"
"4b58367375466e653751a0c258b2f50bd3551408",2,"Sequence-to-Sequence Networks Learn the Meaning of Reflexive Anaphora","This paper considers sequence-to-sequence architectures with recurrent units and shows that such networks are capable of learning semantic interpretations for reflexive anaphora which generalize to novel antecedents.","CRAC",2020,"R. Frank,Jackson Petty",2,24,0,"https://www.semanticscholar.org/paper/4b58367375466e653751a0c258b2f50bd3551408"
"7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8",2,"AND does not mean OR: Using Formal Languages to Study Language Models’ Representations","None of the simulated training corpora result in models which definitively differentiate meaningfully different symbols (e.g., AND vs. OR), suggesting a limitation to the types of semantic signals that current models are able to exploit.","ACL",2021,"Aaron Traylor,Roman Feiman,Elizabeth-Jane Pavlick",7,17,0,"https://www.semanticscholar.org/paper/7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8"
"0b1470014bdbaa80ba63da0491d9db6c7d4febcc",2,"Detecting Compositionally Out-of-Distribution Examples in Semantic Parsing","This work investigates several strong yet simple methods for OOD detection based on predictive uncertainty and shows that these techniques perform well on the standard SCAN and CFQ datasets and can be improved by using a heterogeneous ensemble.","EMNLP",2021,"Denis Lukovnikov,Sina Däubener,Asja Fischer",1,30,0,"https://www.semanticscholar.org/paper/0b1470014bdbaa80ba63da0491d9db6c7d4febcc"
"acf8a1040034820bf99379a3422815f4e0859ec9",2,"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?","NQG-T5 is proposed, a hybrid model that combines a high-precision grammar-based approach with a pre-trained sequence-to-sequence model that outperforms existing approaches across several compositional generalization challenges on non-synthetic data, while also being competitive with the state of theart on standard evaluations.","ACL",2020,"Peter Shaw,Ming-Wei Chang,Panupong Pasupat,Kristina Toutanova",72,67,17,"https://www.semanticscholar.org/paper/acf8a1040034820bf99379a3422815f4e0859ec9"
"40848b41ed8c9c255ecd8a920006877691b52d03",2,"WILDS: A Benchmark of in-the-Wild Distribution Shifts","WILDS is presented, a benchmark of in-the-wild distribution shifts spanning diverse data modalities and applications, and is hoped to encourage the development of general-purpose methods that are anchored to real-world distribution shifts and that work well across different applications and problem settings.","ICML",2020,"P. W. Koh,Shiori Sagawa,H. Marklund,Sang Michael Xie,Marvin Zhang,Akshay Balsubramani,Weihua Hu,Michihiro Yasunaga,Richard L. Phillips,Sara Beery,J. Leskovec,A. Kundaje,E. Pierson,S. Levine,Chelsea Finn,Percy Liang",393,424,66,"https://www.semanticscholar.org/paper/40848b41ed8c9c255ecd8a920006877691b52d03"
"0d39d525f30609d0541330f933007025cd457a83",2,"Exploring Transitivity in Neural NLI Models through Veridicality","It is found that current NLI models do not perform consistently well on transitivity inference tasks, suggesting that they lack the generalization capacity for drawing composite inferences from provided training examples.","EACL",2021,"Hitomi Yanaka,K. Mineshima,Kentaro Inui",12,59,1,"https://www.semanticscholar.org/paper/0d39d525f30609d0541330f933007025cd457a83"
"5505d608a1d482fdc083796db812379ec1cb8723",2,"Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches","Small, targeted synthetic benchmarks are constructed that do not resemble natural language, yet have high concurrence with SQuAD, demonstrating that naturalness and size are not necessary for reflecting historical modeling improvements on SQuad.","ArXiv",2021,"Nelson F. Liu,Tony Lee,Robin Jia,Percy Liang",13,107,1,"https://www.semanticscholar.org/paper/5505d608a1d482fdc083796db812379ec1cb8723"
"577d44a10b424a55165a6bf4839bafce2c695302",2,"SyGNS: A Systematic Generalization Testbed Based on Natural Language Semantics","This work proposes a Systematic Generalization testbed based on Natural language Semantics (SyGNS), whose challenge is to map natural language sentences to multiple forms of scoped meaning representations, designed to account for various semantic phenomena.","FINDINGS",2021,"Hitomi Yanaka,K. Mineshima,Kentarou Inui",3,60,0,"https://www.semanticscholar.org/paper/577d44a10b424a55165a6bf4839bafce2c695302"
"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34",2,"Lexicon Learning for Few Shot Sequence Modeling","This work augments neural decoders with a lexical translation mechanism that generalizes existing copy mechanisms to incorporate learned, decontextualized, token-level translation rules, and shows that it improves systematic generalization on a diverse set of sequence modeling tasks drawn from cognitive science, formal semantics, and machine translation.","ACL",2021,"Ekin Akyürek,Jacob Andreas",14,42,1,"https://www.semanticscholar.org/paper/83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34"
"3962f108081b22c7e54b413f47ba6f2c16f2cc05",2,"Frequency Effects on Syntactic Rule Learning in Transformers","It is shown that BERT often generalizes well to subject–verb pairs that never occurred in training, suggesting a degree of rule-governed behavior, and that performance is heavily influenced by word frequency.","EMNLP",2021,"Jason Wei,Dan Garrette,Tal Linzen,Ellie Pavlick",18,38,2,"https://www.semanticscholar.org/paper/3962f108081b22c7e54b413f47ba6f2c16f2cc05"
"af749e5dbde38914ca6fa1463fca17eac8f69ecc",2,"ReaSCAN: Compositional Reasoning in Language Grounding","This work proposes ReaSCAN, a benchmark dataset that builds off gSCAN but requires compositional language interpretation and reasoning about entities and relations, and assesses two models on Rea SCAN: a multi-modal baseline and a state-of-the-art graph convolutional neural model.","NeurIPS Datasets and Benchmarks",2021,"Zhengxuan Wu,Elisa Kreiss,Desmond C. Ong,Christopher Potts",7,56,1,"https://www.semanticscholar.org/paper/af749e5dbde38914ca6fa1463fca17eac8f69ecc"
"06fbeaf4d16639f177973a06cd7c4f78cb5e38ed",2,"COVR: A Test-Bed for Visually Grounded Compositional Generalization with Real Images","This work proposes COVR, a new test-bed for visually-grounded compositional generalization with real images, and proposes an almost fully automatic procedure for generating question-answer pairs along with a set of context images.","EMNLP",2021,"Ben Bogin,Shivanshu Gupta,Matt Gardner,Jonathan Berant",7,29,1,"https://www.semanticscholar.org/paper/06fbeaf4d16639f177973a06cd7c4f78cb5e38ed"
"4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80",2,"How BPE Affects Memorization in Transformers","It is demonstrated that the size of the subword vocabulary learned by Byte-Pair Encoding greatly affects both ability and tendency of standard Transformer models to memorize training data, even when the authors control for the number of learned parameters.","ArXiv",2021,"E. Kharitonov,Marco Baroni,D. Hupkes",8,67,0,"https://www.semanticscholar.org/paper/4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80"
"04db9b694280134f09af5fa787a306907edba29d",2,"How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN","AVEN, a suite of analyses for assessing the novelty of generated text, focusing on sequential structure (n-grams) and syntactic structure, is introduced, showing that GPT-2's novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues.","ArXiv",2021,"R. Thomas McCoy,P. Smolensky,Tal Linzen,Jianfeng Gao,Asli Celikyilmaz",13,81,2,"https://www.semanticscholar.org/paper/04db9b694280134f09af5fa787a306907edba29d"
"a576512a7562597fd30719a834d5866d010ef6ab",2,"Compositional Generalization for Natural Language Interfaces to Web APIs","New compositional generalization tasks for NL2API are defined which explore the models’ ability to extrapolate from simple API calls in the training set to new and more complex API Calls in the inference phase.","ArXiv",2021,"Saghar Hosseini,A. Awadallah,Yu Su",1,33,1,"https://www.semanticscholar.org/paper/a576512a7562597fd30719a834d5866d010ef6ab"
"dc88d2bbcebd810d7c80ba281739908005b12235",2,"Neurocompositional computing in human and machine intelligence: A tutorial","It is shown that the new techniques now being deployed in second-generation neurocompositional computing create AI systems that are not only more robust and accurate than current systems, but also more comprehensible—making it possible to diagnose errors in, and exert human control over, artificial neural networks through interpretation of their internal states and direct intervention upon those states.","",2022,"P. Smolensky,R. Thomas McCoy,Roland Fernandez,M. Goldrick,Jia-Hao Gao",2,240,0,"https://www.semanticscholar.org/paper/dc88d2bbcebd810d7c80ba281739908005b12235"
"b3f644a5ea1fdd8cec1c34ebed69125838a50de3",2,"The Paradox of the Compositionality of Natural Language: A Neural Machine Translation Case Study","This work re-instantiate three compositionality tests from the literature and reformulate them for neural machine translation (NMT) and highlights that models are sometimes less compositional than expected, but sometimes more, exemplifying that different levels of compositionality are required.","ACL",2021,"Verna Dankers,Elia Bruni,D. Hupkes",20,51,1,"https://www.semanticscholar.org/paper/b3f644a5ea1fdd8cec1c34ebed69125838a50de3"
"b8b813111c411ae61881ab9cd25707d9de6444ec",2,"Compositional Attention: Disentangling Search and Retrieval","This work proposes a novel attention mechanism, called Compositional Attention, that replaces the standard head structure, and demonstrates that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings.","ICLR",2021,"Sarthak Mittal,Sharath Chandra Raparthy,I. Rish,Yoshua Bengio,Guillaume Lajoie",7,45,0,"https://www.semanticscholar.org/paper/b8b813111c411ae61881ab9cd25707d9de6444ec"
"40b4d98588719407fb72a014ab79e4145695654b",2,"Quantifying Adaptability in Pre-trained Language Models with 500 Tasks","A large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence modeling tasks, shows that adaptability to new tasks, like generalization to new examples, can be systematically described and understood.","NAACL",2021,"Belinda Z. Li,Jane A. Yu,Madian Khabsa,Luke Zettlemoyer,A. Halevy,Jacob Andreas",3,60,0,"https://www.semanticscholar.org/paper/40b4d98588719407fb72a014ab79e4145695654b"
"ace2a00425f96e9d0dbbe2869023d56c6c91267f",2,"On Learning Interpreted Languages with Recurrent Models","This work finds LSTM and GRU networks to generalize to compositional interpretation well, but only in the most favorable learning settings, with a well-paced curriculum, extensive training data, and left- to-right (but not right-to-left) composition.","Computational Linguistics",2022,"Denis Paperno",0,32,0,"https://www.semanticscholar.org/paper/ace2a00425f96e9d0dbbe2869023d56c6c91267f"