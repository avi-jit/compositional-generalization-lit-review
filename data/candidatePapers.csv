"id","score","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount","url"
"4c430e6c3a72626bd4cb1893960c7c26dfec6c79",5,"Structurally Diverse Sampling Reduces Spurious Correlations in Semantic Parsing Datasets","This work proposes a novel algorithm for sampling a structurally diverse set of instances from a labeled instance pool with structured outputs and uses information theory to show that reduction in spurious correlations between substructures may be one reason why diverse training sets improve generalization.","arXiv.org",2022,"Shivanshu Gupta,Sameer Singh,Matt Gardner",5,40,0,"https://www.semanticscholar.org/paper/4c430e6c3a72626bd4cb1893960c7c26dfec6c79"
"a19d9fd97cf172e1eb742053770c397d80468448",5,"Translate First Reorder Later: Leveraging Monotonicity in Semantic Parsing","By means of the monotonic translations, TPol can learn reliable lexico-logical patterns from aligned data, significantly improving compositional generalization both over conventional seq2seq models, as well as over other approaches that exploit gold alignments.","Findings",2022,"Francesco Cazzaro,Davide Locatelli,A. Quattoni,X. Carreras",1,48,0,"https://www.semanticscholar.org/paper/a19d9fd97cf172e1eb742053770c397d80468448"
"fcf25e1affc2f8ee5bb49d156f174e9769234deb",5,"Systematic Generalization with Edge Transformers","This work proposes Edge Transformer, a new model that combines inspiration from Transformers and rule-based symbolic AI that outperforms Relation-aware, Universal and classical Transformer baselines on compositional generalization benchmarks in relational reasoning, semantic parsing, and dependency parsing.","Neural Information Processing Systems",2021,"Leon Bergen,T. O’Donnell,Dzmitry Bahdanau",19,44,2,"https://www.semanticscholar.org/paper/fcf25e1affc2f8ee5bb49d156f174e9769234deb"
"b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347",4,"Sequence-to-Sequence Learning with Latent Neural Grammars","This work develops a neural parameterization of the grammar which enables parameter sharing over the combinatorial space of derivation rules without the need for manual feature engineering and applies this latent neural grammar to various domains and finds that it performs respectably compared to standard baselines.","Neural Information Processing Systems",2021,"Yoon Kim",22,134,5,"https://www.semanticscholar.org/paper/b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347"
"b1f33e956e36bf25e118c0d537dcc519cfe52e60",4,"CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations","CTL++ is introduced, a new diagnostic dataset based on compositions of unary symbolic functions designed to test systematicity of NNs, that is, their capability to generalize to unseen compositions of known functions.","Conference on Empirical Methods in Natural Language Processing",2022,"R'obert Csord'as,Kazuki Irie,J. Schmidhuber",1,26,0,"https://www.semanticscholar.org/paper/b1f33e956e36bf25e118c0d537dcc519cfe52e60"
"676fa805bd715591f99bb17e36d673a6a14e92fe",4,"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization","This work investigates automatic generation of synthetic utterance-program pairs for improving compositional generalization in semantic parsing and selects a subset of synthetic examples that are structurally-diverse and uses them to improve compositionalgeneralization.","Conference on Empirical Methods in Natural Language Processing",2021,"I. Oren,Jonathan Herzig,Jonathan Berant",21,48,4,"https://www.semanticscholar.org/paper/676fa805bd715591f99bb17e36d673a6a14e92fe"
"c735740b26ceaa4db9d77233116434c0e8b311d8",4,"Learning Adaptive Control Flow in Transformers for Improved Systematic Generalization","The novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the compositional table lookup task, and its attention and gating patterns tend to be interpretable as an intuitive form of neural routing.","",2021,"R. Csordás,Kazuki Irie,J. Schmidhuber",0,36,0,"https://www.semanticscholar.org/paper/c735740b26ceaa4db9d77233116434c0e8b311d8"
"61d56ece2d19f4bfeb322c92085fb28521e169da",4,"Neural-Symbolic Recursive Machine for Systematic Generalization","The proposed Neural-Symbolic Recursive Machine demonstrates stronger generalization than pure neural networks due to its symbolic representation and inductive biases, and demonstrates better transferability than existing neural-symbolic approaches due to less domain-specific knowledge required.","arXiv.org",2022,"Qing Li,Yixin Zhu,Yitao Liang,Y. Wu,Song-Chun Zhu,Siyuan Huang",1,49,0,"https://www.semanticscholar.org/paper/61d56ece2d19f4bfeb322c92085fb28521e169da"
"b49ebf36a29cf9734313066129ab0d7092d4041e",4,"Categorizing Semantic Representations for Neural Machine Translation","The main idea is to enhance generalization by reducing sparsity and overfitting, which is achieved by finding prototypes of token representations over the training set and integrating their embeddings into the source encoding.","International Conference on Computational Linguistics",2022,"Yongjing Yin,Yafu Li,Fandong Meng,Jie Zhou,Yue Zhang",4,58,1,"https://www.semanticscholar.org/paper/b49ebf36a29cf9734313066129ab0d7092d4041e"
"0dc5dd7c64ee016bdc33a5f32dc25747be5ca702",3,"FROM SCAN TO REAL DATA: SYSTEMATIC GENER-","This paper revisits systematic generalization from the perspective of meaningful learning, an exceptional capability of humans to learn new concepts by connecting them with other previously known knowledge, and proposes to augment a training dataset in either an inductive or deductive manner to build semantic links between new and old concepts.","",2021,"Ning Shi,Boxin Wang,Wei Wang,Xiangyu Liu,Rong Zhang,Hui Xue,Xinbing Wang,Zhouhan Lin",2,49,0,"https://www.semanticscholar.org/paper/0dc5dd7c64ee016bdc33a5f32dc25747be5ca702"
"a406701b5fb05be55244d4f940db7be55fce85c6",3,"Semantic Systematicity in Connectionist Language Production","A novel connectionist model of sentence production that employs rich situation model representations originally proposed for modeling systematicity in comprehension, which provides a sufficient structure from which the neural network can interpret novel inputs.","Inf.",2021,"Jesús Calvillo,Harm Brouwer,M. Crocker",0,59,0,"https://www.semanticscholar.org/paper/a406701b5fb05be55244d4f940db7be55fce85c6"
"268fedc5d786fa197b294dccab7eea02dc08038a",3,"A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics","It is discovered that it is infeasible to solve HINT by merely scaling up the dataset and the model size; this strategy contributes little to the extrapolation of syntax and semantics.","International Conference on Learning Representations",2021,"Qing Li,Siyuan Huang,Yining Hong,Yixin Zhu,Y. Wu,Song-Chun Zhu",2,117,0,"https://www.semanticscholar.org/paper/268fedc5d786fa197b294dccab7eea02dc08038a"
"d129841cb2e30e25000dcd9edb83c880fc4babc1",3,"Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention","This work develops a novel modification to the transformer by implementing two separate input streams: a role stream controls the attention distributions at each layer, and a filler stream determines the values.","North American Chapter of the Association for Computational Linguistics",2022,"Ayush K Chakravarthy,Jacob Russin,R. O’Reilly",3,35,0,"https://www.semanticscholar.org/paper/d129841cb2e30e25000dcd9edb83c880fc4babc1"
"69df5b68fbf492341336b39b4cc9fcc74fff4d5f",3,"Improving Systematic Generalization Through Modularity and Augmentation","This work investigates how two well-known modeling principles -- modularity and data augmentation -- affect systematic generalization of neural networks in grounded language learning, and analyzes how large the vocabulary needs to be and how similar the augmented data Needs to be to the problem at hand.","arXiv.org",2022,"Laura Ruis,B. Lake",6,47,2,"https://www.semanticscholar.org/paper/69df5b68fbf492341336b39b4cc9fcc74fff4d5f"
"2c6230fd6c474790d3a3199b8491bef0d54f8fd3",3,"WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series Tasks","WOODS: eight challenging open-source time series benchmarks covering a diverse range of data modalities, such as videos, brain recordings, and sensor signals is presented, underscoring the new challenges posed by time series tasks.","arXiv.org",2022,"Jean-Christophe Gagnon-Audet,Kartik Ahuja,Mohammad Javad Darvishi Bayazi,G. Dumas,I. Rish",13,147,2,"https://www.semanticscholar.org/paper/2c6230fd6c474790d3a3199b8491bef0d54f8fd3"
"bc16284f517dd0011dcf64ea1c8fe6d6576494a4",3,"Is the Computation of Abstract Sameness Relations Human-Like in Neural Language Models?","The results suggest that the cognitive faculty of computing abstract sameness relations is stronger in infants than in all investigated PLMs.","arXiv.org",2022,"Lukas Thoma,Benjamin Roth",0,58,0,"https://www.semanticscholar.org/paper/bc16284f517dd0011dcf64ea1c8fe6d6576494a4"
"108c25905be36b2a7a0fc7256ac314985ecd9699",3,"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models","This work demonstrates that large language models can succeed in extrapolation without modifying their architecture or training procedure, and shows how generating step-by-step rationales and introducing marker tokens are both required for effective extrapolation.","MATHNLP",2022,"M. Bueno,Carlos Gemmel,Jeffrey Stephen Dalton,R. Lotufo,Rodrigo Nogueira",7,67,1,"https://www.semanticscholar.org/paper/108c25905be36b2a7a0fc7256ac314985ecd9699"
"49aec6fb44ab52181960512a6067eded0ce4182b",3,"Benchmarking Long-tail Generalization with Likelihood Splits","This work creates ‘Likelihood Splits’ where examples that are assigned lower likelihood by a pre-trained language model are placed in the test set, and more likely examples are in the training set, which can be customized to construct meaningful train-test splits for a wide range of tasks.","Findings",2022,"Ameya Godbole,Robin Jia",2,74,0,"https://www.semanticscholar.org/paper/49aec6fb44ab52181960512a6067eded0ce4182b"
"c6e4518dfd687a2a5bed4e78d5d9f999292a1746",3,"Counterfactual Recipe Generation: Exploring Compositional Generalization in a Realistic Scenario","This paper designs the counterfactual recipe generation task, which asks models to modify a base recipe according to the change of an ingredient, and finetune pretrained language models on the recipe corpus, and uses unsupervised counterfactUAL generation methods to generate modified recipes.","Conference on Empirical Methods in Natural Language Processing",2022,"Xiao Liu,Yansong Feng,Jizhi Tang,ChenGang Hu,Dongyan Zhao",0,39,0,"https://www.semanticscholar.org/paper/c6e4518dfd687a2a5bed4e78d5d9f999292a1746"
"59afb82c235f3d89996cefa71cd33ba7592e6b53",3,"Measuring Alignment Bias in Neural Seq2seq Semantic Parsers","This work augments the popular Geo semantic parsing dataset with alignment annotations and creates Geo-Aligned, and studies the performance of standard seq2seq models on the examples that can be aligned monotonically versus examples that require more complex alignments.","STARSEM",2022,"Davide Locatelli,A. Quattoni",1,39,0,"https://www.semanticscholar.org/paper/59afb82c235f3d89996cefa71cd33ba7592e6b53"
"8008348e87d3904842a2dd230c14b83112e8bf48",3,"Compositional Generalization in Dependency Parsing","This work introduces a gold-standard set of dependency parses for CFQ, and uses this to analyze the behaviour of a state-of-the art dependency parser on the CFQ dataset, finding that increasing compound divergence degrades dependency parsing performance, although not as dramatically as semantic parsing performance.","Annual Meeting of the Association for Computational Linguistics",2021,"Emily Goodwin,Siva Reddy,T. O’Donnell,Dzmitry Bahdanau",4,20,0,"https://www.semanticscholar.org/paper/8008348e87d3904842a2dd230c14b83112e8bf48"
"2b060b89324c376892a096c84fd14664f7b71710",3,"Understanding Robust Generalization in Learning Regular Languages","This paper theoretically proves that the compositional strategy generalizes significantly better than the end-to-end strategy and implements it via an auxiliary task where the goal is to predict the intermediate states visited by the DFA when parsing a string.","International Conference on Machine Learning",2022,"Soham Dan,O. Bastani,D. Roth",3,37,0,"https://www.semanticscholar.org/paper/2b060b89324c376892a096c84fd14664f7b71710"
"03eeff98d24383518ce0dacc0b3c4a38b6f1a514",3,"Recursive Decoding: A Situated Cognition Approach to Compositional Generation in Grounded Language Understanding","Recursive Decoding (RD) is presented, a novel procedure for training and using seq2seq models, targeted towards decode-side generalization, which yields dramatic improvement on two previously neglected generalization tasks in gSCAN.","arXiv.org",2022,"Matthew Setzler,Scott Howland,Lauren A. Phillips",4,34,0,"https://www.semanticscholar.org/paper/03eeff98d24383518ce0dacc0b3c4a38b6f1a514"
"3d5699e7f7e085ad72102859b06fa4884d207e77",3,"Iterative Decoding for Compositional Generalization in Transformers","This paper introduces iterative decoding, an alternative toseq2seq that improves transformer compositional generalization in the PCFG and Cartesian product datasets and evidences that, in these datasets, seq2seq transformers do not learn iterations that are not unrolled.","arXiv.org",2021,"Luana Ruiz,J. Ainslie,Santiago Ontan'on",5,25,0,"https://www.semanticscholar.org/paper/3d5699e7f7e085ad72102859b06fa4884d207e77"
"5021fd710fd17dee53bc7bc7bf334b148ef3d8b6",3,"LogicInference: A New Dataset for Teaching Logical Inference to seq2seq Models","LogicInference is presented, a new dataset to evaluate the ability of models to perform logical inference using propositional logic and a small subset of first-order logic, represented both in semi-formal logical notation, as well as in natural language.","arXiv.org",2022,"Santiago Ontañón,J. Ainslie,V. Cvicek,Zachary Kenneth Fisher",2,25,1,"https://www.semanticscholar.org/paper/5021fd710fd17dee53bc7bc7bf334b148ef3d8b6"
"5021fd710fd17dee53bc7bc7bf334b148ef3d8b6",3,"LogicInference: A New Dataset for Teaching Logical Inference to seq2seq Models","LogicInference is presented, a new dataset to evaluate the ability of models to perform logical inference using propositional logic and a small subset of first-order logic, represented both in semi-formal logical notation, as well as in natural language.","arXiv.org",2022,"Santiago Ontañón,J. Ainslie,V. Cvicek,Zachary Kenneth Fisher",2,25,1,"https://www.semanticscholar.org/paper/5021fd710fd17dee53bc7bc7bf334b148ef3d8b6"
"a5378175d31d3dd8fa004037df663aa00f236a0b",3,"Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks","It is shown that two-layer transformers learn reliable solutions to multi-level problems, develop signs of task decomposition, and encode input items in a way that encourages the exploitation of shared computation across related tasks.","arXiv.org",2022,"Yuxuan Li,James L. McClelland",7,39,0,"https://www.semanticscholar.org/paper/a5378175d31d3dd8fa004037df663aa00f236a0b"
"97833e2aa0da5240e62436373b58af988a4ab6ab",3,"The Curious Case of Absolute Position Embeddings","It is observed that models trained with APE over-rely on positional information to the point that they break-down when subjected to sentences with shifted position information, which raises questions about the efficacy of APEs to model the relativity of position information.","Conference on Empirical Methods in Natural Language Processing",2022,"Koustuv Sinha,Amirhossein Kazemnejad,Siva Reddy,J. Pineau,D. Hupkes,Adina Williams",5,62,0,"https://www.semanticscholar.org/paper/97833e2aa0da5240e62436373b58af988a4ab6ab"
"86589b6286ef3c55b8b4fccfb41a3b30b7afdf61",2,"Going Beyond Linear Transformers with Recurrent Fast Weight Programmers","The novel recurrent FWPs (RFWPs) are evaluated on two synthetic algorithmic tasks (code execution and sequential ListOps), Wikitext-103 language models, and on the Atari 2600 2D game environment and report large improvements over LSTM in several Atari games.","Neural Information Processing Systems",2021,"Kazuki Irie,Imanol Schlag,R'obert Csord'as,J. Schmidhuber",34,84,1,"https://www.semanticscholar.org/paper/86589b6286ef3c55b8b4fccfb41a3b30b7afdf61"
"79cb080c84da314c2113692585b1e9ee29afa33a",2,"On Learning Interpreted Languages with Recurrent Models","This work finds LSTM and GRU networks to generalize to compositional interpretation well, but only in the most favorable learning settings, with a well-paced curriculum, extensive training data, and left- to-right (but not right-to-left) composition.","Computational Linguistics",2018,"Denis Paperno",0,31,0,"https://www.semanticscholar.org/paper/79cb080c84da314c2113692585b1e9ee29afa33a"
"4b58367375466e653751a0c258b2f50bd3551408",2,"Sequence-to-Sequence Networks Learn the Meaning of Reflexive Anaphora","This paper considers sequence-to-sequence architectures with recurrent units and shows that such networks are capable of learning semantic interpretations for reflexive anaphora which generalize to novel antecedents.","CRAC",2020,"R. Frank,Jackson Petty",2,24,0,"https://www.semanticscholar.org/paper/4b58367375466e653751a0c258b2f50bd3551408"
"7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8",2,"AND does not mean OR: Using Formal Languages to Study Language Models’ Representations","None of the simulated training corpora result in models which definitively differentiate meaningfully different symbols (e.g., AND vs. OR), suggesting a limitation to the types of semantic signals that current models are able to exploit.","Annual Meeting of the Association for Computational Linguistics",2021,"Aaron Traylor,Roman Feiman,Elizabeth-Jane Pavlick",11,17,0,"https://www.semanticscholar.org/paper/7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8"
"0b1470014bdbaa80ba63da0491d9db6c7d4febcc",2,"Detecting Compositionally Out-of-Distribution Examples in Semantic Parsing","This work investigates several strong yet simple methods for OOD detection based on predictive uncertainty and shows that these techniques perform well on the standard SCAN and CFQ datasets and can be improved by using a heterogeneous ensemble.","Conference on Empirical Methods in Natural Language Processing",2021,"Denis Lukovnikov,Sina Däubener,Asja Fischer",5,29,1,"https://www.semanticscholar.org/paper/0b1470014bdbaa80ba63da0491d9db6c7d4febcc"
"acf8a1040034820bf99379a3422815f4e0859ec9",2,"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?","NQG-T5 is proposed, a hybrid model that combines a high-precision grammar-based approach with a pre-trained sequence-to-sequence model that outperforms existing approaches across several compositional generalization challenges on non-synthetic data, while also being competitive with the state of theart on standard evaluations.","Annual Meeting of the Association for Computational Linguistics",2020,"Peter Shaw,Ming-Wei Chang,Panupong Pasupat,Kristina Toutanova",122,68,24,"https://www.semanticscholar.org/paper/acf8a1040034820bf99379a3422815f4e0859ec9"
"40848b41ed8c9c255ecd8a920006877691b52d03",2,"WILDS: A Benchmark of in-the-Wild Distribution Shifts","WILDS is presented, a benchmark of in-the-wild distribution shifts spanning diverse data modalities and applications, and is hoped to encourage the development of general-purpose methods that are anchored to real-world distribution shifts and that work well across different applications and problem settings.","International Conference on Machine Learning",2020,"Pang Wei Koh,Shiori Sagawa,H. Marklund,Sang Michael Xie,Marvin Zhang,Akshay Balsubramani,Weihua Hu,Michihiro Yasunaga,Richard L. Phillips,Sara Beery,J. Leskovec,A. Kundaje,E. Pierson,S. Levine,Chelsea Finn,Percy Liang",772,425,138,"https://www.semanticscholar.org/paper/40848b41ed8c9c255ecd8a920006877691b52d03"
"0d39d525f30609d0541330f933007025cd457a83",2,"Exploring Transitivity in Neural NLI Models through Veridicality","It is found that current NLI models do not perform consistently well on transitivity inference tasks, suggesting that they lack the generalization capacity for drawing composite inferences from provided training examples.","Conference of the European Chapter of the Association for Computational Linguistics",2021,"Hitomi Yanaka,K. Mineshima,Kentaro Inui",18,59,1,"https://www.semanticscholar.org/paper/0d39d525f30609d0541330f933007025cd457a83"
"5505d608a1d482fdc083796db812379ec1cb8723",2,"Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches","Small, targeted synthetic benchmarks are constructed that do not re-semble natural language, yet have high concurrence with SQuAD, demonstrating that naturalness and size are not necessary for re-�ecting historical modeling improvements on SQuad.","arXiv.org",2021,"Nelson F. Liu,Tony Lee,Robin Jia,Percy Liang",19,112,1,"https://www.semanticscholar.org/paper/5505d608a1d482fdc083796db812379ec1cb8723"
"577d44a10b424a55165a6bf4839bafce2c695302",2,"SyGNS: A Systematic Generalization Testbed Based on Natural Language Semantics","This work proposes a Systematic Generalization testbed based on Natural language Semantics (SyGNS), whose challenge is to map natural language sentences to multiple forms of scoped meaning representations, designed to account for various semantic phenomena.","Findings",2021,"Hitomi Yanaka,K. Mineshima,Kentaro Inui",7,63,0,"https://www.semanticscholar.org/paper/577d44a10b424a55165a6bf4839bafce2c695302"
"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34",2,"Lexicon Learning for Few Shot Sequence Modeling","This work augments neural decoders with a lexical translation mechanism that generalizes existing copy mechanisms to incorporate learned, decontextualized, token-level translation rules, and shows that it improves systematic generalization on a diverse set of sequence modeling tasks drawn from cognitive science, formal semantics, and machine translation.","Annual Meeting of the Association for Computational Linguistics",2021,"Ekin Akyürek,Jacob Andreas",24,42,4,"https://www.semanticscholar.org/paper/83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34"
"3962f108081b22c7e54b413f47ba6f2c16f2cc05",2,"Frequency Effects on Syntactic Rule Learning in Transformers","It is shown that BERT often generalizes well to subject–verb pairs that never occurred in training, suggesting a degree of rule-governed behavior, and that performance is heavily influenced by word frequency.","Conference on Empirical Methods in Natural Language Processing",2021,"Jason Wei,Dan Garrette,Tal Linzen,Ellie Pavlick",37,62,4,"https://www.semanticscholar.org/paper/3962f108081b22c7e54b413f47ba6f2c16f2cc05"
"af749e5dbde38914ca6fa1463fca17eac8f69ecc",2,"ReaSCAN: Compositional Reasoning in Language Grounding","ReaSCAN is proposed, a benchmark dataset that builds off gSCAN but requires compositional language interpretation and reasoning about entities and relations and is assessed on a multi-modal baseline and a state-of-the-art graph convolutional neural model.","NeurIPS Datasets and Benchmarks",2021,"Zhengxuan Wu,Elisa Kreiss,Desmond C. Ong,Christopher Potts",12,67,0,"https://www.semanticscholar.org/paper/af749e5dbde38914ca6fa1463fca17eac8f69ecc"
"06fbeaf4d16639f177973a06cd7c4f78cb5e38ed",2,"COVR: A Test-Bed for Visually Grounded Compositional Generalization with Real Images","This work proposes COVR, a new test-bed for visually-grounded compositional generalization with real images, and proposes an almost fully automatic procedure for generating question-answer pairs along with a set of context images.","Conference on Empirical Methods in Natural Language Processing",2021,"Ben Bogin,Shivanshu Gupta,Matt Gardner,Jonathan Berant",12,36,2,"https://www.semanticscholar.org/paper/06fbeaf4d16639f177973a06cd7c4f78cb5e38ed"
"4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80",2,"How BPE Affects Memorization in Transformers","It is demonstrated that the size of the subword vocabulary learned by Byte-Pair Encoding greatly affects both ability and tendency of standard Transformer models to memorize training data, even when the authors control for the number of learned parameters.","arXiv.org",2021,"E. Kharitonov,Marco Baroni,D. Hupkes",13,67,0,"https://www.semanticscholar.org/paper/4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80"
"04db9b694280134f09af5fa787a306907edba29d",2,"How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN","RAVEN, a suite of analyses for assessing the novelty of generated text, focusing on sequential structure (n-grams) and syntactic structure, is introduced, finding evidence that GPT-2 uses both compositional and analogical generalization mechanisms and showing that its novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues.","International Conference on Topology, Algebra and Categories in Logic",2021,"R. Thomas McCoy,P. Smolensky,Tal Linzen,Jianfeng Gao,Asli Celikyilmaz",37,101,4,"https://www.semanticscholar.org/paper/04db9b694280134f09af5fa787a306907edba29d"
"a576512a7562597fd30719a834d5866d010ef6ab",2,"Compositional Generalization for Natural Language Interfaces to Web APIs","New compositional generalization tasks for NL2API are defined which explore the models' ability to extrapolate from simple API calls in the training set to new and more complex API Calls in the inference phase.","arXiv.org",2021,"Saghar Hosseini,A. Awadallah,Yu Su",2,33,2,"https://www.semanticscholar.org/paper/a576512a7562597fd30719a834d5866d010ef6ab"
"dc88d2bbcebd810d7c80ba281739908005b12235",2,"Neurocompositional computing in human and machine intelligence: A tutorial","It is shown that the new techniques now being deployed in second-generation neurocompositional computing create AI systems that are not only more robust and accurate than current systems, but also more comprehensible—making it possible to diagnose errors in, and exert human control over, artificial neural networks through interpretation of their internal states and direct intervention upon those states.","",2022,"P. Smolensky,R. Thomas McCoy,Roland Fernandez,Matthew A. Goldrick,Jia-Hao Gao",3,225,1,"https://www.semanticscholar.org/paper/dc88d2bbcebd810d7c80ba281739908005b12235"
"b3f644a5ea1fdd8cec1c34ebed69125838a50de3",2,"The Paradox of the Compositionality of Natural Language: A Neural Machine Translation Case Study","This work re-instantiate three compositionality tests from the literature and reformulate them for neural machine translation (NMT) and highlights that models are sometimes less compositional than expected, but sometimes more, exemplifying that different levels of compositionality are required.","Annual Meeting of the Association for Computational Linguistics",2021,"Verna Dankers,Elia Bruni,D. Hupkes",41,55,4,"https://www.semanticscholar.org/paper/b3f644a5ea1fdd8cec1c34ebed69125838a50de3"
"b8b813111c411ae61881ab9cd25707d9de6444ec",2,"Compositional Attention: Disentangling Search and Retrieval","This work proposes a novel attention mechanism, called Compositional Attention, that replaces the standard head structure, and demonstrates that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings.","arXiv.org",2021,"Sarthak Mittal,Sharath Chandra Raparthy,I. Rish,Yoshua Bengio,Guillaume Lajoie",11,52,1,"https://www.semanticscholar.org/paper/b8b813111c411ae61881ab9cd25707d9de6444ec"
"40b4d98588719407fb72a014ab79e4145695654b",2,"Quantifying Adaptability in Pre-trained Language Models with 500 Tasks","A large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence modeling tasks, shows that adaptability to new tasks, like generalization to new examples, can be systematically described and understood.","North American Chapter of the Association for Computational Linguistics",2021,"Belinda Z. Li,Jane A. Yu,Madian Khabsa,Luke Zettlemoyer,A. Halevy,Jacob Andreas",10,64,1,"https://www.semanticscholar.org/paper/40b4d98588719407fb72a014ab79e4145695654b"
"79cb080c84da314c2113692585b1e9ee29afa33a",2,"On Learning Interpreted Languages with Recurrent Models","This work finds LSTM and GRU networks to generalize to compositional interpretation well, but only in the most favorable learning settings, with a well-paced curriculum, extensive training data, and left- to-right (but not right-to-left) composition.","Computational Linguistics",2018,"Denis Paperno",0,31,0,"https://www.semanticscholar.org/paper/79cb080c84da314c2113692585b1e9ee29afa33a"