"id","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount","url"
"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","Making Transformers Solve Compositional Tasks","This paper explores the design space of Transformer models showing that the inductive biases given to the model by several design decisions significantly impact compositional generalization.","ACL",2021,"Santiago Ontan'on,J. Ainslie,V. Cvicek,Zachary Kenneth Fisher",23,31,3,"https://www.semanticscholar.org/paper/49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2"
"ab72bccf6f3981537389510ecc609109e79595c3","Disentangled Sequence to Sequence Learning for Compositional Generalization","An extension to sequence-to-sequence models which encourage disentanglement by adaptively re-encoding (at each time step) the source input by condition the source representations on the newly decoded target context which makes it easier for the encoder to exploit specialized information for each prediction.","ACL",2021,"Hao Zheng,Mirella Lapata",8,50,0,"https://www.semanticscholar.org/paper/ab72bccf6f3981537389510ecc609109e79595c3"
"d3edc20ed4a07195f3663abc0ead4220266fd75b","*-CFQ: Analyzing the Scalability of Machine Learning on a Compositional Task","It is shown that compositional generalization remains a challenge at all training sizes, and that increasing the scope of natural language leads to consistently higher error rates, which are only partially offset by increased training data.","AAAI",2020,"D. Tsarkov,Tibor Tihon,Nathan Scales,Nikola Momchev,Danila Sinopalnikov,Nathanael Scharli",10,58,1,"https://www.semanticscholar.org/paper/d3edc20ed4a07195f3663abc0ead4220266fd75b"
"9fe39e0f2cf3b6d5f70a379654f9c08ffa48ddee","Unobserved Local Structures Make Compositional Generalization Hard","A criterion for the difficulty of an example is proposed: a test instance is hard if it contains a local structure that was not observed at training time and it predicts instance-level generalization well across 5 different semantic parsing datasets, substantially better than alternative decision rules.","",2022,"Ben Bogin,Shivanshu Gupta,Jonathan Berant",9,30,1,"https://www.semanticscholar.org/paper/9fe39e0f2cf3b6d5f70a379654f9c08ffa48ddee"
"40047a74b707743157051d38f76061ba5ff9aab4","Compositional Semantic Parsing with Large Language Models","The best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse.","ArXiv",2022,"Andrew Drozdov,Nathanael Scharli,Ekin Akyuurek,Nathan Scales,Xinying Song,Xinyun Chen,O. Bousquet,Denny Zhou",3,62,0,"https://www.semanticscholar.org/paper/40047a74b707743157051d38f76061ba5ff9aab4"
"6d00b1024298e5b64ee873028385f7bb4396b05d","Learning Algebraic Recombination for Compositional Generalization","This paper proposes LEAR, an end-toend neural model to learn algebraic recombination for compositional generalization, to model the semantic parsing task as a homomorphism between a latent syntactic algebra and a semantic algebra, thus encouraging algebraic rewriting.","FINDINGS",2021,"Chenyao Liu,Shengnan An,Zeqi Lin,Qian Liu,Bei Chen,Jian-Guang Lou,L. Wen,Nanning Zheng,Dongmei Zhang",13,59,5,"https://www.semanticscholar.org/paper/6d00b1024298e5b64ee873028385f7bb4396b05d"
"856fe866bcce5e7a540655bea6ecc7406bdcfcba","Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks","This paper introduces the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences, and tests the zero-shot generalization capabilities of a variety of recurrent neural networks trained on SCAN with sequence-to-sequence methods.","ICML",2017,"B. Lake,Marco Baroni",463,50,74,"https://www.semanticscholar.org/paper/856fe866bcce5e7a540655bea6ecc7406bdcfcba"
"b20ddcbd239f3fa9acc603736ac2e4416302d074","COGS: A Compositional Generalization Challenge Based on Semantic Interpretation","In experiments with Transformers and LSTMs, it is found that in-distribution accuracy on the COGS test set was near-perfect, but generalization accuracy was substantially lower, and the dataset showed high sensitivity to random seed.","EMNLP",2020,"Najoung Kim,Tal Linzen",93,58,16,"https://www.semanticscholar.org/paper/b20ddcbd239f3fa9acc603736ac2e4416302d074"
"70a136547d81290b9f4dbc1fac49d31bc010bd3c","Meta-Learning to Compositionally Generalize","A meta-learning augmented version of supervised learning whose objective directly optimizes for out-of-distribution generalization is implemented, and Experimental results on the COGS and SCAN datasets show that this similarity-driven meta- learning can improve generalization performance.","ACL",2021,"Henry Conklin,Bailin Wang,Kenny Smith,Ivan Titov",29,46,5,"https://www.semanticscholar.org/paper/70a136547d81290b9f4dbc1fac49d31bc010bd3c"
"ed535e93d5b5a8b689e861e9c6083a806d1535c2","The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers","By revisiting model configurations as basic as scaling of embeddings, early stopping, relative positional embedding, and Universal Transformer variants, this work can drastically improve the performance of Transformers on systematic generalization.","EMNLP",2021,"R. Csord√°s,Kazuki Irie,J. Schmidhuber",46,55,10,"https://www.semanticscholar.org/paper/ed535e93d5b5a8b689e861e9c6083a806d1535c2"
"557ebd17b7c7ac4e09bd167d7b8909b8d74d1153","Compositional Generalization Requires Compositional Parsers","The accuracy of different 005 parsers on the recent COGS corpus is analyzed and the role of syntactic generalization in compo- 016 sitional generalization is analyzed.","ArXiv",2022,"Pia Weissenhorn,Yuekun Yao,L. Donatelli,Alexander Koller",0,40,0,"https://www.semanticscholar.org/paper/557ebd17b7c7ac4e09bd167d7b8909b8d74d1153"