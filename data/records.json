{"papers":[{"url":"https://www.semanticscholar.org/paper/49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks","venue":"ACL","year":2021,"referenceCount":31,"citationCount":23,"influentialCitationCount":3,"publicationDate":"08/09/2021","authors":"Santiago Ontan'on,J. Ainslie,V. Cvicek,Zachary Kenneth Fisher","citations":[{"paperId":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models"},{"paperId":"97833e2aa0da5240e62436373b58af988a4ab6ab","title":"The Curious Case of Absolute Position Embeddings"},{"paperId":"b49ebf36a29cf9734313066129ab0d7092d4041e","title":"Categorizing Semantic Representations for Neural Machine Translation"},{"paperId":"ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","title":"Compositional Generalisation with Structured Reordering and Fertility Layers"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"61d56ece2d19f4bfeb322c92085fb28521e169da","title":"Neural-Symbolic Recursive Machine for Systematic Generalization"},{"paperId":"837cc9a366c873c84ceec7e84d5cb3d5753757d6","title":"Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks"},{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"a122909a31acf41cb2d9eb602c01b24b9b85a061","title":"LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing"},{"paperId":"aa8f3e081ad2869c9469e2726364bdae0d9bdc7f","title":"Fusing finetuned models for better pretraining"},{"paperId":"5021fd710fd17dee53bc7bc7bf334b148ef3d8b6","title":"LogicInference: A New Dataset for Teaching Logical Inference to seq2seq Models"},{"paperId":"69078af65fc934f81fd340e9d1323d6c08194548","title":"Revisiting the Compositional Generalization Abilities of Neural Sequence Models"},{"paperId":"16bf88a6d172699cb9a26a6936efb4941e3f3c13","title":"An Application of Pseudo-Log-Likelihoods to Natural Language Scoring"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"e528466e2aff981511d4ca6e063211297c0b4175","title":"The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization"},{"paperId":"c6dbd97f458c76925363a6b8f6c5e6198163e54e","title":"The Grammar-Learning Trajectories of Neural Language Models"},{"paperId":"a77468f6bd4db7f8d761a0569d9cc29d5a8f0034","title":"L OGIC I NFERENCE : A N EW D ATASET FOR T EACHING L OGICAL I NFERENCE TO SEQ 2 SEQ M ODELS"},{"paperId":"fcf25e1affc2f8ee5bb49d156f174e9769234deb","title":"Systematic Generalization with Edge Transformers"},{"paperId":"00050c15896e8ae6bb534f10d072351547993f72","title":"LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing"},{"paperId":"3d5699e7f7e085ad72102859b06fa4884d207e77","title":"Iterative Decoding for Compositional Generalization in Transformers"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"269de1d1e26559613fa4b02320aefc07bb2d556b","title":"Enhancing the Transformer Decoder with Transition-based Syntax"},{"paperId":"c735740b26ceaa4db9d77233116434c0e8b311d8","title":"Learning Adaptive Control Flow in Transformers for Improved Systematic Generalization"}],"references":[{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"523745e29f6cb1890f18352d449fd3597910c485","title":"Improving Compositional Generalization in Classification Tasks via Structure Annotations"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":null,"title":"Lexicon learning for few-shot neural sequence modeling"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"56676aef356ebb13cba77fc9e4d70760fbc151f5","title":"ETC: Encoding Long and Structured Inputs in Transformers"},{"paperId":"1b04936c2599e59b120f743fbb30df2eed3fd782","title":"Shortcut Learning in Deep Neural Networks"},{"paperId":"71b6394ad5654f5cd0fba763768ba4e523f7bbca","title":"Longformer: The Long-Document Transformer"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","title":"Reformer: The Efficient Transformer"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"f51497f463566581874c941353dd9d80069c5b77","title":"Compressive Transformers for Long-Range Sequence Modelling"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"815a3d56401483b635cfad9468852cddb46350ee","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"f4238bd2385a52413ccbacfd9e409a650235bd13","title":"Adaptive Attention Span in Transformers"},{"paperId":"203b543bfa1e564bb80ff4229b43174d7c71b0c0","title":"HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization"},{"paperId":"21da617a0f79aabf94272107184606cefe90ab75","title":"Generating Long Sequences with Sparse Transformers"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","title":"Self-Attention with Relative Position Representations"},{"paperId":"08fbb1b4cfdc83977d2c8f08bdfb663f13c0e60a","title":"Memorize or generalize? Searching for a compositional RNN in a haystack"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"595c45a6c4fe895e00742f8316710e1177896deb","title":"Diagnostic Classifiers Revealing how Neural Networks Process Hierarchical Structure"},{"paperId":"b71ac1e9fb49420d13e084ac67254a0bbd40f83f","title":"Understanding the difficulty of training deep feedforward neural networks"},{"paperId":"56010a55d49ac1f42355538f494427fd22402be1","title":"Exploring the Limits"},{"paperId":null,"title":"Add AddNeg Reverse Dup Cart Inters SCAN-l SCAN-aj PCFG-p PCFG-s COGS"}],"id":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","summary":"This paper explores the design space of Transformer models showing that the inductive biases given to the model by several design decisions significantly impact compositional generalization."},{"url":"https://www.semanticscholar.org/paper/ab72bccf6f3981537389510ecc609109e79595c3","title":"Disentangled Sequence to Sequence Learning for Compositional Generalization","venue":"ACL","year":2021,"referenceCount":50,"citationCount":8,"influentialCitationCount":0,"publicationDate":"10/09/2021","authors":"Hao Zheng,Mirella Lapata","citations":[{"paperId":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models"},{"paperId":"ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","title":"Compositional Generalisation with Structured Reordering and Fertility Layers"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"cdce13c7d3f344d0eed77a2437ac6ae635369262","title":"A Simple and Unified Tagging Model with Priming for Relational Structure Predictions"},{"paperId":"1d0ec47feac1a7b5c1ec56b7437e032bcd736a39","title":"Blackbird's language matrices (BLMs): a new benchmark to investigate disentangled generalisation in neural networks"},{"paperId":"557ebd17b7c7ac4e09bd167d7b8909b8d74d1153","title":"Compositional Generalization Requires Compositional Parsers"},{"paperId":"03eeff98d24383518ce0dacc0b3c4a38b6f1a514","title":"Recursive Decoding: A Situated Cognition Approach to Compositional Generation in Grounded Language Understanding"},{"paperId":"90c1a63aada7704eadc4324c16a66ec793d4b698","title":"Compositional generalization with a broad-coverage semantic parser"}],"references":[{"paperId":"95c20f35d352f23b19c378c0758b8dc1d7622872","title":"On Aspects of the Theory of Syntax"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","title":"Lexicon Learning for Few Shot Sequence Modeling"},{"paperId":"88dbde378e9ac5c25fc7d78f5da147223e8d34d4","title":"Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention"},{"paperId":"03ad126cfe495933f7bb769f27c03e5f31caedf8","title":"On Compositional Generalization of Neural Machine Translation"},{"paperId":"cc0fc60e9b9d036acf53899c259574c8ce2aedfc","title":"Compositional Generalization via Semantic Tagging"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"e69e5953905b9b9ded4c07f0505ed401ec39babf","title":"Universal Grammar"},{"paperId":"986cc3d0e3afb23f84564ea9588b7b8e9c3e1dd6","title":"Hierarchical Poset Decoding for Compositional Generalization in Language"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"37882abaec01eba1bf5bda8a36c904aaea0d5642","title":"Improving Compositional Generalization in Semantic Parsing"},{"paperId":"84476fdf6ead3553f4493dff8e02308439d6222b","title":"Improve Transformer Models with Better Relative Position Embeddings"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"0b40141779fafcedc28d83bd678807ddb5980df3","title":"The Pitfalls of Simplicity Bias in Neural Networks"},{"paperId":"9fe497d7c5a60806aae431d443aa155a4325661a","title":"Improving Disentangled Text Representation Learning with Information-Theoretic Guidance"},{"paperId":"5d0e2635a1ebe2c9347529975bc876d4286c9ab7","title":"Distributionally Robust Neural Networks"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"753b7a701adc1b6072378bd048cfa8567885d9c7","title":"Invariant Risk Minimization"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"9c5c794094fbf5da8c48df5c3242615dc0b1d245","title":"Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations"},{"paperId":"ea249d6793de488352858f11169bd718914731e1","title":"Disentangled Representation Learning for Non-Parallel Text Style Transfer"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"3e85fbde18cd4d9bb36aa7f227b84f78a2390cd2","title":"Towards Robust Neural Machine Translation"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","title":"Self-Attention with Relative Position Representations"},{"paperId":"2997b26ffb8c291ce478bd8a6e47979d5a55c466","title":"Annotation Artifacts in Natural Language Inference Data"},{"paperId":"04541599accc47d8174f63345ce9c987ef21685b","title":"Disentangling by Factorising"},{"paperId":"6fe99c4969c2f2d3dde8ada84e7388d74eaf0528","title":"Isolating Sources of Disentanglement in Variational Autoencoders"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"97394554eb5a74c3160c6bd743fcd3e4bd6cbe28","title":"LSDSem 2017 Shared Task: The Story Cloze Test"},{"paperId":"a90226c41b79f8b06007609f39f82757073641e2","title":"beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework"},{"paperId":"b7eac64a8410976759445cce235469163d23ee65","title":"Data Recombination for Neural Semantic Parsing"},{"paperId":"85b68477a6e031d88b963833e15a4b4fc6855264","title":"A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories"},{"paperId":"558ac446dc26bee9789d660a251b75728cb6eeb2","title":"Language to Logical Form with Neural Attention"},{"paperId":null,"title":"2016. A corpus"},{"paperId":null,"title":"2016. Data recombination"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":null,"title":"Autoencoding variational bayes"},{"paperId":"184ac0766262312ba76bbdece4e7ffad0aa8180b","title":"Representation Learning: A Review and New Perspectives"},{"paperId":"1c46943103bd7b7a2c7be86859995a4144d1938b","title":"Visualizing Data using t-SNE"},{"paperId":"a6383f155fa9d3e9b15092bfefbf613f982eb263","title":"The Algebraic Mind: Integrating Connectionism and Cognitive Science"},{"paperId":"07d2993d7b5cce0058c29010d5f85d4f2dc02067","title":"Lexical semantics and compositionality."},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":null,"title":"Die Grundlagen der Arithmetik (The Foundations of Arithmetic): eine logisch- mathematische Untersuchung ber den Begriff der Zahl"}],"id":"ab72bccf6f3981537389510ecc609109e79595c3","summary":"An extension to sequence-to-sequence models which encourage disentanglement by adaptively re-encoding (at each time step) the source input by condition the source representations on the newly decoded target context which makes it easier for the encoder to exploit specialized information for each prediction."},{"url":"https://www.semanticscholar.org/paper/d3edc20ed4a07195f3663abc0ead4220266fd75b","title":"*-CFQ: Analyzing the Scalability of Machine Learning on a Compositional Task","venue":"AAAI","year":2020,"referenceCount":58,"citationCount":10,"influentialCitationCount":1,"publicationDate":"12/15/2020","authors":"D. Tsarkov,Tibor Tihon,Nathan Scales,Nikola Momchev,Danila Sinopalnikov,Nathanael Scharli","citations":[{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"2b060b89324c376892a096c84fd14664f7b71710","title":"Understanding Robust Generalization in Learning Regular Languages"},{"paperId":"4b32ccab244cd8a8036c4d780cba81c9d60929ca","title":"Transformer Module Networks for Systematic Generalization in Visual Question Answering"},{"paperId":"8008348e87d3904842a2dd230c14b83112e8bf48","title":"Compositional Generalization in Dependency Parsing"},{"paperId":"ad331dce175b1d38d6516455013c1ec0e26e606b","title":"Compositional Generalization in Multilingual Semantic Parsing over Wikidata"},{"paperId":"fcf25e1affc2f8ee5bb49d156f174e9769234deb","title":"Systematic Generalization with Edge Transformers"},{"paperId":"bb0ab8591d6d57c7e2bd1ec35d806b3f25277752","title":"Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks"},{"paperId":"676fa805bd715591f99bb17e36d673a6a14e92fe","title":"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization"},{"paperId":"45496cd0b256b75bfbe3bd95890b496069c7821c","title":"Multilingual Compositional Wikidata Questions"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"}],"references":[{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"9b4c9f7714fa60d54feb9527c86ba4dfd9adfa51","title":"Building a Multi-domain Neural Machine Translation Model using Knowledge Distillation"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"d28c18a3c2a0afdc0a8634d18345af8d36e1f948","title":"A Constructive Prediction of the Generalization Error Across Scales"},{"paperId":"ae3501afbe8f3f5cf1c5270fa00d0b65fc1c9484","title":"Environmental drivers of systematicity and generalization in a situated agent"},{"paperId":"77d868cf7a5da2b030e8defba9a2b3804c5720b0","title":"Learning a Multi-Domain Curriculum for Neural Machine Translation"},{"paperId":"815a3d56401483b635cfad9468852cddb46350ee","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":"8195787260dfc6bc9abea3b1dac1ce15f747caa2","title":"A Survey of Unsupervised Deep Domain Adaptation"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"4beaabe0c4277ddb850a2f91a20b5fcec84f18af","title":"Emergent Systematic Generalization in a Situated Agent"},{"paperId":"4d031258a66076187001b4d6182345198624d872","title":"The compositionality of neural networks: integrating symbolism and connectionism"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"07fa8c8a703abd7496f4781e9dee53d5de9c8717","title":"Neural Shuffle-Exchange Networks - Sequence Processing in O(n log n) Time"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"636904d91d9dd1a641a595d9578ba7640f35aa74","title":"MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension"},{"paperId":"ec9984003962eb70a73bf0882ab49ef38cd6c239","title":"Curriculum Learning for Domain Adaptation in Neural Machine Translation"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"19b7769dab4e6092aa4b7eeb8aa078a7b725c9b4","title":"Relational inductive biases, deep learning, and graph networks"},{"paperId":"39af58c32e76e875d667804707ea110323207988","title":"A Survey of Domain Adaptation for Neural Machine Translation"},{"paperId":"1b362a75b40a0242bfd7996b02ccc0815edd18df","title":"Multi-Domain Neural Machine Translation"},{"paperId":"642c1b4a9da95ea4239708afc5929a5007a1870d","title":"Tensor2Tensor for Neural Machine Translation"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"a1c922be467d1c0c64b963e65dae41778b81b2a0","title":"Deep Learning Scaling is Predictable, Empirically"},{"paperId":"c80725ad0c0cd06416f3c01a78b7c419359d3fe2","title":"Instance Weighting for Neural Machine Translation Domain Adaptation"},{"paperId":"a5ae9a23a587dee02aedeb3ec271de0da95eabe4","title":"Effective Domain Mixing for Neural Machine Translation"},{"paperId":"8760bc7631c0cb04e7138254e9fd6451b7def8ca","title":"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"bc7fcefa3e333d50463d524406d107060c4a0cec","title":"Neural Semantic Parsing over Multiple Knowledge-bases"},{"paperId":"a21de9f6408b333d917f7a8b2585230ed8b6c57a","title":"How much data is needed to train a medical image deep learning system to achieve necessary high accuracy"},{"paperId":"b4482761879009635e04d170f9f9c70a74f4ba39","title":"A Unified Perspective on Multi-Domain and Multi-Task Learning"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"2826f9dccdcceb113b33ccf2841d488f1419bb30","title":"Stanford Neural Machine Translation Systems for Spoken Language Domains"},{"paperId":"2f31cfb36a8d0c9b2039eee1b5848f618d1a50dc","title":"Predicting sample size required for classification performance"},{"paperId":"6e785a402a60353e6e22d6883d3998940dcaea96","title":"Three Models for the Description of Language"},{"paperId":"7628b62d64d2e5c33a13a5a473bc41b2391c1ebc","title":"Scaling to Very Very Large Corpora for Natural Language Disambiguation"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":null,"title":"Was a movie whose art director and executive producer influenced and was influenced by a film distributor's employee and founder M1? 8. Who was the Dutch sibling and husband of M0?"},{"paperId":null,"title":"Was M2's Hindu spouse a English wife of M3?"},{"paperId":null,"title":"Who was a parent of a company's white Catholic Canadian founder?"},{"paperId":null,"title":"Was M2's child's friend, offspring, parent, wife, and sibling M0's child and parent?"},{"paperId":null,"title":"Was the woman's spouse, sibling, father, and child M2? 17. Who was the child, sibling, parent, and husband of M2's friend and sister?"},{"paperId":null,"title":"Was M2 a daughter, spouse, and sibling of the thriller movie's executive producer, costume designer, director, cinematographer, and star?"},{"paperId":null,"title":"Which Buddhist Mormon male Muslim parent of M6's star was M1's Hindu spouse?"},{"paperId":null,"title":"What female Irish American Indian child of a cinematographer did M2 and M3 employ?"},{"paperId":null,"title":"Was a Hindu costume designer's parent, spouse, and son a Mexican child of the creator and producer of M2?"},{"paperId":null,"title":"Was M3 a actor whose parent and wife distributed and produced M0 and M1?"},{"paperId":null,"title":"Was a Hindu Jewish Chinese Irish American brother of a drama film's producer's child M2?"},{"paperId":null,"title":"Was M2's producer's Buddhist sibling a Jewish TV director's daughter?"},{"paperId":null,"title":"Was M0's art director, costume designer, executive producer, producer, writer, editor, and star M2's mother? 19. Was the man the executive producer of M1, M2, M3, M4, and M5?"},{"paperId":null,"title":"Who was M2's Hindu costume designer's husband, sibling, friend, and parent?"}],"id":"d3edc20ed4a07195f3663abc0ead4220266fd75b","summary":"It is shown that compositional generalization remains a challenge at all training sizes, and that increasing the scope of natural language leads to consistently higher error rates, which are only partially offset by increased training data."},{"url":"https://www.semanticscholar.org/paper/9fe39e0f2cf3b6d5f70a379654f9c08ffa48ddee","title":"Unobserved Local Structures Make Compositional Generalization Hard","venue":"","year":2022,"referenceCount":30,"citationCount":9,"influentialCitationCount":1,"publicationDate":"01/15/2022","authors":"Ben Bogin,Shivanshu Gupta,Jonathan Berant","citations":[{"paperId":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models"},{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"}],"references":[{"paperId":"06fbeaf4d16639f177973a06cd7c4f78cb5e38ed","title":"COVR: A Test-Bed for Visually Grounded Compositional Generalization with Real Images"},{"paperId":"676fa805bd715591f99bb17e36d673a6a14e92fe","title":"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":"df66fcd3fd4f0d55bd96528cddb0e2f08ddb3385","title":"Revisiting Iterative Back-Translation from the Perspective of Compositional Generalization"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"b95184d5eb25b0fe66d8bd1ad1b7677a51c21702","title":"Latent Compositional Representations Improve Systematic Generalization in Grounded Question Answering"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"37882abaec01eba1bf5bda8a36c904aaea0d5642","title":"Improving Compositional Generalization in Semantic Parsing"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"4bc2bb6584774b0d8ad0b4f5215dc2075487c192","title":"A Benchmark for Systematic Generalization in Grounded Language Understanding"},{"paperId":"94f11f6cf04477fb38419f3ec4af097f57abc741","title":"Schema2QA: High-Quality and Low-Cost Q&A Agents for the Structured Web"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"80deaca65c2c155bd15718eeecff584841eb25b0","title":"CLOSURE: Assessing Systematic Generalization of CLEVR Models"},{"paperId":"f906264694759f1beda0cb07d02bf098b98c17bb","title":"Genie: a generator of natural language semantic parsers for virtual assistant commands"},{"paperId":"33ecb49e7b1eb1f44790fb6ceca6eed82cb0c7cd","title":"Jump to better conclusions: SCAN both left and right"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"ba30df190664193514d1d309cb673728ed48f449","title":"Incorporating Copying Mechanism in Sequence-to-Sequence Learning"},{"paperId":"25369f56a933e3bfb1d8e1588cdc6c50df93ecae","title":"Building a Semantic Parser Overnight"},{"paperId":"3ecd3e00bbbfd94446c3adc9c6878de27e250f7c","title":"Learning Dependency-Based Compositional Semantics"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"6dae782f8be8fe5c25dbf2d0d681b3f708bf1a32","title":"Expanding the Scope of the ATIS Task: The ATIS-3 Corpus"},{"paperId":"1d19708290ef3cc3f43c2c95b07acdd4f52f5cda","title":"The ATIS Spoken Language Systems Pilot Corpus"}],"id":"9fe39e0f2cf3b6d5f70a379654f9c08ffa48ddee","summary":"A criterion for the difficulty of an example is proposed: a test instance is hard if it contains a local structure that was not observed at training time and it predicts instance-level generalization well across 5 different semantic parsing datasets, substantially better than alternative decision rules."},{"url":"https://www.semanticscholar.org/paper/40047a74b707743157051d38f76061ba5ff9aab4","title":"Compositional Semantic Parsing with Large Language Models","venue":"ArXiv","year":2022,"referenceCount":62,"citationCount":3,"influentialCitationCount":0,"publicationDate":"09/29/2022","authors":"Andrew Drozdov,Nathanael Scharli,Ekin Akyuurek,Nathan Scales,Xinying Song,Xinyun Chen,O. Bousquet,Denny Zhou","citations":[{"paperId":"cca80d8d5edb0513b5ac744e6854744fa4aeb4b8","title":"Transcending Scaling Laws with 0.1% Extra Compute"},{"paperId":"663a41c866d49ce052801fbc88947d39764cad29","title":"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"},{"paperId":"95add249447a03653fc76921efc3691be1dcd204","title":"Large Language Models are few(1)-shot Table Reasoners"}],"references":[{"paperId":"d429373fb5190778856da933564d0d52092cc9bd","title":"Emergent Abilities of Large Language Models"},{"paperId":"4288d44c2b8e6a89607780caf1272061028f6f97","title":"On the Advance of Making Language Models Better Reasoners"},{"paperId":"1ee6a0b12264001c65b83536efcddac7f27e8571","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"f75a0ccb3c9b7e12dfb8b9c9dc3d35ca4518a643","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"51c2a0835fc565ad1fc7a58559ede9cbe8f6551e","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"716f9d0f6e96f437e127de90c87f7b2f7a6c8f12","title":"SeqZero: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models"},{"paperId":"a40693eefd351659cdeb3885917b1506ea01c38a","title":"Measuring and Improving Compositional Generalization in Text-to-SQL via Component Alignment"},{"paperId":"bcd4c46e4d75ddedb6138cfd77600c6d964a9aa8","title":"Natural Language to Code Translation with Execution"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"cb5e3f085caefd1f3d5e08637ab55d39e61234fc","title":"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"},{"paperId":"23dd78e424d32f6a48660dcd67ce994b8a7db8be","title":"STaR: Bootstrapping Reasoning With Reasoning"},{"paperId":"79b88230fabb59a1d368641bbc822af0f09bf262","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"5d0db797a45ce2453f821f7ded0b547d3fdab054","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a","title":"Few-Shot Semantic Parsing with Language Models Trained on Code"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"7cc74ffa1215321712d4a830bb9dee19d9f0fb47","title":"Grounded Graph Decoding Improves Compositional Generalization in Question Answering"},{"paperId":"b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347","title":"Sequence-to-Sequence Learning with Latent Neural Grammars"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","title":"Lexicon Learning for Few Shot Sequence Modeling"},{"paperId":"88dbde378e9ac5c25fc7d78f5da147223e8d34d4","title":"Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention"},{"paperId":"64a1dbdd7653eaca25c78e87335ee156b6f6959e","title":"Constrained Language Models Yield Few-Shot Semantic Parsers"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":"62d1a3137b01a69443bebf4d92c1990ec512a6a1","title":"Extracting Training Data from Large Language Models"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"e69e5953905b9b9ded4c07f0505ed401ec39babf","title":"Universal Grammar"},{"paperId":"986cc3d0e3afb23f84564ea9588b7b8e9c3e1dd6","title":"Hierarchical Poset Decoding for Compositional Generalization in Language"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"937fef6a786c4463a3bb19770c704945d1600b66","title":"Learning Compositional Rules via Neural Program Synthesis"},{"paperId":"4bc2bb6584774b0d8ad0b4f5215dc2075487c192","title":"A Benchmark for Systematic Generalization in Grounded Language Understanding"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"ac713aebdcc06f15f8ea61e1140bb360341fdf27","title":"Thieves on Sesame Street! Model Extraction of BERT-based APIs"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"80deaca65c2c155bd15718eeecff584841eb25b0","title":"CLOSURE: Assessing Systematic Generalization of CLEVR Models"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"32c9a0acee8d236c553395052c29a6d853d8ea2d","title":"Compositional Generalization in Image Captioning"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"210feb22ff541920caa4884e73eaff1c09644114","title":"Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"03eb382e04cca8cca743f7799070869954f1402a","title":"CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"},{"paperId":"7260c0692f8d265e11c4e9c4c8ef4c185bd587ad","title":"Building machines that learn and think like people"},{"paperId":"c2ecc66c0e5f976b0e0d95c64ed2d1e283a2625d","title":"Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus"},{"paperId":"6843890926bf0e5c887ffc78dcb1203135981bf1","title":"The compositionality papers"},{"paperId":"d65f4afadac11cd2ec6f0b14f244060c387926ce","title":"An introduction to unification-based approaches to grammar"},{"paperId":null,"title":"Syntactic structures. The Hague: Mouton"},{"paperId":null,"title":"?x0 produced M1 . ?x0 wrote M0 } Question: Did M2 's editor , director , star , art director , and cinematographer produce , direct , and edit M0 and M1 Answer: SELECT count(*) WHERE { ?x0 edited M0"},{"paperId":null,"title":"Static prompt context illustrating the composition of subclauses and prepositional phrases 2. Dynamically selected exemplars as additional context 3"},{"paperId":null,"title":"Adds sequential least-to-most prompting, including subproblems in the prompt that were used to select the exemplars"},{"paperId":null,"title":"Chain-of-thought (CoT) grounding. A constant prompt prefix with basic subproblems solved in a chain-of-thought-like way. This corresponds to the full implementation of dynamic least-to-most prompting"},{"paperId":null,"title":"3 COGS SOLUTION: DETAILS AND PROMPTS"},{"paperId":null,"title":"The * raisin) (was frozen [freeze]) PARSE: freeze ( theme = * raisin ) DONE"},{"paperId":null,"title":"Same as above, but adds brackets to sentences indicating compositional structure and hinting at relations between exemplars (see Appendix A for examples"},{"paperId":null,"title":"These parts are detailed throughout the rest of this section. input: Who was influenced by M1 , influenced by M4 's producer , cinematographer , and director"}],"id":"40047a74b707743157051d38f76061ba5ff9aab4","summary":"The best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse."},{"url":"https://www.semanticscholar.org/paper/6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization","venue":"FINDINGS","year":2021,"referenceCount":59,"citationCount":13,"influentialCitationCount":5,"publicationDate":"07/14/2021","authors":"Chenyao Liu,Shengnan An,Zeqi Lin,Qian Liu,Bei Chen,Jian-Guang Lou,L. Wen,Nanning Zheng,Dongmei Zhang","citations":[{"paperId":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models"},{"paperId":"04ca71d9089b2e86f9e4a874fd66ee8bd0baba8e","title":"When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks"},{"paperId":"1bd799cf462f926041dd2fc8fbe4af54bddbf5c5","title":"Translate First Reorder Later: Leveraging Monotonicity in Semantic Parsing"},{"paperId":"ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","title":"Compositional Generalisation with Structured Reordering and Fertility Layers"},{"paperId":"40047a74b707743157051d38f76061ba5ff9aab4","title":"Compositional Semantic Parsing with Large Language Models"},{"paperId":"a122909a31acf41cb2d9eb602c01b24b9b85a061","title":"LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing"},{"paperId":"1167b3864046b732cf057b8b05db311e726cadab","title":"Measuring Alignment Bias in Neural Seq2seq Semantic Parsers"},{"paperId":"557ebd17b7c7ac4e09bd167d7b8909b8d74d1153","title":"Compositional Generalization Requires Compositional Parsers"},{"paperId":"39f604fdd3ade5bd5a67d5284a6d9c12e535db85","title":"Compositionality as Lexical Symmetry"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"90c1a63aada7704eadc4324c16a66ec793d4b698","title":"Compositional generalization with a broad-coverage semantic parser"},{"paperId":"c764ecba2bace12b9bfb9c2b0651a12ff6888ea7","title":"Learning to Generalize Compositionally by Transferring Across Semantic Parsing Tasks"},{"paperId":"00050c15896e8ae6bb534f10d072351547993f72","title":"LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing"}],"references":[{"paperId":"d3edc20ed4a07195f3663abc0ead4220266fd75b","title":"*-CFQ: Analyzing the Scalability of Machine Learning on a Compositional Task"},{"paperId":"7344ed64d1717780422fd1d58fae85edc544d180","title":"Iterative Utterance Segmentation for Neural Semantic Parsing"},{"paperId":"df66fcd3fd4f0d55bd96528cddb0e2f08ddb3385","title":"Revisiting Iterative Back-Translation from the Perspective of Compositional Generalization"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"e69e5953905b9b9ded4c07f0505ed401ec39babf","title":"Universal Grammar"},{"paperId":"106fb432d2b62f3824a9d6f4a1b30e1f8b6ea9d7","title":"Sequence-level Mixed Sample Data Augmentation"},{"paperId":"4bc9d6596069c9277b57a7ee1e1127d231f28663","title":"Unsupervised Parsing with S-DIORA: Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders"},{"paperId":"986cc3d0e3afb23f84564ea9588b7b8e9c3e1dd6","title":"Hierarchical Poset Decoding for Compositional Generalization in Language"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"07fa8c8a703abd7496f4781e9dee53d5de9c8717","title":"Neural Shuffle-Exchange Networks - Sequence Processing in O(n log n) Time"},{"paperId":"7ea59779ffb392f099d5304680126b4299f43750","title":"Compound Probabilistic Context-Free Grammars for Grammar Induction"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"b0e2fe0fe9f4fc4ce05d5f637baff96a7e966c01","title":"Cooperative Learning of Disjoint Syntax and Semantics"},{"paperId":"16c844fd4d97f3c6eb38b0d6527c87d184efedc3","title":"The Evolved Transformer"},{"paperId":"15d6f3d815d0ff176fafb14a3f46e5723ebac723","title":"Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"027f9695189355d18ec6be8e48f3d23ea25db35d","title":"Learning to Compose Task-Specific Tree Structures"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"e1a25af38d1f8ba9b2b5c3256973d6cf4fe00559","title":"Grammar induction from (lots of) words alone"},{"paperId":"387f65f42b26479169bab9e58ee55f838cc31197","title":"Imitation Learning of Agenda-based Semantic Parsers"},{"paperId":"b41e95c8c97846d5ca4c11ef79d7814499cc9663","title":"Compositional Semantic Parsing on Semi-Structured Tables"},{"paperId":"32de44f01a96d4473d21099d15e25bc2b9f08e2f","title":"Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"b29447ba499507a259ae9d8f685d60cc1597d7d3","title":"Semantic Parsing on Freebase from Question-Answer Pairs"},{"paperId":"8729441d734782c3ed532a7d2d9611b438c0a09a","title":"ADADELTA: An Adaptive Learning Rate Method"},{"paperId":null,"title":"φa + γ ·E R(τ)∇ log πφa (ga|x, z, gl)"},{"paperId":"4004494592b288e135602146ab290148c8ff58d8","title":"Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models"},{"paperId":"3ecd3e00bbbfd94446c3adc9c6878de27e250f7c","title":"Learning Dependency-Based Compositional Semantics"},{"paperId":"8de174ab5419b9d3127695405efd079808e956e8","title":"Curriculum learning"},{"paperId":"42a9c575acb53fac332993087c1e1dbcc8161ccd","title":"An All-Subtrees Approach to Unsupervised Parsing"},{"paperId":"74fe7ec751cd50295b15cfd46389a8fefb37c414","title":"Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars"},{"paperId":"8dd9fd6a45afd266d48255c398429e01ea4fd6db","title":"Learning to Transform Natural to Formal Languages"},{"paperId":"4c915c1eecb217c123a36dc6d3ce52d12c742614","title":"Simple statistical gradient-following algorithms for connectionist reinforcement learning"},{"paperId":"622042fca4fe303178c87b934e6cc1e5459f33c4","title":"Context Dependence and Compositionality"},{"paperId":"de2df29b0a0312de7270c3f5a0af6af5645cf91a","title":"A Systematic Comparison of Various Statistical Alignment Models"},{"paperId":"97f95249ce73188a9488de160ddd0410d5c4fb5f","title":"MEANING POTENTIALS AND CONTEXT : SOME CONSEQUENCES FOR THE ANALYSIS OF VARIATION IN MEANING"},{"paperId":"6843890926bf0e5c887ffc78dcb1203135981bf1","title":"The compositionality papers"},{"paperId":"4d92df4a844c94fbb31b95157488e4b562b4f681","title":"The Optimal Reward Baseline for Gradient-Based Reinforcement Learning"},{"paperId":"5feb019135e705641380ec0dd9be8cc7bdcd973b","title":"Natural Language Grammar Induction Using a Constituent-Context Model"},{"paperId":"a6383f155fa9d3e9b15092bfefbf613f982eb263","title":"The Algebraic Mind: Integrating Connectionism and Cognitive Science"},{"paperId":"a20f0ce0616def7cc9a87446c228906cd5da093b","title":"Policy Gradient Methods for Reinforcement Learning with Function Approximation"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"b7c0e47f8b768258b7d536c21b218e6c46ab8791","title":"Learning to Parse Database Queries Using Inductive Logic Programming"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":null,"title":"Syntactic structures (the hague: Mouton, 1957)"},{"paperId":null,"title":"Joshua liked that Mason hoped that Amelia awarded the hedgehog beside the stage in the tent to a cat\". Figure 6: Examples of generated tree-structures and semantics in CFQ and COGS benchmarks"},{"paperId":null,"title":"An example of generated results in CFQ benchmark with the input \"Did M6' s star, costume designer, and director influence M0, M1, M2, and M3 and influence M4 and M5"}],"id":"6d00b1024298e5b64ee873028385f7bb4396b05d","summary":"This paper proposes LEAR, an end-toend neural model to learn algebraic recombination for compositional generalization, to model the semantic parsing task as a homomorphism between a latent syntactic algebra and a semantic algebra, thus encouraging algebraic rewriting."},{"url":"https://www.semanticscholar.org/paper/856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks","venue":"ICML","year":2017,"referenceCount":50,"citationCount":463,"influentialCitationCount":74,"publicationDate":"10/31/2017","authors":"B. Lake,Marco Baroni","citations":[{"paperId":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models"},{"paperId":"04ca71d9089b2e86f9e4a874fd66ee8bd0baba8e","title":"When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks"},{"paperId":"97833e2aa0da5240e62436373b58af988a4ab6ab","title":"The Curious Case of Absolute Position Embeddings"},{"paperId":"798abf86efae9e37b9b6a694ef87b6c1dbaab263","title":"Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models"},{"paperId":"c6e4518dfd687a2a5bed4e78d5d9f999292a1746","title":"Counterfactual Recipe Generation: Exploring Compositional Generalization in a Realistic Scenario"},{"paperId":"a4f1793b23a7f7ec3d65824d6ca3e6011567980d","title":"ELASTIC: Numerical Reasoning with Adaptive Symbolic Compiler"},{"paperId":"a638036c48cbd76ae3af30d6b273d49bb28a22e4","title":"Schrödinger's tree—On syntax and neural language models"},{"paperId":"6494c6149e5036c09ee92da9fd67cbecc998a52f","title":"PCFG-based Natural Language Interface Improves Generalization for Controlled Text Generation"},{"paperId":"a3cddf0b160b1d9b585e74bc114b403fc8590264","title":"Equi-Tuning: Group Equivariant Fine-Tuning of Pretrained Models"},{"paperId":"1ed29beb55b10de8553c926ce6da2625ec2c8776","title":"Benchmarking Long-tail Generalization with Likelihood Splits"},{"paperId":"b49ebf36a29cf9734313066129ab0d7092d4041e","title":"Categorizing Semantic Representations for Neural Machine Translation"},{"paperId":"b1f33e956e36bf25e118c0d537dcc519cfe52e60","title":"CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations"},{"paperId":"1bd799cf462f926041dd2fc8fbe4af54bddbf5c5","title":"Translate First Reorder Later: Leveraging Monotonicity in Semantic Parsing"},{"paperId":"eac5e5ef2bb1d158645ee8ae8f3e167767316b46","title":"Understanding and Improving Zero-shot Multi-hop Reasoning in Generative Question Answering"},{"paperId":"0828722a8317a556c8753cfe1a8cf3a3eec0004f","title":"Measuring and Narrowing the Compositionality Gap in Language Models"},{"paperId":"ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","title":"Compositional Generalisation with Structured Reordering and Fertility Layers"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"61d56ece2d19f4bfeb322c92085fb28521e169da","title":"Neural-Symbolic Recursive Machine for Systematic Generalization"},{"paperId":"efbfca5e8f922a9d804c388719ebf8fcf07bd0de","title":"Curriculum learning for human compositional generalization"},{"paperId":"837cc9a366c873c84ceec7e84d5cb3d5753757d6","title":"Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks"},{"paperId":"40047a74b707743157051d38f76061ba5ff9aab4","title":"Compositional Semantic Parsing with Large Language Models"},{"paperId":"ff4e8de2705773c2dd38365a28ffe05b32706efb","title":"Equivariant Transduction through Invariant Alignment"},{"paperId":"a265394d782bbdb869730775399be3dfe45fc1db","title":"Compositional generalization through abstract representations in human and artificial neural networks"},{"paperId":"60cd45dc61bc41456e00a5f48d8589674cc9fdf7","title":"Trust in Language Grounding: a new AI challenge for human-robot teams"},{"paperId":"60f208b19bb63d82fda5759897677f92d4e1e2fc","title":"Improving Compositional Generalization in Math Word Problem Solving"},{"paperId":"d6236d1ad7e5678c3537f8d74f63e678b7015250","title":"PU-GEN: Enhancing generative commonsense reasoning for language models with human-centered knowledge"},{"paperId":"c4899af5e101b84ada1acfed60ec90eb5113f4a5","title":"On a Built-in Conflict between Deep Learning and Systematic Generalization"},{"paperId":"01b2f7601ab3df0d2982a204e2fb309f6622646f","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models"},{"paperId":"13babb4aa168b26d5330775dd87582a0e0efad4f","title":"Learning Transductions to Test Systematic Compositionality"},{"paperId":"a6be79aa22eaeef05caae66aab924bcb4e07cfa8","title":"Benchmarking Compositionality with Formal Languages"},{"paperId":"1a7a24c73521eecf0a2d555e921b27e2c4d8e3c3","title":"What Artificial Neural Networks Can Tell Us About Human Language Acquisition"},{"paperId":"74cc3d340039c67bdabaef090d1386fe2c5376ca","title":"CLEVR-Math: A Dataset for Compositional Language, Visual and Mathematical Reasoning"},{"paperId":"22f9a5fe8e446d215530fb90ea08b10499b36b0b","title":"Unit Testing for Concepts in Neural Networks"},{"paperId":"294b7f9927f10e944a3d33f548d9b92792f5df84","title":"Interpolation, extrapolation, and local generalization in common neural networks"},{"paperId":"d045f97124c9fb64f5d5f4d77da85eb4df502667","title":"Meta-Referential Games to Learn Compositional Learning Behaviours"},{"paperId":"9337d750993d8715c872db8d406480d58464555a","title":"How to Reuse and Compose Knowledge for a Lifetime of Tasks: A Survey on Continual Learning and Functional Composition"},{"paperId":"8e21576387f46f1b9090bdbff1ceadf187feeada","title":"CompoSuite: A Compositional Reinforcement Learning Benchmark"},{"paperId":"7821736570438c2d4db4e9cb4e0c04182b20b605","title":"Inferring the nature of linguistic computations in the brain"},{"paperId":"0b3ca2a700085a877c560e20558566536f18d5a2","title":"Modular Lifelong Reinforcement Learning via Neural Composition"},{"paperId":"196d1820f48e8fc6fc5275adb76d4c8730359917","title":"A Benchmark for Compositional Visual Reasoning"},{"paperId":"60584b10f794ed74fac7990b8fcbb7f4ff93d73e","title":"Defending Compositionality in Emergent Languages"},{"paperId":"e10ed48cceca216d8ac43113c0562cf340dbdce3","title":"Unveiling Transformers with LEGO: a synthetic reasoning task"},{"paperId":"ba6ff2c50862a6be18d1c610d1fadc751fd6d8e7","title":"Few-shot Subgoal Planning with Language Models"},{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"51c2a0835fc565ad1fc7a58559ede9cbe8f6551e","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"a122909a31acf41cb2d9eb602c01b24b9b85a061","title":"LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing"},{"paperId":"1167b3864046b732cf057b8b05db311e726cadab","title":"Measuring Alignment Bias in Neural Seq2seq Semantic Parsers"},{"paperId":"e184d1113060bbeec171b2faf0db0a4022515859","title":"TreeMix: Compositional Constituency-based Data Augmentation for Natural Language Understanding"},{"paperId":"bc16284f517dd0011dcf64ea1c8fe6d6576494a4","title":"Is the Computation of Abstract Sameness Relations Human-Like in Neural Language Models?"},{"paperId":"a40693eefd351659cdeb3885917b1506ea01c38a","title":"Measuring and Improving Compositional Generalization in Text-to-SQL via Component Alignment"},{"paperId":"b06c41432dc060c5591b91f8ed40ae15913a150d","title":"SUBS: Subtree Substitution for Compositional Semantic Parsing"},{"paperId":"f2611a09cf0942170785ee3025cb511de3bdec2e","title":"Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems"},{"paperId":"919cebaad8a2f98ec5d6fb540f7dd17bac9e9ef8","title":"Toward Compositional Generalization in Object-Oriented World Modeling"},{"paperId":"106b55f668eb9be1c2558095b22251c1be23618b","title":"Counterfactual Explanations for Natural Language Interfaces"},{"paperId":"cb16b85891172572cd856142880b503db0c2bc61","title":"What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment"},{"paperId":"997410e2bf80f25f73752dd6fd7122227385ed2d","title":"Measuring Compositional Consistency for Video Question Answering"},{"paperId":"6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis"},{"paperId":"0ea06fdde94a7d5e8a49a0b4f68799dd102d7a03","title":"Learning to Compose Soft Prompts for Compositional Zero-Shot Learning"},{"paperId":"0d156c24aa1adba4d04b0b4717aba547887a21af","title":"Disentangling Abstraction from Statistical Pattern Matching in Human and Machine Learning"},{"paperId":"376726c82ccc30f59d01a7c609ffbda6db42abeb","title":"Learning Reduplication with a Neural Network that Lacks Explicit Variables"},{"paperId":"5021fd710fd17dee53bc7bc7bf334b148ef3d8b6","title":"LogicInference: A New Dataset for Teaching Logical Inference to seq2seq Models"},{"paperId":"260a57327415c0a498f0b27da9e4311fa78902c6","title":"Compositional Temporal Grounding with Structured Variational Cross-Graph Correspondence Learning"},{"paperId":"59494dcb572cebb577a1bcb2d6f87dfca93d6591","title":"Differentiable Reasoning over Long Stories - Assessing Systematic Generalisation in Neural Models"},{"paperId":"66f3f0e8ebc780e570770986f50bf9cb9cd53ec1","title":"WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series Tasks"},{"paperId":"4c430e6c3a72626bd4cb1893960c7c26dfec6c79","title":"Structurally Diverse Sampling Reduces Spurious Correlations in Semantic Parsing Datasets"},{"paperId":"e0b63d7870fd2f6aeee4d06ae8c0e849846df59f","title":"Zipfian environments for Reinforcement Learning"},{"paperId":"69078af65fc934f81fd340e9d1323d6c08194548","title":"Revisiting the Compositional Generalization Abilities of Neural Sequence Models"},{"paperId":"1d41a0ddda57caa6c8d268dd1703e4c9b35db18b","title":"One-Shot Learning from a Demonstration with Hierarchical Latent Language"},{"paperId":"bd4cff8ae98125958e865531150c4a8ad66153d0","title":"Neuro-symbolic Natural Logic with Introspective Revision for Natural Language Inference"},{"paperId":"557ebd17b7c7ac4e09bd167d7b8909b8d74d1153","title":"Compositional Generalization Requires Compositional Parsers"},{"paperId":"69df5b68fbf492341336b39b4cc9fcc74fff4d5f","title":"Improving Systematic Generalization Through Modularity and Augmentation"},{"paperId":"2b060b89324c376892a096c84fd14664f7b71710","title":"Understanding Robust Generalization in Learning Regular Languages"},{"paperId":"e824da3543603c99da0066469d7fd413059a3c68","title":"Do Transformers use variable binding?"},{"paperId":"39f604fdd3ade5bd5a67d5284a6d9c12e535db85","title":"Compositionality as Lexical Symmetry"},{"paperId":"03eeff98d24383518ce0dacc0b3c4a38b6f1a514","title":"Recursive Decoding: A Situated Cognition Approach to Compositional Generation in Grounded Language Understanding"},{"paperId":"4b32ccab244cd8a8036c4d780cba81c9d60929ca","title":"Transformer Module Networks for Systematic Generalization in Visual Question Answering"},{"paperId":"91a25facc0829c320f274e0196b8abc58b72f035","title":"Learning Invariable Semantical Representation from Language for Extensible Policy Generalization"},{"paperId":"9fe39e0f2cf3b6d5f70a379654f9c08ffa48ddee","title":"Unobserved Local Structures Make Compositional Generalization Hard"},{"paperId":"ace2a00425f96e9d0dbbe2869023d56c6c91267f","title":"On Learning Interpreted Languages with Recurrent Models"},{"paperId":"790f75e70a42c661b60b5d929c6e48a7f405a88c","title":"Data-driven Model Generalizability in Crosslinguistic Low-resource Morphological Segmentation"},{"paperId":"3743a90fa56345d92a07b18400c0e513e2fdc3cd","title":"Does entity abstraction help generative Transformers reason?"},{"paperId":"c3e7f95e7d1e37003bbef8dca1aee6dcf05a5c16","title":"Learning Bounded Context-Free-Grammar via LSTM and the Transformer: Difference and Explanations"},{"paperId":"63f17017257063ee034c4082d93005dc4b25d42d","title":"Pushing the Limits of Rule Reasoning in Transformers through Natural Language Satisfiability"},{"paperId":"0b483b550b21ec42d693fc04a372dbb10dd07019","title":"Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"40b4d98588719407fb72a014ab79e4145695654b","title":"Quantifying Adaptability in Pre-trained Language Models with 500 Tasks"},{"paperId":"8a221ca3a48727ccaec03133c8973469ef66e92b","title":"Neurosymbolic Systems of Perception and Cognition: The Role of Attention"},{"paperId":"ba2c5e980733378cc4e63fc34e6e2b54331c3809","title":"Building human-like communicative intelligence: A grounded perspective"},{"paperId":"ccd04c27bf1237368b35eb456b3dd1c18ef9a9b9","title":"Inducing Causal Structure for Interpretable Neural Networks"},{"paperId":"37c8f0f4915b68f94669d7eeb51b4785b35c70de","title":"Dyna-bAbI: unlocking bAbI’s potential with dynamic synthetic benchmarking"},{"paperId":"ea3fa9fb842e055c3c5e231838b548c7aaa90fa2","title":"NeuroLISP: High-level symbolic programming with attractor neural networks"},{"paperId":"b8b813111c411ae61881ab9cd25707d9de6444ec","title":"Compositional Attention: Disentangling Search and Retrieval"},{"paperId":"daa5ca0a39ecec8ab5c534196eca526bafe41051","title":"Illiterate DALL-E Learns to Compose"},{"paperId":"dbeff5429ff0caa85f9e02621928e787e789ca2b","title":"Hey AI, Can You Solve Complex Tasks by Talking to Agents?"},{"paperId":"6a9394e5d49c1251c0fb6d7fb0c0813d26c6a907","title":"On The Ingredients of an Effective Zero-shot Semantic Parser"},{"paperId":"6bd91a3183ddb844641acb9f3fe9faec6a9ff617","title":"Meta-learning via Language Model In-context Tuning"},{"paperId":"e528466e2aff981511d4ca6e063211297c0b4175","title":"The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization"},{"paperId":"8008348e87d3904842a2dd230c14b83112e8bf48","title":"Compositional Generalization in Dependency Parsing"},{"paperId":"ab72bccf6f3981537389510ecc609109e79595c3","title":"Disentangled Sequence to Sequence Learning for Compositional Generalization"},{"paperId":"a38253162bdaab2c6d0ae7b3e295913b982c7c87","title":"Distinguishing rule- and exemplar-based generalization in learning systems"},{"paperId":"00ef52092ef3f109a09b66037707cd3227accb42","title":"Challenges in Generalization in Open Domain Question Answering"},{"paperId":"b3f644a5ea1fdd8cec1c34ebed69125838a50de3","title":"The Paradox of the Compositionality of Natural Language: A Neural Machine Translation Case Study"},{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"ad331dce175b1d38d6516455013c1ec0e26e606b","title":"Compositional Generalization in Multilingual Semantic Parsing over Wikidata"},{"paperId":"99425e899f9cfca016e87fe38e044df544215e53","title":"The Structure of Systematicity in the Brain"},{"paperId":"61b1171efcf0242eb011816de1aa415f4262c55a","title":"Visual Representation Learning Does Not Generalize Strongly Within the Same Domain"},{"paperId":"7e38476342ce1fcc8ef0dcd23686539395961769","title":"Inductive biases for deep learning of higher-level cognition"},{"paperId":"2aa1d4350e80613feed88d5a6337e79693f7aa57","title":"KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base"},{"paperId":"f8f00ac17b8facf855f1ecf851ddb93240469c28","title":"Hangul Fonts Dataset: a Hierarchical and Compositional Dataset for Interrogating Learned Representations"},{"paperId":"db0fedb5be998be95ef2f33d327e10ab16f79735","title":"A Dog Is Passing Over The Jet? A Text-Generation Dataset for Korean Commonsense Reasoning and Evaluation"},{"paperId":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"DECOMPOSITION IN NEURAL PROGRAM SYNTHESIS"},{"paperId":"90c1a63aada7704eadc4324c16a66ec793d4b698","title":"Compositional generalization with a broad-coverage semantic parser"},{"paperId":"3e60cba99b4e8a45f2e3ba3df462ac949a720833","title":"Towards Collaborative Neural-Symbolic Graph Semantic Parsing via Uncertainty"},{"paperId":"7732a48dc3f66b857d42ba5d2bb6309daf216c75","title":"Experiments in Learning Dyck-1 Languages with Recurrent Neural Networks"},{"paperId":"6ada57ec94cd8712cd140f0741c6df0bc01e7509","title":"Assessing Combinational Generalization of Language Models in Biased Scenarios"},{"paperId":"44772fe1c3fa422a3da7e25092db2544893d6bfb","title":"Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming"},{"paperId":"bbaee85fe38122e4691a78ac23d4a6a751c357e6","title":"Learning Proof Path Selection Policies in Neural Theorem Proving"},{"paperId":"bb494b7d150ef15a45de24a7f02560c7fae3751f","title":"Towards Understanding and Improving the Generalization Performance of Neural Networks"},{"paperId":"6299cdda42d6c2a960f890b92e10924e01a0d0ca","title":"Compositional Generalization for Kinship Prediction through Data Augmentation"},{"paperId":"d129841cb2e30e25000dcd9edb83c880fc4babc1","title":"Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention"},{"paperId":"ff6d561552db2e6d113df35504dbef51a5cd8e28","title":"COAT: Measuring Object Compositionality in Emergent Representations"},{"paperId":"3207b6820785e243fdc402ce43ff5e93e4b1a92d","title":"Un algorithme d’analyse sémantique fondée sur les graphes via le problème de l’arborescence généralisée couvrante (A graph-based semantic parsing algorithm via the generalized spanning arborescence problem)"},{"paperId":"c7572eb872d032a180925be77b9ebd402c9362de","title":"O BJECT - CENTRIC C OMPOSITIONAL I MAGINATION FOR V ISUAL A BSTRACT R EASONING"},{"paperId":"0fdf910a78b153b76b813408636744f49c31d2eb","title":"C ATEGORIAL G RAMMAR I NDUCTION AS A C OMPOSI TIONALITY M EASURE FOR E MERGENT L ANGUAGES IN S IGNALING G AMES"},{"paperId":"123434be58eb21d41525171130b14679a4cc0010","title":"A GENT , DO YOU SEE IT NOW ? SYSTEMATIC GENERALI SATION IN DEEP REINFORCEMENT LEARNING"},{"paperId":"a143cac1bc440135b612132c89e603f364b8a3b7","title":"Combine to Describe: Evaluating Compositional Generalization in Image Captioning"},{"paperId":"474c954231413f2a249f04272dbeda19cd8ff09b","title":"Bridging the Generalization Gap in Text-to-SQL Parsing with Schema Expansion"},{"paperId":"dc88d2bbcebd810d7c80ba281739908005b12235","title":"Neurocompositional computing in human and machine intelligence: A tutorial"},{"paperId":"a77468f6bd4db7f8d761a0569d9cc29d5a8f0034","title":"L OGIC I NFERENCE : A N EW D ATASET FOR T EACHING L OGICAL I NFERENCE TO SEQ 2 SEQ M ODELS"},{"paperId":"b581c4973f6106a4597270483518e9c6f25924b2","title":"Deep Learning for Object Detection and Segmentation in Videos: Toward an Integration With Domain Knowledge"},{"paperId":"2c749751960af3a920ff04beaff62596442baca7","title":"Grounding Spatio-Temporal Language with Transformers"},{"paperId":"2fea588313c899ac512bb90f77b7ce9d31db421d","title":"Grammar-Based Grounded Lexicon Learning"},{"paperId":"97a0df299077a95de002c353965a2b6157f9e8d2","title":"Discrete and continuous representations and processing in deep learning: Looking forward"},{"paperId":"67b283f83c8730101019d3f0ec690d43a2602443","title":"Towards Interactive Language Modeling"},{"paperId":"f565a79ea8b29fc537dc317e7f38c83c9d3c49d3","title":"A discriminative account of the learning, representation and processing of inflection systems"},{"paperId":"a576512a7562597fd30719a834d5866d010ef6ab","title":"Compositional Generalization for Natural Language Interfaces to Web APIs"},{"paperId":"fcf25e1affc2f8ee5bb49d156f174e9769234deb","title":"Systematic Generalization with Edge Transformers"},{"paperId":"03da1b6759f70c10e49b33a0ee914cb893d6f949","title":"Learning Algebraic Representation for Systematic Generalization in Abstract Reasoning"},{"paperId":"2db6c10f135d5701ae7aec45986124ce264c1344","title":"Learning Symbolic Rules for Reasoning in Quasi-Natural Language"},{"paperId":"04db9b694280134f09af5fa787a306907edba29d","title":"How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN"},{"paperId":"c764ecba2bace12b9bfb9c2b0651a12ff6888ea7","title":"Learning to Generalize Compositionally by Transferring Across Semantic Parsing Tasks"},{"paperId":"7cc74ffa1215321712d4a830bb9dee19d9f0fb47","title":"Grounded Graph Decoding Improves Compositional Generalization in Question Answering"},{"paperId":"582f977cc2b06700dea6faa183c64f5d11204cfe","title":"How Do Neural Sequence Models Generalize? Local and Global Context Cues for Out-of-Distribution Prediction"},{"paperId":"f0787aace6a6d892140d9da85b49e76cb06f6862","title":"Automatic Knowledge Augmentation for Generative Commonsense Reasoning"},{"paperId":"1da81224d2a781b88186d81872755535e82fce5c","title":"Understanding How Encoder-Decoder Architectures Attend"},{"paperId":"d91dae75e7d3a13aad9a6815d6cbdf9a42f897e2","title":"SQALER: Scaling Question Answering by Decoupling Multi-Hop and Logical Reasoning"},{"paperId":"382504c7013f4578d0f1829fdcd413fed529cde1","title":"Learning to Solve Complex Tasks by Talking to Agents"},{"paperId":"00050c15896e8ae6bb534f10d072351547993f72","title":"LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing"},{"paperId":"8ccaf0c0fbd5e4079f36fa720cf23890be10dd66","title":"Dynamic Inference with Neural Interpreters"},{"paperId":"b07efb22576761eb15f82f2be9b5923f4f71aa43","title":"Learning to Follow Language Instructions with Compositional Policies"},{"paperId":"3d5699e7f7e085ad72102859b06fa4884d207e77","title":"Iterative Decoding for Compositional Generalization in Transformers"},{"paperId":"4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80","title":"How BPE Affects Memorization in Transformers"},{"paperId":"76c9558b3fa10baf0e094386a650015b29a8a4bc","title":"Compositional generalization in semantic parsing with pretrained transformers"},{"paperId":"bb0ab8591d6d57c7e2bd1ec35d806b3f25277752","title":"Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks"},{"paperId":"cdb2a71f42c88be6d5c5e4fecb8288a9a315f094","title":"Visually Grounded Concept Composition"},{"paperId":"85ef684e8f761e31cc8da9fcd6fe37c553d093f3","title":"Hierarchy in language interpretation: evidence from behavioural experiments and computational modelling"},{"paperId":"6ed7f8d8673fbf380c45cefa30138ff2b77d1d1b","title":"Abstraction, Reasoning and Deep Learning: A Study of the \"Look and Say\" Sequence"},{"paperId":"d41417f22f898125c4d34f672938ebb2a3764961","title":"Systematic Generalization on gSCAN: What is Nearly Solved and What is Next?"},{"paperId":"42c9186a600555f55a65c61638c155046774f081","title":"Robust Generalization of Quadratic Neural Networks via Function Identification"},{"paperId":"06fbeaf4d16639f177973a06cd7c4f78cb5e38ed","title":"COVR: A Test-Bed for Visually Grounded Compositional Generalization with Real Images"},{"paperId":"af749e5dbde38914ca6fa1463fca17eac8f69ecc","title":"ReaSCAN: Compositional Reasoning in Language Grounding"},{"paperId":"3962f108081b22c7e54b413f47ba6f2c16f2cc05","title":"Frequency Effects on Syntactic Rule Learning in Transformers"},{"paperId":"676fa805bd715591f99bb17e36d673a6a14e92fe","title":"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization"},{"paperId":"26c60f8ffb0d66d8732d22af6f5b539e49f2a1e6","title":"Discover AI Knowledge to Preserve Cultural Heritage"},{"paperId":"b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347","title":"Sequence-to-Sequence Learning with Latent Neural Grammars"},{"paperId":"4bc9521e56bca8995ed5e18274301961b25499a9","title":"How children learn to communicate discriminatively"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"95c379fa77e05cd2adc9f65e0dbb8e8065e30c43","title":"Neural Symbolic Representation Learning for Image Captioning"},{"paperId":"a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99","title":"Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning"},{"paperId":"45496cd0b256b75bfbe3bd95890b496069c7821c","title":"Multilingual Compositional Wikidata Questions"},{"paperId":"b61de520bc1ae57abde895601b62b4f92d82c0b4","title":"Pointer Value Retrieval: A new benchmark for understanding the limits of neural network generalization"},{"paperId":"d9911a0996fe6a4af7b67f378a0fd1c01aaf5e11","title":"Language Models as Zero-shot Visual Semantic Learners"},{"paperId":"be793883b04967307b4c59764c0199d65bec5972","title":"Hierarchical clustering optimizes the tradeoff between compositionality and expressivity of task structures for flexible reinforcement learning"},{"paperId":"7af9eb636ebe9da4aac10314e43da234e31ad535","title":"Systematic Generalization in Neural Networks-based Multivariate Time Series Forecasting Models"},{"paperId":"d839256cf7445748c4cf7354d2c6a1ff94efb694","title":"Generalization in Multimodal Language Learning from Simulation"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"},{"paperId":"10b809531cdd20b05274adcffcd4aa927f1fe54c","title":"Zero-Shot Compositional Concept Learning"},{"paperId":"d4b61b49c3b0c16d8d5c103802b77135fb9bf650","title":"What underlies rapid learning and systematic generalization in humans"},{"paperId":"1bed382373aed687c045bb65bc7541b16fc7a6be","title":"Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN"},{"paperId":"523745e29f6cb1890f18352d449fd3597910c485","title":"Improving Compositional Generalization in Classification Tasks via Structure Annotations"},{"paperId":"013ef3de1cc144b3014216255e8161b6e013d7ee","title":"Grounding Spatio-Temporal Language with Transformers"},{"paperId":"2d9b578e74d9b82cd30c6a5bee50162e626c5f5b","title":"How Modular Should Neural Module Networks Be for Systematic Generalization?"},{"paperId":"946179bd263d46d70422cdff7e6657b97b81230c","title":"Learning to Combine Per-Example Solutions for Neural Program Synthesis"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"3b8c8bbf2c1a69f2bf60b2434f2d1996aea2e740","title":"One Semantic Parser to Parse Them All: Sequence to Sequence Multi-Task Learning on Semantic Parsing Datasets"},{"paperId":"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","title":"Lexicon Learning for Few Shot Sequence Modeling"},{"paperId":"c0e059c46aea358872b4760aed53c4da3beaaeee","title":"Structured Reordering for Modeling Latent Alignments in Sequence Transduction"},{"paperId":"af84a6ed24ff37a6c1147240584a5cdb31b38f06","title":"Using top-down modulation to optimally balance shared versus separated task representations"},{"paperId":"83ed3184cc7b2dcee3c2b91529870bc304513468","title":"Examining the Inductive Bias of Neural Language Models with Artificial Languages"},{"paperId":"577d44a10b424a55165a6bf4839bafce2c695302","title":"SyGNS: A Systematic Generalization Testbed Based on Natural Language Semantics"},{"paperId":"32feca141fce06c6588b4014d27953a3fc25f19b","title":"PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World"},{"paperId":"88dbde378e9ac5c25fc7d78f5da147223e8d34d4","title":"Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention"},{"paperId":"35c6bdab35e8fd4e982302b5270da3c8098c58b1","title":"Modular Networks for Compositional Instruction Following"},{"paperId":"b2d08f6e17882545b2cbb865fd8c907eb21c3aac","title":"Generalization in Instruction Following Systems"},{"paperId":"03ad126cfe495933f7bb769f27c03e5f31caedf8","title":"On Compositional Generalization of Neural Machine Translation"},{"paperId":"9bc7c64812fe4a14ac319e07a00926ce93b20b5a","title":"Modelling the development of counting with memory-augmented neural networks"},{"paperId":"f3879fedf036175aefdb750c5527d184f038b932","title":"Compositional Processing Emerges in Neural Networks Solving Math Problems"},{"paperId":"18bb3bbeffe2b00378342a876d3de4ed695c57b4","title":"Multi-Task Recurrent Modular Networks"},{"paperId":"2de6f237a86e733d335a023ff644e8fbe624b217","title":"Image interpretation by iterative bottom-up top-down processing"},{"paperId":"8bb292d04fcef84864d5a4ac9c170d4fc8003ae9","title":"gComm: An environment for investigating generalization in Grounded Language Acquisition"},{"paperId":"ca3535dcdda9849350ad7c991a60660b22844f2f","title":"Searchable Hidden Intermediates for End-to-End Models of Decomposable Sequence Tasks"},{"paperId":"fadbe4a7cac1abffd3b3f7b39588d01e7d14e919","title":"The Future of Computational Linguistics: On Beyond Alchemy"},{"paperId":"53bbc4213bad0ff9e0d56392dd2d66b5168ffbda","title":"Intuitive Physics Guided Exploration for Sample Efficient Sim2real Transfer"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":"c114db5f1c38cbe6797bc74ef98072cac71f6cc6","title":"ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser"},{"paperId":"d969066d1dff7203055a493daaa3af8c490bf58e","title":"Recognizing and Verifying Mathematical Equations using Multiplicative Differential Neural Units"},{"paperId":"0d73c5d6aa5d75fbde7b8fbffcb2bcf58268e650","title":"Embodying Pre-Trained Word Embeddings Through Robot Actions"},{"paperId":"cf2b945be7b695d01a9abc5b099c54d8cdabac2a","title":"AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning"},{"paperId":"316c5d697f7caf92b419213e929a6063afaf253c","title":"ACRE: Abstract Causal REasoning Beyond Covariation"},{"paperId":"2ab7473f9975d28ca40c6a5e4c39ba48c89a911d","title":"Relational Weight Priors in Neural Networks for Abstract Pattern Learning and Language Modelling"},{"paperId":"d3ef7abfa1dc2545e1842e98d09fb6473d600ce7","title":"Harnessing Geometric Constraints from Emotion Labels to improve Face Verification"},{"paperId":"eaa88d697f92739f3569564329e9d037aabbe2d7","title":"A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics"},{"paperId":"4d1b18ee2d6093fc977df09d62b2a0e40b62a698","title":"A HINT from Arithmetic: On Systematic Generalization of Perception, Syntax, and Semantics"},{"paperId":"c9873d7240df69d84fcad24b18d248466ec2e83d","title":"HALMA: Humanlike Abstraction Learning Meets Affordance in Rapid Problem Solving"},{"paperId":"c8fc3da2d6a0a7f05e716fa3c06fc7c813c9c049","title":"Can deep learning beat numerical weather prediction?"},{"paperId":"24a5ebb09502fcd29b7af851f40f858137d41818","title":"Compositional memory in attractor neural networks with one-step learning"},{"paperId":"02dac573bff1d0620bb7412ed68681d21aee7e12","title":"Systematic Generalization for Predictive Control in Multivariate Time Series"},{"paperId":"be09ed6cd73654a23f78416433a1b23ea623ea79","title":"Symbolic Behaviour in Artificial Intelligence"},{"paperId":"5505d608a1d482fdc083796db812379ec1cb8723","title":"Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches"},{"paperId":"10161db52bfa53bdab84ae97b47cef2f22119131","title":"The Role of Syntactic Planning in Compositional Image Captioning"},{"paperId":"0d39d525f30609d0541330f933007025cd457a83","title":"Exploring Transitivity in Neural NLI Models through Veridicality"},{"paperId":"64b8692b83b2b1d13790fc549c7b3f71348a9968","title":"Situation and Behavior Understanding by Trope Detection on Films"},{"paperId":"2c0a266f9cb88bb914c138ece0deaab8cf528f78","title":"Neural Sequence-to-grid Module for Learning Symbolic Rules"},{"paperId":"1a0cb72de90835245316562c6e56537f5ff0d0a4","title":"Can RNNs learn Recursive Nested Subject-Verb Agreements?"},{"paperId":"8eaa6a82e1f94c1552e5c284061e25fee36ec427","title":"Emergent Symbols through Binding in External Memory"},{"paperId":"d3edc20ed4a07195f3663abc0ead4220266fd75b","title":"*-CFQ: Analyzing the Scalability of Machine Learning on a Compositional Task"},{"paperId":"40848b41ed8c9c255ecd8a920006877691b52d03","title":"WILDS: A Benchmark of in-the-Wild Distribution Shifts"},{"paperId":"7344ed64d1717780422fd1d58fae85edc544d180","title":"Iterative Utterance Segmentation for Neural Semantic Parsing"},{"paperId":"df66fcd3fd4f0d55bd96528cddb0e2f08ddb3385","title":"Revisiting Iterative Back-Translation from the Perspective of Compositional Generalization"},{"paperId":"15ad6f868acb05e836e88774aa2b71b0e082e5aa","title":"Meta-Learning of Structured Task Distributions in Humans and Machines"},{"paperId":"b9c3e87bc09c4c6167a03a835c30b1c23bef7a40","title":"Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"856208bf9f17320dddb4bdd7127f08cae7d922e3","title":"A Differentiable Relaxation of Graph Segmentation and Alignment for AMR Parsing"},{"paperId":"cc0fc60e9b9d036acf53899c259574c8ce2aedfc","title":"Compositional Generalization via Semantic Tagging"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":"1ffed50c5a14012145ee4d87855ae23deee44be5","title":"CURI: A Benchmark for Productive Concept Learning Under Uncertainty"},{"paperId":"649c758b0e59ddedaae37a3757e8eabdba664e5a","title":"Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks"},{"paperId":"a29c55573839087490d7469fbbcfd06728566b44","title":"Paired Examples as Indirect Supervision in Latent Decision Models"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"b95184d5eb25b0fe66d8bd1ad1b7677a51c21702","title":"Latent Compositional Representations Improve Systematic Generalization in Grounded Question Answering"},{"paperId":"032399b7fc693a9fc12bb26d6be8c02d77dd397a","title":"What they do when in doubt: a study of inductive biases in seq2seq learners"},{"paperId":"27ad6a5a17a75d1879f47a21a6d07f56ce87cab9","title":"Learning Task-General Representations with Generative Neuro-Symbolic Modeling"},{"paperId":"320a260faa9a0533b29e062def6dc97ac1353d28","title":"Compositional Embeddings for Multi-Label One-Shot Learning"},{"paperId":"6001843fde0f61cf886cf3fc2d0ec9ac3f2a3197","title":"Challenges of Acquiring Compositional Inductive Biases via Meta-Learning"},{"paperId":"bcf2bc325e4a48b615efb9cad2da2ce3e2ecbec7","title":"Solving SCAN Tasks with Data Augmentation and Input Embeddings"},{"paperId":"bd652738efa000e4a74d27f5495577dce8ba34a0","title":"Few-Shot Novel Concept Learning for Semantic Parsing"},{"paperId":"8d5f5c693b803ca58db748d97af905d5b5a0130b","title":"Categorial Grammar Induction as a Compositionality Measure for Understanding the Structure of Emergent Languages"},{"paperId":"581f52c4a045b2fd2d906b8d9f31165b832d0049","title":"Supplementary Material for Grammar-Based Grounded Lexicon Learning"},{"paperId":"c735740b26ceaa4db9d77233116434c0e8b311d8","title":"Learning Adaptive Control Flow in Transformers for Improved Systematic Generalization"},{"paperId":"30c9c73ed04a2dad44c20312653b4c65a27a6b7b","title":"KommonGen: A Dataset for Korean Generative Commonsense Reasoning Evaluation"},{"paperId":"15e2520e7f3bd96ba9d9d6e967771b34e448a9f1","title":"Compositional Generalization and Neuro-Symbolic Architectures"},{"paperId":"6fedcd81fb5aa0e3a30d1c32895759456b8b3cee","title":"Neural Structure Mapping For Learning Abstract Visual Analogies"},{"paperId":"751f55beac45cec14b0aff6174ed0139afb54b08","title":"Modelling Symbolic Knowledge Using Neural Representations"},{"paperId":"aa04c312bad4ab8ed192a33c355dbfbab65d8098","title":"LEARNING STRUCTURE FROM THE GROUND UP: HIERARCHICAL REPRESENTATION LEARNING BY CHUNKING"},{"paperId":"b156ddf444b997cbbc3168ee96ff76a85f0a5d62","title":"Robust Visual Reasoning via Language Guided Neural Module Networks"},{"paperId":"0b1470014bdbaa80ba63da0491d9db6c7d4febcc","title":"Detecting Compositionally Out-of-Distribution Examples in Semantic Parsing"},{"paperId":"6d466de7180776023a539371fad3d521eb5ff791","title":"Retrieval, Analogy, and Composition: A framework for Compositional Generalization in Image Captioning"},{"paperId":"1a95eed753096bcf219fa5394623141899115851","title":"How Do Neural Sequence Models Generalize? Local and Global Cues for Out-of-Distribution Prediction"},{"paperId":"d68255e8210843118d641175105e69686ad5b40f","title":"Mind the Context: The Impact of Contextualization in Neural Module Networks for Grounding Visual Referring Expressions"},{"paperId":"6d8943bd6edf34b022862be286cfd11fff16c89d","title":"Who’s on First?: Probing the Learning and Representation Capabilities of Language Models on Deterministic Closed Domains"},{"paperId":"af4147ed4d8c4dd81fdfb21eb4658e2ef3b825df","title":"Controlled tasks for model analysis: Retrieving discrete information from sequences"},{"paperId":"87e0b67e02a90e7e714336be5941190907cfd33f","title":"C L ] 1 7 M ay 2 01 8 Extrapolation in NLP"},{"paperId":"a406701b5fb05be55244d4f940db7be55fce85c6","title":"Semantic Systematicity in Connectionist Language Production"},{"paperId":"dff59ec1f1d3c01c3c7046517aa7b0612655764c","title":"Guiding Multi-Step Rearrangement Tasks with Natural Language Instructions"},{"paperId":"10a269a4f37a3d5f3b1c7e585b81ac704a3036a3","title":"Learning to Represent State with Perceptual Schemata"},{"paperId":"95f8cf3dd2cd1d050d6b155e3c056f0e14f496b7","title":"Better Chinese Sentence Segmentation with Reinforcement Learning"},{"paperId":"7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8","title":"AND does not mean OR: Using Formal Languages to Study Language Models’ Representations"},{"paperId":"41e9b2470849ceeeabcead25255519bf80311f8d","title":"Contribution d’informations syntaxiques aux capacités de généralisation compositionelle des modèles seq2seq convolutifs (Assessing the Contribution of Syntactic Information for Compositional Generalization of seq2seq Convolutional Networks)"},{"paperId":"240467c5c29ebf4926e8df392cc75e2a4fcffd3a","title":"Structure-(in)dependent Interpretation of Phrases in Humans and LSTMs"},{"paperId":"bab0efc6870d848ab4fe89f47d0fc947769cee7f","title":"Systemic Oversimplification Limits the Potential for Human-AI Partnership"},{"paperId":"fed5ad5d8f4a745c04eefc6fbff4bee4b0631323","title":"Episodes Meta Sequence S 2 Fast Update Slow Update Fast Update Slow Update"},{"paperId":"7f9b6708572d1950e40dae89fd5e556198c105e7","title":"Modular Networks for Compositional Instruction Following"},{"paperId":"94997502ffeafe48545a23a7f6cf6b1de79d4979","title":"A computational framework for learning and transforming task representations"},{"paperId":"eadd4de61cdc3ddd8c6e237187ea5cede78d484f","title":"USING MULTIPLICATIVE DIFFERENTIAL NEURAL UNITS"},{"paperId":"1f0e1657063ea38cf225eaf1c1187ae7b2e4a0e0","title":"Increasing Robustness to Spurious Correlations using Forgettable Examples"},{"paperId":"359c56a068e4a84a9f3b78f43b53fe3b333c0ba0","title":"Systematic generalisation with group invariant predictions"},{"paperId":"fd57ee5b9aa17c263281a8901ac36f83bb540a0d","title":"Harnessing Geometric Constraints from Auxiliary Labels to Improve Embedding Functions for One-Shot Learning"},{"paperId":"01295091d318dca8212431590cce2c7a1081469c","title":"Compositional Generalization via Parsing Tree Annotation"},{"paperId":"b82b89bf6405bc98cb39fc477e0c5a85a563e7fb","title":"GENERATIVE NEURO-SYMBOLIC MODELING"},{"paperId":"9dc7fba6551c8e142b95ee6baf0d25ccb1aed988","title":"On (Emergent) Systematic Generalisation and Compositionality in Visual Referential Games with Straight-Through Gumbel-Softmax Estimator"},{"paperId":"373bc164d7b552f8782988e7da6b0d00092a20b0","title":"Continual Lifelong Learning in Natural Language Processing: A Survey"},{"paperId":"d642868ce4325ebf3026c0aa0c497a079f112a8d","title":"On the Binding Problem in Artificial Neural Networks"},{"paperId":"690e5b12b80b33eb8da4efd5621e56def1217db1","title":"Infinite use of finite means: Zero-Shot Generalization using Compositional Emergent Protocols"},{"paperId":"f162b64756f01cdf04bc59c7592a77e4c8981656","title":"Question Answering over Knowledge Bases by Leveraging Semantic Parsing and Neuro-Symbolic Reasoning"},{"paperId":"d8ee993d54b83f81f7f65f2d33be04ae540d5590","title":"Invertible Tree Embeddings using a Cryptographic Role Embedding Scheme"},{"paperId":"ac5ed37f35375ea423c8474a944f27d3a0ab8774","title":"Learning Canonical Transformations"},{"paperId":"4b58367375466e653751a0c258b2f50bd3551408","title":"Sequence-to-Sequence Networks Learn the Meaning of Reflexive Anaphora"},{"paperId":"f4dbae9454eaf73a5dd7e2b7a087b52946d286dd","title":"Distilling Structured Knowledge for Text-Based Relational Reasoning"},{"paperId":"106fb432d2b62f3824a9d6f4a1b30e1f8b6ea9d7","title":"Sequence-level Mixed Sample Data Augmentation"},{"paperId":"e0acae87ae6d1d14bb2852aad7d645fceee87eb2","title":"The MAGICAL Benchmark for Robust Imitation"},{"paperId":"83d2d970db0eeb645d087a7f37bb05adb780706e","title":"Modularity Improves Out-of-Domain Instruction Following"},{"paperId":"259cf65eeae13861031f44cf906d43b155192b10","title":"Explicitly Modeling Syntax in Language Model improves Generalization"},{"paperId":"5262a59603b816eb3a339da937170e2b134a139f","title":"Compositional Generalization with Tree Stack Memory Units."},{"paperId":"42d376cdf2437769b9619aa38db64921772920ca","title":"Linguistically-Informed Transformations (LIT): A Method for Automatically Generating Contrast Sets"},{"paperId":"e16820ec1445cf6ec55d6e8ec27afa5c02812a11","title":"Inferring symmetry in natural language"},{"paperId":"986cc3d0e3afb23f84564ea9588b7b8e9c3e1dd6","title":"Hierarchical Poset Decoding for Compositional Generalization in Language"},{"paperId":"227fe850a72fab24998c7e08d75db214715dc74e","title":"The EOS Decision and Length Extrapolation"},{"paperId":"37882abaec01eba1bf5bda8a36c904aaea0d5642","title":"Improving Compositional Generalization in Semantic Parsing"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"055fac05cd424e7b1bdcd359ff7980ca8d938ef3","title":"Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually)"},{"paperId":"3fd45fc420a882ab2fba3166ef08f376cc758ad0","title":"On Long-Tailed Phenomena in Neural Machine Translation"},{"paperId":"f65f90c41aafd40449edc8e2c2c80a63bc767e6d","title":"Interpretable Neural Computation for Real-World Compositional Visual Question Answering"},{"paperId":"389596027e577fb28ea5e1cc313b4f3d610cffd3","title":"Recursive Top-Down Production for Sentence Generation with Latent Trees"},{"paperId":"e28ce15bc1e5108dbed621a1e72af6906a772d42","title":"Meta-Learning of Compositional Task Distributions in Humans and Machines"},{"paperId":"31a5556d5d311a17a20bc9cfb6c7fed7c4efe66d","title":"Unseen Filler Generalization In Attention-based Natural Language Reasoning Models"},{"paperId":"89cb62dc83c1b1895267bd28639fbf5bb7ed21a4","title":"Measuring Systematic Generalization in Neural Proof Generation with Transformers"},{"paperId":"ddf1b38b6ba885326e7f44721a135a7b2d7f415a","title":"Think before you act: A simple baseline for compositional generalization"},{"paperId":"00b1e962182c42949822821dc1a929bd132ec082","title":"Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models"},{"paperId":"1f95641726dc6875a7f6a89c1d8b0ee414bd4b52","title":"Recurrent Inference in Text Editing"},{"paperId":"3ecc8f61418f6afdbb600d9f6fbb286143e56026","title":"Systematic Generalization on gSCAN with Language Conditioned Embedding"},{"paperId":"5b7547aa20140b29cd6d8426e4110d4ef97717ed","title":"ReLEx: Regularisation for Linear Extrapolation in Neural Networks with Rectified Linear Units"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"8320ea909c38a616f9daccff4e5a49cfce4d9735","title":"Analogical Reasoning for Visually Grounded Language Acquisition"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"2e8c84fd61c91e067dddef52ced76b824beb7013","title":"Learning Reasoning Strategies in End-to-End Differentiable Proving"},{"paperId":"189d489d7dabf51949124f55fb0104326d8c84a4","title":"Learning Representations that Support Extrapolation"},{"paperId":"c3375b9b26c8f32e5c6a16443e80e9ec45055e0e","title":"The Scattering Compositional Learner: Discovering Objects, Attributes, Relationships in Analogical Reasoning"},{"paperId":"79d7648aeacc5d5e90b296acbf156ed1131eae37","title":"Compositionality Decomposed: How do Neural Networks Generalise? (Extended Abstract)"},{"paperId":"61a412f7c351a2a582f65853522ad7a632b0186d","title":"Evaluating Compositionality of Sentence Representation Models"},{"paperId":"23835438889899885d9f33de2fb2356da10bbc0c","title":"Compositional Generalization by Factorizing Alignment and Translation"},{"paperId":"aecb95605083c460feb289ec40901e328805fae5","title":"Deep Reinforcement Learning and Its Neuroscientific Implications"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"c103f1903e6cb9904729e65fb167a18dbfe3b129","title":"A Study of Compositional Generalization in Neural Models"},{"paperId":"61c49e6399e76a96ebc82dbe486131db6c07b772","title":"Relational reasoning and generalization using non-symbolic neural networks"},{"paperId":"5abfd3c004f79402a3a321564e8963797f60c6b9","title":"Systematic Generalisation through Task Temporal Logic and Deep Reinforcement Learning"},{"paperId":"432a572956526239103c6d8658bdd46c29104aa1","title":"Closed Loop Neural-Symbolic Learning via Integrating Neural Perception, Grammar Parsing, and Symbolic Reasoning"},{"paperId":"2eb710b446570f48377b25eb279295648d05f65d","title":"On sample efficiency and systematic generalization of grounded language understanding with deep learning"},{"paperId":"0bfd4ed399054eae26c3cdaabc0aed80ca95e125","title":"Neural Power Units"},{"paperId":"b0434df9b6ed85aa79859335905f549eeac3e817","title":"Constitutional Rights in the Machine Learning State"},{"paperId":"347b926e82fa8e635050a5c7781598642c115596","title":"Transforming task representations to perform novel tasks"},{"paperId":"5ea7d562df6aac215630df8abf4a1321a9e47e6e","title":"Transforming task representations to allow deep learning models to perform novel tasks"},{"paperId":"4dc005ea288c50d57222122903edf87f21689781","title":"Probing Linguistic Systematicity"},{"paperId":"f7f20e163cba2ec5ca74fd1c4352a987d67ef7b7","title":"Emergence of Syntax Needs Minimal Supervision"},{"paperId":"4f91d16d7d9c21e2d9fffeff6abc78619be4d133","title":"Visually Grounded Continual Learning of Compositional Phrases"},{"paperId":"ca48348e0d00b5c3766b9a1ed8864ffce9285d96","title":"Visually Grounded Continual Learning of Compositional Semantics"},{"paperId":"ec3d5bdfda2c5c841c2481f5da123b2c086e6f5c","title":"Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking"},{"paperId":"c30b457fdfb0623b87379de79ffaa570a7f3bb48","title":"Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation"},{"paperId":"7e69f2255d3685c6aae2790d0807b5df5fd031f5","title":"Compositional Continual Language Learning"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"4599f96dd1a2584e00d342953fc7e1361ffd6e1f","title":"Neural Status Registers"},{"paperId":"db03f8de1944ae6001f860219f6e483bd6280030","title":"Abstract Rule Based Pattern Learning with Neural Networks"},{"paperId":"387b5988331f8fe779c323f8a88df23daa715a8a","title":"Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?"},{"paperId":"eb7d630c0fc1fc5601775be72265230707382107","title":"What Limits Our Capacity to Process Nested Long-Range Dependencies in Sentence Comprehension?"},{"paperId":"d70af4990cba2574c41b1235030f7a5b702e2d70","title":"Compositionality and Generalization In Emergent Languages"},{"paperId":"39e3c1c4ea9ca492123941c0963146e1898a5a10","title":"Generating new concepts with hybrid neuro-symbolic models"},{"paperId":"022dcaa4cc51ecd038970683146b2ad19f45df61","title":"Synonymous Generalization in Sequence-to-Sequence Recurrent Networks"},{"paperId":"0dc5dd7c64ee016bdc33a5f32dc25747be5ca702","title":"From SCAN to Real Data: Systematic Generalization via Meaningful Learning"},{"paperId":"6f0be1f9bda7530b1fa654cac84d595ca9d53740","title":"Revisit Systematic Generalization via Meaningful Learning"},{"paperId":"7e9af8a6081dc00187bd4a6727751d1721bd7816","title":"Evaluating Logical Generalization in Graph Neural Networks"},{"paperId":"937fef6a786c4463a3bb19770c704945d1600b66","title":"Learning Compositional Rules via Neural Program Synthesis"},{"paperId":"4bc2bb6584774b0d8ad0b4f5215dc2075487c192","title":"A Benchmark for Systematic Generalization in Grounded Language Understanding"},{"paperId":"e4b0ec63559b6792bb129385e3c67d541afb53a6","title":"A Puzzle concerning Compositionality in Machines"},{"paperId":"c04262cb3f76ff769af32afad05263bd47ebef18","title":"Evaluating visually grounded language capabilities using microworlds"},{"paperId":"0119a57cf88ef16e6dc291252fae340bb6b3953c","title":"CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning"},{"paperId":"8bae31b144575426f14131a3f04f1e7cd2cc02f5","title":"Compositional properties of emergent languages in deep learning"},{"paperId":"4aea918d9bd66440ce0c00abbfbb57b212d76158","title":"Neural Arithmetic Units"},{"paperId":"4e91c1fde4030c9518932c7a0b46358bb93ba91a","title":"Exploiting Language Instructions for Interpretable and Compositional Reinforcement Learning"},{"paperId":"b0ea633e0c22fbd8cbc531c7326376725d16ce25","title":"Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"3ee9a301c4f269f5bd20390d28d2a50d0935339b","title":"Hierarchical Character Embeddings: Learning Phonological and Semantic Representations in Languages of Logographic Origin Using Recursive Neural Networks"},{"paperId":"fe2f525349d31f854472dc39b63612322bf8a445","title":"Weight Priors for Learning Identity Relations"},{"paperId":"d715b4a9282562b9d84fb66e04ee70e66b12e86d","title":"Location Attention for Extrapolation to Longer Sequences"},{"paperId":"cf96b99f4d35b452d4679b664b74aa255e000b8c","title":"Capacity, Bandwidth, and Compositionality in Emergent Language Learning"},{"paperId":"80993cc44eef146116bf682fd89f5f256211620f","title":"Discovering the Compositional Structure of Vector Representations with Role Learning Networks"},{"paperId":"ae3501afbe8f3f5cf1c5270fa00d0b65fc1c9484","title":"Environmental drivers of systematicity and generalization in a situated agent"},{"paperId":"681fbcd98acf20df3355eff3585994bd1f9008b7","title":"Probing Natural Language Inference Models through Semantic Fragments"},{"paperId":"d4fc020db15584ea040162a1afb6abc81aa6c7e4","title":"Analyzing machine-learned representations: A natural language case study"},{"paperId":"097c71968b5eaacec908f4d1ce1137a53dca8bdc","title":"Learning First-Order Symbolic Representations for Planning from the Structure of the State Space"},{"paperId":"815a3d56401483b635cfad9468852cddb46350ee","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":"4cfd8f903506865e7ccf28b0a07ee3c551487e92","title":"Detecting semantic anomalies"},{"paperId":"ba310a9b9f7b812478b2a08dbd9917ea937a4e36","title":"Mutual exclusivity as a challenge for deep neural networks"},{"paperId":"57bb2a385675f8b2ab24e7b9b397553fdaafb849","title":"Sequential Mastery of Multiple Visual Tasks: Networks Naturally Learn to Learn and Forget to Forget"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"389b9d9f88c262fa65800318cf029aca1b37eb1f","title":"Systematicity in a Recurrent Neural Network by Factorizing Syntax and Semantics"},{"paperId":"ab9e5d6804c5051e4f7fa07e467e766ca0126500","title":"Are Neural Nets Modular? Inspecting Their Functionality Through Differentiable Weight Masks"},{"paperId":"7b85c3dd9120ff2cab03f36d778520cd7fae903c","title":"THE SCATTERING COMPOSITIONAL LEARNER: DISCOVERING OBJECTS, ATTRIBUTES, RELATION-"},{"paperId":"3f51de90ef9c875b4aa16d5dd3a90c9689bbf249","title":"MODELS WITH VARIABILITY IN DATA"},{"paperId":"d2b39c91d0f49557ff01844ee22411fcf9933a36","title":"E MERGENT S YMBOLS THROUGH B INDING IN E XTERNAL M EMORY"},{"paperId":"55c9b62b69b7f644e326969f8f712e6487cd4b9d","title":"Constitutional Rights in the Machine Learning State"},{"paperId":"2afad1b280396594911012ae58bfc427d9bb3873","title":"LATENT DECISION MODELS"},{"paperId":"39a7e79063cf9e967e65599fe89ed45efe2f63fb","title":"A Simple and General Strategy for Refer- ential Problem in Low-Resource Neural Ma- chine Translation"},{"paperId":"502a1995c0a56af4487d364f56e1ce8abe78f23a","title":"The DeepMind Chinese–English Document Translation System at WMT2020"},{"paperId":"6027dd5d9491a04b3845c5e62fc6df8bcd320e66","title":"Artificial Intelligence XXXVII: 40th SGAI International Conference on Artificial Intelligence, AI 2020, Cambridge, UK, December 15–17, 2020, Proceedings"},{"paperId":"871d5f31e23f427ba229023ea10ec6873ec36a0d","title":"Multimodal Graph Networks for Compositional Generalization in Visual Question Answering"},{"paperId":"dc8020f05b18ed434009b5d56398c57456c7dcde","title":"Disentangling neural network representations for improved generalization"},{"paperId":"80deaca65c2c155bd15718eeecff584841eb25b0","title":"CLOSURE: Assessing Systematic Generalization of CLEVR Models"},{"paperId":"6e8bd7310e2603776ec4f51b6abc2a485f9ca7ce","title":"Extending Machine Language Models toward Human-Level Language Understanding"},{"paperId":"6422d5e83caec99487936035cfbb2b0d18f2a76d","title":"Biology and Compositionality: Empirical Considerations for Emergent-Communication Protocols"},{"paperId":"83989d1dcf1bb533d37db6a6ff9af478feeb4aae","title":"Composing and Embedding the Words-as-Classifiers Model of Grounded Semantics"},{"paperId":"72e0d7e374937374897cab79306079f6f436a9d4","title":"On Compositionality in Neural Machine Translation"},{"paperId":"9dc75988a53b86c1e36539daa0d8ac003b234502","title":"Big Generalizations with Small Data: Exploring the Role of Training Samples in Learning Adjectives of Size"},{"paperId":"287e85aca777d6d3d73e1484ba9c0f09d40f578a","title":"Ordered Memory"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"112ac68ddb0f021517dd465e89918fa52755cc35","title":"Measuring Arithmetic Extrapolation Performance"},{"paperId":"4beaabe0c4277ddb850a2f91a20b5fcec84f18af","title":"Emergent Systematic Generalization in a Situated Agent"},{"paperId":"d88f31a0091eee02c5a2aa2013914818cdef114e","title":"Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving"},{"paperId":"48e43bb9d4843e9c9e0599d85dde3121989de01a","title":"Learning First-Order Symbolic Planning Representations from Plain Graphs"},{"paperId":"32c9a0acee8d236c553395052c29a6d853d8ea2d","title":"Compositional Generalization in Image Captioning"},{"paperId":"a1a662301818cdc77dd015280da285847223765b","title":"How to study the neural mechanisms of multiple tasks"},{"paperId":"4d031258a66076187001b4d6182345198624d872","title":"The compositionality of neural networks: integrating symbolism and connectionism"},{"paperId":"5e35895fc4731858f0b286cb5a1613a819cc2367","title":"CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text"},{"paperId":"363a24a96423227ed6b03fc71a169ad0f80e6fac","title":"Grammatical Sequence Prediction for Real-Time Neural Semantic Parsing"},{"paperId":"9d671f60388f1da0ec1605b42260f7271f35a3de","title":"Modeling question asking using neural program generation"},{"paperId":"136c05cb8dd359fb8e0dc7947172a9ecb74ccbec","title":"Learning by Abstraction: The Neural State Machine"},{"paperId":"c7abc9d3d3b540ec9e3b423df706f750c8dc36f9","title":"The teaching size: computable teachers and learners for universal languages"},{"paperId":"2030b07a36a77821b978ff8a20a14d88c1c60db1","title":"Mutual exclusivity as a challenge for neural networks"},{"paperId":"3a04b6098a965781f5ed06ea7f260a66421f61bf","title":"Neural Theorem Provers Do Not Learn Rules Without Exploration"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"32c0e0b181eff3e9dd1c1dacec4138770008125e","title":"Assessing Incrementality in Sequence-to-Sequence Models"},{"paperId":"b39efed2e73357db4691f66935cf62e7b51f30e1","title":"Transcoding Compositionally: Using Attention to Find More Generalizable Solutions"},{"paperId":"5ee4bd410e633f07953d88827ee9dce4688b436e","title":"Scaling characteristics of sequential multitask learning: Networks naturally learn to learn"},{"paperId":"82dac30bc25eb1470e07ff5bd1ec000f28f4c6d8","title":"Siamese recurrent networks learn first-order logic reasoning and exhibit zero-shot compositional generalization"},{"paperId":"6a9cab267526de228a51f70ae56f1e07ecfc7bf8","title":"Grammar-based Neural Text-to-SQL Generation"},{"paperId":"3323d0956b2db141f3727aacbf7c111e9d39f17d","title":"Planning with State Abstractions for Non-Markovian Task Specifications"},{"paperId":"ed2f6d491c3796afa3e03460ede800cf13191231","title":"Sequential mastery of multiple tasks: Networks naturally learn to learn"},{"paperId":"2621323502fc779c79bca7ba112bc4d0c1db1d3f","title":"CNNs found to jump around more skillfully than RNNs: Compositional Generalization in Seq2seq Convolutional Networks"},{"paperId":"46940a78ad8ba3292666738fcfd92d2f5ec12ba9","title":"The relational processing limits of classic and contemporary neural network models of language processing"},{"paperId":"e24e44515b15e1326dd25ab092a152a067c63fc1","title":"Word-order Biases in Deep-agent Emergent Communication"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"b42c1eb6a8eadc8d734acd4c1ff1ff0f6b164500","title":"Emergence of Compositional Language with Deep Generational Transmission"},{"paperId":"e2bee23ac912cf36f0479b830ff78d4bb6cc73c3","title":"Analyzing and interpreting neural networks for NLP: A report on the first BlackboxNLP workshop"},{"paperId":"0dc092d33f7c71bc9d8d42b53ebb1fad101db4c8","title":"Linguistic generalization and compositionality in modern artificial neural networks"},{"paperId":"dc30dd7d209f518a2af01fc20b49ee55bd2c4957","title":"Training neural networks to encode symbols enables combinatorial generalization"},{"paperId":"66a388a200bd36aff05ec8acb6fbb092b3e1d777","title":"Data for free: Fewer-shot algorithm learning with parametricity data augmentation"},{"paperId":"32948ae25dbd35f2d94a59c27f6bee935bd602b8","title":"Studying the Inductive Biases of RNNs with Synthetic Variations of Natural Languages"},{"paperId":"d74037476b85dc02eb74369b12e0a5848617ecbc","title":"Deep Learning for Cognitive Neuroscience"},{"paperId":"1dd5bcc9c55416bacab30fd96818bc1e6e02e02e","title":"No integration without structured representations: Response to Pater"},{"paperId":"cef77310f326bd30b172459dbecaedf228fc7b23","title":"Probabilistic Neural-symbolic Models for Interpretable Visual Question Answering"},{"paperId":"0d16b0fbe1e2bf6ef02aea2e058f2e13c3a83fa2","title":"Learning to Make Analogies by Contrasting Abstract Relational Structure"},{"paperId":"7a8f8109e65ed9a6048859681a825eb5655e5dd2","title":"No Training Required: Exploring Random Encoders for Sentence Classification"},{"paperId":"9d9b4cc02fc0ac6fe7eac649599db1a47cf99d89","title":"Human few-shot learning of compositional instructions"},{"paperId":"648c33300ae7597f24e73600353aef31df745086","title":"Task representations in neural networks trained to perform many cognitive tasks"},{"paperId":"668f42a4d4094f0a66d402a16087e14269b31a1f","title":"Analysis Methods in Neural Language Processing: A Survey"},{"paperId":"a686c3becc42588517a4ca061badeb12c3c5a333","title":"Modelling Identity Rules with Neural Networks"},{"paperId":"6c7494a47cc5421a7b636c244e13586dc2dab007","title":"Systematic Generalization: What Is Required and Can It Be Learned?"},{"paperId":"56326d969591d3c30e2f48027b926be20a3e75f7","title":"Automatically Composing Representation Transformations as a Means for Generalization"},{"paperId":"ae307bfefb75495331e66be955e7f64a0aba1430","title":"Natural Language Statistical Features of LSTM-Generated Texts"},{"paperId":"9fcd9536c03870255d13935a4fd6379052ac15cc","title":"SYSTEMATIC GENERALIZATION: WHAT IS REQUIRED AND CAN IT BE LEARNED?"},{"paperId":"7d7487351353cd2708106e995d6296c2c84c295d","title":"Systematic Compositionality in Recurrent Neural Networks (RNNs)"},{"paperId":"8f8216c871b52812f4122e55d39dbfa759d1fdda","title":"Compositional Generalization in Image Captioning"},{"paperId":"96a7232eb547fed9e7fd2a2b1d6a100feee3a2ed","title":"Compositionality as Directional Consistency in Sequential Neural Networks"},{"paperId":"8225f6f8a037188f2701e9596241da519df8e8ef","title":"M UTUAL EXCLUSIVITY AS A CHALLENGE FOR DEEP NEURAL NETWORKS"},{"paperId":"c356394d9f1296103d3cb8c3b4cf56ab1e3305e5","title":"Compositionality as Directional Consistency in Sequential Neural Networks"},{"paperId":"d4eb596212d25bb52aae4c43ef65236439357af6","title":"Structured learning and inference with neural networks and generative models by"},{"paperId":"486f4a078560dd258ca5af63cb025759cc5638be","title":"Learning Reduplication with a Neural Network without Explicit Variables"},{"paperId":"09fb53961e9a4e3c172c3e3b726ebf94961528e1","title":"Zero-Shot Transfer VQA Dataset"},{"paperId":"33ecb49e7b1eb1f44790fb6ceca6eed82cb0c7cd","title":"Jump to better conclusions: SCAN both left and right"},{"paperId":"1519e58ca9f6a14c7b08fd644508977caae9477d","title":"Limitations in learning an interpreted language with recurrent models"},{"paperId":"79cb080c84da314c2113692585b1e9ee29afa33a","title":"On learning an interpreted language with recurrent models"},{"paperId":"31f48073366d5fad7b1c9e60af315690475b52f6","title":"RNNs as psycholinguistic subjects: Syntactic state and grammatical dependency"},{"paperId":"5fc548f3f3112de7eddad3744717dc2f9d22ca38","title":"Neural Arithmetic Logic Units"},{"paperId":"843c6b0a35b02e2c3d74bb545e74bc655e16e992","title":"Assessing Composition in Sentence Vector Representations"},{"paperId":"210feb22ff541920caa4884e73eaff1c09644114","title":"Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks"},{"paperId":"395f0f8676a0b41360fbfe3a001b9f8c8dfcd24c","title":"Extrapolation in NLP"},{"paperId":"c7a46543829b59a9423a067ca798c63b872eb613","title":"Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing"},{"paperId":"08fbb1b4cfdc83977d2c8f08bdfb663f13c0e60a","title":"Memorize or generalize? Searching for a compositional RNN in a haystack"},{"paperId":"3c092128a2c98e5e3be5f8872cf05c635430cd60","title":"Evaluating Compositionality in Sentence Embeddings"},{"paperId":"fc05801280853ff6f6a15c55d9b76d8c58182f39","title":"Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction"},{"paperId":"744c22e95eaceabb031589ba808bf2e9f24b04c7","title":"From Sequence to Attention Search for a Compositional Bias in Sequence-to-Sequence Models by Kristian Korrel"},{"paperId":"0d45e411a7b588b70b82f22b04c7ccfaef0e3fd7","title":"SYSTEMATIC GENERALIZATION: WHAT IS REQUIRED"},{"paperId":"0c43dfe8a834fce0467ba6a74b2daeebb5bb8b53","title":"On internal language representations in deep learning: an analysis of machine translation and speech recognition"},{"paperId":"acf13c52c86a3b38642ba0c6cbcd1b771778965c","title":"NAACL HLT 2018 Generalization in the Age of Deep Learning Proceedings of the Workshop"},{"paperId":"0fff5c49c05c27c22ac7685130197146491f0b36","title":"The Consciousness Prior"}],"references":[{"paperId":"c6c171d2a9be192d60af7b434e4ba2fcbbad7f48","title":"Memory-augmented Neural Machine Translation"},{"paperId":"2e17cf6a339fd071ad222062f868e882ef4120a4","title":"Inferring and Executing Programs for Visual Reasoning"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"a396a6febdacb84340d139096455e67049ac1e22","title":"Learning to Reason: End-to-End Module Networks for Visual Question Answering"},{"paperId":"c889d6f98e6d79b89c3a6adf8a921f88fa6ba518","title":"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"},{"paperId":"bc7fcefa3e333d50463d524406d107060c4a0cec","title":"Neural Semantic Parsing over Multiple Knowledge-bases"},{"paperId":"53bb7789be36a58f865f2ec84f6d8f816ddaae6a","title":"Learning to Learn"},{"paperId":"784ee73d5363c711118f784428d1ab89f019daa5","title":"Hybrid computing using a neural network with dynamic external memory"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"1a327709cc53ff9e52454e50a643abf4a0ac92af","title":"Findings of the 2016 Conference on Machine Translation"},{"paperId":"b7eac64a8410976759445cce235469163d23ee65","title":"Data Recombination for Neural Semantic Parsing"},{"paperId":"9e7ad19160313552175f7dc3e5acf94a430f66ac","title":"The Architecture of Cognition: Rethinking Fodor and Pylyshyn’s Systematicity Challenge"},{"paperId":"7260c0692f8d265e11c4e9c4c8ef4c185bd587ad","title":"Building machines that learn and think like people"},{"paperId":"558ac446dc26bee9789d660a251b75728cb6eeb2","title":"Language to Logical Form with Neural Attention"},{"paperId":"3457ddb9b9f614aad52052d680f9b11c08b3a4cf","title":"A Roadmap Towards Machine Intelligence"},{"paperId":"b59d91e0699d4e1896a15bae13fd180bdaf77ea5","title":"Neural Programmer-Interpreters"},{"paperId":null,"title":"TreeStructured Composition in Neural Networks without TreeStructured Architectures"},{"paperId":null,"title":"Neural programmerinterpreters"},{"paperId":"04d1a26c2516dc14a765112a63ec60dc3cb3de72","title":"Tree-Structured Composition in Neural Networks without Tree-Structured Architectures"},{"paperId":"d38e8631bba0720becdaf7b89f79d9f9dca45d82","title":"Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"adfcf065e15fd3bc9badf6145034c84dfb08f204","title":"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"a2f09447e8f1ceda391101e5ae7f863a8f2b2836","title":"Efficient Gradient-Based Inference through Transformations between Bayes Nets and Neural Nets"},{"paperId":"b4ff85584c7b5f487c735a131240c12d1baed9c2","title":"Getting real about systematicity"},{"paperId":"9ec7bdd58594c5da03b967c9f91dc31c5bd394c0","title":"A fundamental limitation of the conjunctive codes learned in PDP models of cognition: comment on Botvinick and Plaut (2006)."},{"paperId":"a83699a15ce3364aeb0ccd049615adc90d04f50b","title":"Empirical and computational support for context-dependent representations of serial order: reply to Bowers, Damian, and Davis (2009)."},{"paperId":"2b68712e29750f7ad530edd13e4eb1e3a67794cc","title":"How novelty search escapes the deceptive trap of learning to learn"},{"paperId":"4731f53d939c190653f99f434886d358f196732b","title":"37. Distributions in text"},{"paperId":"15c460439979d1ddf617ad343308359532f316d2","title":"Connectionist semantic systematicity"},{"paperId":"06c10fcd7cfce4614f8460298820a65b5b8e1818","title":"Strong systematicity in sentence processing by simple recurrent networks"},{"paperId":null,"title":"Distributions in text"},{"paperId":"3ccaa9d20e1f16f6c818853a970755ce888df792","title":"Generalisation towards Combinatorial Productivity in Language Acquisition by Simple Recurrent Networks"},{"paperId":"08f2d51806af94338c4cc35024d553b362da797b","title":"Short-term memory for serial order: a recurrent neural network model."},{"paperId":"dfc79017e52efb270155ce8b93337467804cb697","title":"Constructions at Work: The Nature of Generalization in Language"},{"paperId":"a56ee505e5651d90210d7f19adb77b1a6a424d3a","title":"Lack of combinatorial productivity in language processing with simple recurrent networks"},{"paperId":"6843890926bf0e5c887ffc78dcb1203135981bf1","title":"The compositionality papers"},{"paperId":"2ced1a9aa4fd00b8da18b7b9c8f2274bd12ff608","title":"Symbolically speaking: a connectionist model of sentence production"},{"paperId":"a6383f155fa9d3e9b15092bfefbf613f982eb263","title":"The Algebraic Mind: Integrating Connectionism and Cognitive Science"},{"paperId":"08dc7b19e679539f0f93db0192a8e8d11538b3dd","title":"Rethinking Eliminative Connectionism"},{"paperId":"e78dd4aa91817bbeb2fd87c82ae4db91c23b0997","title":"Are Feedforward and Recurrent Networks Systematic? Analysis and Implications for a Connectionist Cognitive Architecture"},{"paperId":null,"title":"Rethinking Eliminative Connectionism. Cognitive Psychology"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":null,"title":"Long short - term mem"},{"paperId":"ae1f906de7d0136acb8ba7b42665e9bdad245619","title":"Generalization and connectionist language learning"},{"paperId":"668087f0ae7ce1de6e0bd0965dbb480c08103260","title":"Finding Structure in Time"},{"paperId":"ce9a21b93ba29d4145a8ef6bf401e77f261848de","title":"A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":null,"title":"Syntactic Structures. Mouton, Berlin, Germany"},{"paperId":null,"title":"Syntactic Structures"}],"id":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","summary":"This paper introduces the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences, and tests the zero-shot generalization capabilities of a variety of recurrent neural networks trained on SCAN with sequence-to-sequence methods."},{"url":"https://www.semanticscholar.org/paper/b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation","venue":"EMNLP","year":2020,"referenceCount":58,"citationCount":93,"influentialCitationCount":16,"publicationDate":"10/12/2020","authors":"Najoung Kim,Tal Linzen","citations":[{"paperId":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models"},{"paperId":"04ca71d9089b2e86f9e4a874fd66ee8bd0baba8e","title":"When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks"},{"paperId":"c6e4518dfd687a2a5bed4e78d5d9f999292a1746","title":"Counterfactual Recipe Generation: Exploring Compositional Generalization in a Realistic Scenario"},{"paperId":"a638036c48cbd76ae3af30d6b273d49bb28a22e4","title":"Schrödinger's tree—On syntax and neural language models"},{"paperId":"76e2b3b6e1da49764d342ea922290410162125ca","title":"Measures of Information Reﬂect Memorization Patterns"},{"paperId":"4129fb99f5c8785452d60ddaf29d178525e29d0a","title":"Measures of Information Reflect Memorization Patterns"},{"paperId":"711d5e8ddbb840ad31a9ffa3d38590603ba69a92","title":"Prompting GPT-3 To Be Reliable"},{"paperId":"1ed29beb55b10de8553c926ce6da2625ec2c8776","title":"Benchmarking Long-tail Generalization with Likelihood Splits"},{"paperId":"b1f33e956e36bf25e118c0d537dcc519cfe52e60","title":"CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations"},{"paperId":"1bd799cf462f926041dd2fc8fbe4af54bddbf5c5","title":"Translate First Reorder Later: Leveraging Monotonicity in Semantic Parsing"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","title":"Compositional Generalisation with Structured Reordering and Fertility Layers"},{"paperId":"61d56ece2d19f4bfeb322c92085fb28521e169da","title":"Neural-Symbolic Recursive Machine for Systematic Generalization"},{"paperId":"40047a74b707743157051d38f76061ba5ff9aab4","title":"Compositional Semantic Parsing with Large Language Models"},{"paperId":"60f208b19bb63d82fda5759897677f92d4e1e2fc","title":"Improving Compositional Generalization in Math Word Problem Solving"},{"paperId":"1a7a24c73521eecf0a2d555e921b27e2c4d8e3c3","title":"What Artificial Neural Networks Can Tell Us About Human Language Acquisition"},{"paperId":"22f9a5fe8e446d215530fb90ea08b10499b36b0b","title":"Unit Testing for Concepts in Neural Networks"},{"paperId":"d2018d1b0f69ca6805aa18a22be95cab9b5c44c7","title":"On Neural Architecture Inductive Biases for Relational Tasks"},{"paperId":"96486c71106f0a61ebe02061c32f9fb80c227011","title":"Linear Connectivity Reveals Generalization Strategies"},{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"a122909a31acf41cb2d9eb602c01b24b9b85a061","title":"LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing"},{"paperId":"1167b3864046b732cf057b8b05db311e726cadab","title":"Measuring Alignment Bias in Neural Seq2seq Semantic Parsers"},{"paperId":"bc16284f517dd0011dcf64ea1c8fe6d6576494a4","title":"Is the Computation of Abstract Sameness Relations Human-Like in Neural Language Models?"},{"paperId":"f2611a09cf0942170785ee3025cb511de3bdec2e","title":"Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems"},{"paperId":"6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis"},{"paperId":"66f3f0e8ebc780e570770986f50bf9cb9cd53ec1","title":"WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series Tasks"},{"paperId":"4c430e6c3a72626bd4cb1893960c7c26dfec6c79","title":"Structurally Diverse Sampling Reduces Spurious Correlations in Semantic Parsing Datasets"},{"paperId":"69078af65fc934f81fd340e9d1323d6c08194548","title":"Revisiting the Compositional Generalization Abilities of Neural Sequence Models"},{"paperId":"557ebd17b7c7ac4e09bd167d7b8909b8d74d1153","title":"Compositional Generalization Requires Compositional Parsers"},{"paperId":"69df5b68fbf492341336b39b4cc9fcc74fff4d5f","title":"Improving Systematic Generalization Through Modularity and Augmentation"},{"paperId":"2b060b89324c376892a096c84fd14664f7b71710","title":"Understanding Robust Generalization in Learning Regular Languages"},{"paperId":"39f604fdd3ade5bd5a67d5284a6d9c12e535db85","title":"Compositionality as Lexical Symmetry"},{"paperId":"03eeff98d24383518ce0dacc0b3c4a38b6f1a514","title":"Recursive Decoding: A Situated Cognition Approach to Compositional Generation in Grounded Language Understanding"},{"paperId":"9fe39e0f2cf3b6d5f70a379654f9c08ffa48ddee","title":"Unobserved Local Structures Make Compositional Generalization Hard"},{"paperId":"ace2a00425f96e9d0dbbe2869023d56c6c91267f","title":"On Learning Interpreted Languages with Recurrent Models"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"40b4d98588719407fb72a014ab79e4145695654b","title":"Quantifying Adaptability in Pre-trained Language Models with 500 Tasks"},{"paperId":"e968a3c9590481cd13f2f86a7ac8839e3cf3455f","title":"Improving Compositional Generalization with Self-Training for Data-to-Text Generation"},{"paperId":"8008348e87d3904842a2dd230c14b83112e8bf48","title":"Compositional Generalization in Dependency Parsing"},{"paperId":"ab72bccf6f3981537389510ecc609109e79595c3","title":"Disentangled Sequence to Sequence Learning for Compositional Generalization"},{"paperId":"aead4418733b998792deb9cbf198a834449e00d2","title":"Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics"},{"paperId":"b3f644a5ea1fdd8cec1c34ebed69125838a50de3","title":"The Paradox of the Compositionality of Natural Language: A Neural Machine Translation Case Study"},{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"ad331dce175b1d38d6516455013c1ec0e26e606b","title":"Compositional Generalization in Multilingual Semantic Parsing over Wikidata"},{"paperId":"04833b92c9002f241b8f8b956d018759eebc85b3","title":"Tailor: Generating and Perturbing Text with Semantic Controls"},{"paperId":"10e824b010c2125b13decef30cf1dceec6196817","title":"ANLIzing the Adversarial Natural Language Inference Dataset"},{"paperId":"90c1a63aada7704eadc4324c16a66ec793d4b698","title":"Compositional generalization with a broad-coverage semantic parser"},{"paperId":"d129841cb2e30e25000dcd9edb83c880fc4babc1","title":"Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention"},{"paperId":"45291baa6a05ef8319e07442de5f6a5c85c16611","title":"SemEval-2022 Task 9: R2VQ – Competence-based Multimodal Question Answering"},{"paperId":"a143cac1bc440135b612132c89e603f364b8a3b7","title":"Combine to Describe: Evaluating Compositional Generalization in Image Captioning"},{"paperId":"dc88d2bbcebd810d7c80ba281739908005b12235","title":"Neurocompositional computing in human and machine intelligence: A tutorial"},{"paperId":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"DECOMPOSITION IN NEURAL PROGRAM SYNTHESIS"},{"paperId":"a576512a7562597fd30719a834d5866d010ef6ab","title":"Compositional Generalization for Natural Language Interfaces to Web APIs"},{"paperId":"fcf25e1affc2f8ee5bb49d156f174e9769234deb","title":"Systematic Generalization with Edge Transformers"},{"paperId":"cbf98ebe967e0f3f3236e7932f37013b98244e94","title":"ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning"},{"paperId":"04db9b694280134f09af5fa787a306907edba29d","title":"How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN"},{"paperId":"c764ecba2bace12b9bfb9c2b0651a12ff6888ea7","title":"Learning to Generalize Compositionally by Transferring Across Semantic Parsing Tasks"},{"paperId":"00050c15896e8ae6bb534f10d072351547993f72","title":"LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing"},{"paperId":"4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80","title":"How BPE Affects Memorization in Transformers"},{"paperId":"bb0ab8591d6d57c7e2bd1ec35d806b3f25277752","title":"Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks"},{"paperId":"76c9558b3fa10baf0e094386a650015b29a8a4bc","title":"Compositional generalization in semantic parsing with pretrained transformers"},{"paperId":"c2dda1f632072a331a50398d613079b71d76ee95","title":"Transformers Generalize Linearly"},{"paperId":"06fbeaf4d16639f177973a06cd7c4f78cb5e38ed","title":"COVR: A Test-Bed for Visually Grounded Compositional Generalization with Real Images"},{"paperId":"3962f108081b22c7e54b413f47ba6f2c16f2cc05","title":"Frequency Effects on Syntactic Rule Learning in Transformers"},{"paperId":"676fa805bd715591f99bb17e36d673a6a14e92fe","title":"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization"},{"paperId":"b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347","title":"Sequence-to-Sequence Learning with Latent Neural Grammars"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"45496cd0b256b75bfbe3bd95890b496069c7821c","title":"Multilingual Compositional Wikidata Questions"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","title":"Lexicon Learning for Few Shot Sequence Modeling"},{"paperId":"2a6ea3d8ad7ef4fb1ca0b1568748c284c156eb42","title":"It's the Meaning That Counts: The State of the Art in NLP and Semantics"},{"paperId":"577d44a10b424a55165a6bf4839bafce2c695302","title":"SyGNS: A Systematic Generalization Testbed Based on Natural Language Semantics"},{"paperId":"52b1cf563d1368f72e82b91b0349a7012a746f4f","title":"On the Interplay Between Fine-tuning and Composition in Transformers"},{"paperId":"ef06ddd84acea3c6dddd5a86eb500f11f828e07a","title":"Designing Multimodal Datasets for NLP Challenges"},{"paperId":"2365410a710b421b2295cdca0074946cb50bb1d4","title":"Are Pre-trained Convolutions Better than Pre-trained Transformers?"},{"paperId":"40c3327a6ddb0603b6892344509c7f428ab43d81","title":"Documenting the English Colossal Clean Crawled Corpus"},{"paperId":"77a096d80eb4dd4ccd103d1660c5a5498f7d026b","title":"Dynabench: Rethinking Benchmarking in NLP"},{"paperId":"eaa88d697f92739f3569564329e9d037aabbe2d7","title":"A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics"},{"paperId":"5505d608a1d482fdc083796db812379ec1cb8723","title":"Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches"},{"paperId":"0d39d525f30609d0541330f933007025cd457a83","title":"Exploring Transitivity in Neural NLI Models through Veridicality"},{"paperId":"40848b41ed8c9c255ecd8a920006877691b52d03","title":"WILDS: A Benchmark of in-the-Wild Distribution Shifts"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"642492003112a47b0bf86d60fac5507bc3b35a49","title":"Are Pretrained Convolutions Better than Pretrained Transformers?"},{"paperId":"0b1470014bdbaa80ba63da0491d9db6c7d4febcc","title":"Detecting Compositionally Out-of-Distribution Examples in Semantic Parsing"},{"paperId":"a406701b5fb05be55244d4f940db7be55fce85c6","title":"Semantic Systematicity in Connectionist Language Production"},{"paperId":"7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8","title":"AND does not mean OR: Using Formal Languages to Study Language Models’ Representations"},{"paperId":"21210677494669cc83a7878fbd3a6360ce02f7f3","title":"Testing for Grammatical Category Abstraction in Neural Language Models"},{"paperId":"4b58367375466e653751a0c258b2f50bd3551408","title":"Sequence-to-Sequence Networks Learn the Meaning of Reflexive Anaphora"},{"paperId":"6d0cc01e4bdf18b86bb74d1c6d9a41b5a4890c58","title":"Priorless Recurrent Networks Learn Curiously"},{"paperId":"0dc5dd7c64ee016bdc33a5f32dc25747be5ca702","title":"From SCAN to Real Data: Systematic Generalization via Meaningful Learning"},{"paperId":"6f0be1f9bda7530b1fa654cac84d595ca9d53740","title":"Revisit Systematic Generalization via Meaningful Learning"},{"paperId":"79cb080c84da314c2113692585b1e9ee29afa33a","title":"On learning an interpreted language with recurrent models"}],"references":[{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"815a3d56401483b635cfad9468852cddb46350ee","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":"ba310a9b9f7b812478b2a08dbd9917ea937a4e36","title":"Mutual exclusivity as a challenge for deep neural networks"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"356645552f8f40adf5a99b4e3a69f47699399010","title":"Quantity doesn’t buy quality syntax with neural language models"},{"paperId":"2030b07a36a77821b978ff8a20a14d88c1c60db1","title":"Mutual exclusivity as a challenge for neural networks"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"4043a936960de8e149dc208178fe1bcb157c7fa4","title":"Recent Advances in Natural Language Inference: A Survey of Benchmarks, Resources, and Approaches"},{"paperId":"42ed4a9994e6121a9f325f5b901c5b3d7ce104f5","title":"Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"},{"paperId":"bc488b437a53849c54c93e1a4c0eb87a942ddd2c","title":"Demonstratives"},{"paperId":"4d00097433a538002b36cfd7a621daddde3e4c0d","title":"Targeted Syntactic Evaluation of Language Models"},{"paperId":"3d42ddf7c5ce59ae04d1d27085be9f736d1be04b","title":"Colorless Green Recurrent Networks Dream Hierarchically"},{"paperId":"bdea8b6ceabaeb86bd23c2d2585da1ff3858d968","title":"Can Neural Networks Understand Logical Entailment?"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":null,"title":"A sequence-tosequence model for semantic role labeling"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"faee0c81a1170402b149500f1b91c51ccaf24027","title":"Universal Semantic Parsing"},{"paperId":"aab5002a22b9b4244a8329b140bd0a86021aa2d1","title":"OpenNMT: Open-Source Toolkit for Neural Machine Translation"},{"paperId":"03eb382e04cca8cca743f7799070869954f1402a","title":"CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"},{"paperId":null,"title":"2017) as closely as possible. The training time for each model was around 1 to 2 hours on a single NVIDIA K80"},{"paperId":null,"title":"Mark Steedman, and Mirella Lapata"},{"paperId":"558ac446dc26bee9789d660a251b75728cb6eeb2","title":"Language to Logical Form with Neural Attention"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"7c05a4ffee7e159e34b2efea7e44d994333ec628","title":"Recursive Neural Networks Can Learn Logical Semantics"},{"paperId":"15e8358e7be027a7bdfa5ad7a5f794a41456c83b","title":"Syntactic generalization with novel intransitive verbs."},{"paperId":"9ee949dc55a12849afb60c2d883e0bbc11d781df","title":"Zipf’s word frequency law in natural language: A critical review and future directions"},{"paperId":"187b06d8a18cbf7c5d9df4e232bd6649fc3f5a27","title":"MacArthur‐Bates Communicative Development Inventories"},{"paperId":"571a82875226d4e2f6189e25b346bb48b70809b2","title":"Once is Enough: N400 Indexes Semantic Integration of Novel Word Meanings from a Single Exposure in Context"},{"paperId":"84e3829aaa9d60cd42b2bc45ad1b67b63b23e14e","title":"The learnability of abstract syntactic principles"},{"paperId":null,"title":"PP modification on the subject of a declarative sentence occurred only 13 times whereas PP modification on the object occurred over 100 times"},{"paperId":"0831500f97b619e75a15160e0fe84729726d95f4","title":"Learning to use words: Event-related potentials index single-shot contextual word learning"},{"paperId":"4ed5b385d3c56f021833651f5ccbd37f3bf57c85","title":"Predicting syntax: Processing dative constructions in American and Australian varieties of English"},{"paperId":"c73dc3e30849e72f5465a681ad2bdb3878d0fcf5","title":"Syntactic recursion and iteration"},{"paperId":"5de1e28a1c538142b7c450194cc0a36b4934a45b","title":"Wide-Coverage Semantic Analysis with Boxer"},{"paperId":"e73da7791d71552a39e26d1ddbe725163ed7e51e","title":"Frequency of Basic English Grammatical Structures: A Corpus Analysis."},{"paperId":"9779c706c7e9e4b3ed14f65da70896ad59577887","title":"Early syntactic productivity: Evidence from dative shift"},{"paperId":"3adeb68c3a172de451433ea097576a1f32e331c6","title":"Situation variables and licensing by modification in opaque demonstratives"},{"paperId":"16179fb9a36e1eafb4d2d78329210397fda0d56f","title":"Individual differences in preschoolers ' ability to generalize unaccusative intransitive constructions in novel verb experiments : Evidence from their familiar verb usage in naturalistic play contexts"},{"paperId":"14f0c5bf45ad0dc55c80c8a772484d7bd3d6ee20","title":"MacArthur-Bates Communicative Development Inventories"},{"paperId":"a5dd6c4943f8cf774f9da441f9db73a1217f19e9","title":"Young children ’ s use of unaccusative intransitives in novel verb experiments"},{"paperId":"99d2dcdcf4cf05facaa101a48c7e31d140b4736d","title":"The Proposition Bank: An Annotated Corpus of Semantic Roles"},{"paperId":"bb6898d6041e97c4946661b3a3df0f82286a43b5","title":"Verbnet: a broad-coverage, comprehensive verb lexicon"},{"paperId":"beaca3493aed271bdfc42490fd22dd11cb40ce0e","title":"The faculty of language: what is it, who has it, and how did it evolve?"},{"paperId":"da75b3cbeeb22c75c5b2b66af9446718c6c1565d","title":"The (Non)Necessity of Recursion in Natural Language Processing"},{"paperId":"003969511c37448306fb951dba378c6c8561f4ce","title":"Word Frequencies in Written and Spoken English: based on the British National Corpus"},{"paperId":"45313a157dcaa1103eaa53f099204c66c7dd9cd9","title":"Toward a connectionist model of recursion in human linguistic performance"},{"paperId":"83fc20d0f34e794433079fe38da914d41cf84a87","title":"Young children learn to produce passives with nonce verbs."},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"f3c17a95ea5a5db6c8bcfe07d497b6c469ffa732","title":"Severing the External Argument from its Verb"},{"paperId":"0736a262b557c86e8d14cc7577bb94de5067d65e","title":"Systematicity in Connectionist Language Learning"},{"paperId":"59b39a93dd135b93c2d109692e72624fb8a4a9a4","title":"Twenty-Three-Month-Old Children Have a Grammatical Category of Noun."},{"paperId":"6cbc1eb25f4ab29a613418b3b0740e74141a0f17","title":"English Verb Classes and Alternations: A Preliminary Investigation"},{"paperId":null,"title":"Events in the Semantics of English, volume 334"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":null,"title":"Syntactic theory and the projection problem"},{"paperId":"1b5ad278a01a2c91f35c4c86fbbffc09d6fe2d72","title":"ENGLISH AS A FORMAL LANGUAGE"},{"paperId":null,"title":"Short-term memory limitations on decoding selfembedded sentences"}],"id":"b20ddcbd239f3fa9acc603736ac2e4416302d074","summary":"In experiments with Transformers and LSTMs, it is found that in-distribution accuracy on the COGS test set was near-perfect, but generalization accuracy was substantially lower, and the dataset showed high sensitivity to random seed."},{"url":"https://www.semanticscholar.org/paper/70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize","venue":"ACL","year":2021,"referenceCount":46,"citationCount":29,"influentialCitationCount":5,"publicationDate":"06/08/2021","authors":"Henry Conklin,Bailin Wang,Kenny Smith,Ivan Titov","citations":[{"paperId":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models"},{"paperId":"b49ebf36a29cf9734313066129ab0d7092d4041e","title":"Categorizing Semantic Representations for Neural Machine Translation"},{"paperId":"ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","title":"Compositional Generalisation with Structured Reordering and Fertility Layers"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"40047a74b707743157051d38f76061ba5ff9aab4","title":"Compositional Semantic Parsing with Large Language Models"},{"paperId":"e7b025f8bcd7e7ca0db65f666a99524d738a4717","title":"Exploring diversity in back translation for low-resource machine translation"},{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"a122909a31acf41cb2d9eb602c01b24b9b85a061","title":"LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing"},{"paperId":"bc16284f517dd0011dcf64ea1c8fe6d6576494a4","title":"Is the Computation of Abstract Sameness Relations Human-Like in Neural Language Models?"},{"paperId":"a40693eefd351659cdeb3885917b1506ea01c38a","title":"Measuring and Improving Compositional Generalization in Text-to-SQL via Component Alignment"},{"paperId":"43b7437ed33a29d3d90239ad66f325a465ff7e91","title":"Meta Learning for Natural Language Processing: A Survey"},{"paperId":"6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis"},{"paperId":"4c430e6c3a72626bd4cb1893960c7c26dfec6c79","title":"Structurally Diverse Sampling Reduces Spurious Correlations in Semantic Parsing Datasets"},{"paperId":"69078af65fc934f81fd340e9d1323d6c08194548","title":"Revisiting the Compositional Generalization Abilities of Neural Sequence Models"},{"paperId":"1d41a0ddda57caa6c8d268dd1703e4c9b35db18b","title":"One-Shot Learning from a Demonstration with Hierarchical Latent Language"},{"paperId":"557ebd17b7c7ac4e09bd167d7b8909b8d74d1153","title":"Compositional Generalization Requires Compositional Parsers"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"ab72bccf6f3981537389510ecc609109e79595c3","title":"Disentangled Sequence to Sequence Learning for Compositional Generalization"},{"paperId":"90c1a63aada7704eadc4324c16a66ec793d4b698","title":"Compositional generalization with a broad-coverage semantic parser"},{"paperId":"a143cac1bc440135b612132c89e603f364b8a3b7","title":"Combine to Describe: Evaluating Compositional Generalization in Image Captioning"},{"paperId":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"DECOMPOSITION IN NEURAL PROGRAM SYNTHESIS"},{"paperId":"c764ecba2bace12b9bfb9c2b0651a12ff6888ea7","title":"Learning to Generalize Compositionally by Transferring Across Semantic Parsing Tasks"},{"paperId":"00050c15896e8ae6bb534f10d072351547993f72","title":"LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing"},{"paperId":"bb0ab8591d6d57c7e2bd1ec35d806b3f25277752","title":"Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks"},{"paperId":"676fa805bd715591f99bb17e36d673a6a14e92fe","title":"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization"},{"paperId":"b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347","title":"Sequence-to-Sequence Learning with Latent Neural Grammars"},{"paperId":"c735740b26ceaa4db9d77233116434c0e8b311d8","title":"Learning Adaptive Control Flow in Transformers for Improved Systematic Generalization"},{"paperId":"a406701b5fb05be55244d4f940db7be55fce85c6","title":"Semantic Systematicity in Connectionist Language Production"},{"paperId":"6f0be1f9bda7530b1fa654cac84d595ca9d53740","title":"Revisit Systematic Generalization via Meaningful Learning"}],"references":[{"paperId":"95c20f35d352f23b19c378c0758b8dc1d7622872","title":"On Aspects of the Theory of Syntax"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"19bd467b1c8de94b9bdaef1499788467937f594e","title":"Meta-Learning for Domain Generalization in Semantic Parsing"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"68d5ffe0637e704ab2ff25e93e631f76790c5707","title":"Characterizing Structural Regularities of Labeled Data in Overparameterized Models"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"bb82a8d24d8a564cbbbe04e6752451e8260a966a","title":"Understanding Human Intelligence through Human Limitations"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"d70af4990cba2574c41b1235030f7a5b702e2d70","title":"Compositionality and Generalization In Emergent Languages"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"0c5bc409e62e65f86838968a2a7cdae5fa0b288b","title":"RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":null,"title":"Underspecification Presents Challenges for Credibility"},{"paperId":null,"title":"Learning to recombine and resam"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"4d031258a66076187001b4d6182345198624d872","title":"The compositionality of neural networks: integrating symbolism and connectionism"},{"paperId":"e962c301df1d33bc12d8115f4c82093103c94eeb","title":"Model-Agnostic Meta-Learning for Relation Classification with Limited Supervision"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"efef34c1caef102ad5cc052642d75beaaf5adcaf","title":"Deep RNNs Encode Soft Hierarchical Syntax"},{"paperId":"97856a4c31fec7b189446a130aab4cbfa8d6a3e8","title":"Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":null,"title":"Learning to generalize: Metalearning for domain generalization"},{"paperId":"dcb028149bb3cf934fbd2e4cbb773ffbb9b0e49d","title":"Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2caa021d85d4878d3369000e0068f617576d6cca","title":"Natural Language Does Not Emerge ‘Naturally’ in Multi-Agent Dialog"},{"paperId":"c889d6f98e6d79b89c3a6adf8a921f88fa6ba518","title":"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"},{"paperId":"29c887794eed2ca9462638ff853e6fe1ab91d5d8","title":"Optimization as a Model for Few-Shot Learning"},{"paperId":"be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6","title":"Matching Networks for One Shot Learning"},{"paperId":"b7eac64a8410976759445cce235469163d23ee65","title":"Data Recombination for Neural Semantic Parsing"},{"paperId":"558ac446dc26bee9789d660a251b75728cb6eeb2","title":"Language to Logical Form with Neural Attention"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"0cddfeeab92d9abbd9b42065e916fe1995a7c2e6","title":"Formal Semantics : an Introduction"},{"paperId":"c393c48a2d634acb1daee3567eaaea733a1224fb","title":"Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees"},{"paperId":"e6c7adc28e20d361d5c35aa9808094b10f6a34d1","title":"Convolution Kernels for Natural Language"},{"paperId":"f330f1f472f860212b980bb9be81eff884f7f0e1","title":"Text Classification using String Kernels"},{"paperId":"5ee0d8aeb2cb01ef4d8a858d234e72a7400c03ac","title":"Convolution kernels on discrete structures"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"277ff0c74cc72663d0aabbeae25a3e97b245457c","title":"Simple Fast Algorithms for the Editing Distance Between Trees and Related Problems"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":"de28c165623adabcdba0fdb18b65eba685aaf31d","title":"On Estimation of a Probability Density Function and Mode"}],"id":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","summary":"A meta-learning augmented version of supervised learning whose objective directly optimizes for out-of-distribution generalization is implemented, and Experimental results on the COGS and SCAN datasets show that this similarity-driven meta- learning can improve generalization performance."},{"url":"https://www.semanticscholar.org/paper/ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers","venue":"EMNLP","year":2021,"referenceCount":55,"citationCount":46,"influentialCitationCount":10,"publicationDate":"08/26/2021","authors":"R. Csordás,Kazuki Irie,J. Schmidhuber","citations":[{"paperId":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models"},{"paperId":"92394181881a9ff4063d9aedb3e4fd4ada466edb","title":"Functional Indirection Neural Estimator for Better Out-of-distribution Generalization"},{"paperId":"04ca71d9089b2e86f9e4a874fd66ee8bd0baba8e","title":"When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks"},{"paperId":"97833e2aa0da5240e62436373b58af988a4ab6ab","title":"The Curious Case of Absolute Position Embeddings"},{"paperId":"1ed29beb55b10de8553c926ce6da2625ec2c8776","title":"Benchmarking Long-tail Generalization with Likelihood Splits"},{"paperId":"b49ebf36a29cf9734313066129ab0d7092d4041e","title":"Categorizing Semantic Representations for Neural Machine Translation"},{"paperId":"b1f33e956e36bf25e118c0d537dcc519cfe52e60","title":"CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations"},{"paperId":"ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","title":"Compositional Generalisation with Structured Reordering and Fertility Layers"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"61d56ece2d19f4bfeb322c92085fb28521e169da","title":"Neural-Symbolic Recursive Machine for Systematic Generalization"},{"paperId":"837cc9a366c873c84ceec7e84d5cb3d5753757d6","title":"Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks"},{"paperId":"0ac7966e8146799d49c9d5212bbd20bc85054bd8","title":"Training on the Test Set: Mapping the System-Problem Space in AI"},{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"a122909a31acf41cb2d9eb602c01b24b9b85a061","title":"LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing"},{"paperId":"6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis"},{"paperId":"aa8f3e081ad2869c9469e2726364bdae0d9bdc7f","title":"Fusing finetuned models for better pretraining"},{"paperId":"5021fd710fd17dee53bc7bc7bf334b148ef3d8b6","title":"LogicInference: A New Dataset for Teaching Logical Inference to seq2seq Models"},{"paperId":"66f3f0e8ebc780e570770986f50bf9cb9cd53ec1","title":"WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series Tasks"},{"paperId":"4c430e6c3a72626bd4cb1893960c7c26dfec6c79","title":"Structurally Diverse Sampling Reduces Spurious Correlations in Semantic Parsing Datasets"},{"paperId":"69078af65fc934f81fd340e9d1323d6c08194548","title":"Revisiting the Compositional Generalization Abilities of Neural Sequence Models"},{"paperId":"557ebd17b7c7ac4e09bd167d7b8909b8d74d1153","title":"Compositional Generalization Requires Compositional Parsers"},{"paperId":"9fe39e0f2cf3b6d5f70a379654f9c08ffa48ddee","title":"Unobserved Local Structures Make Compositional Generalization Hard"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"b8b813111c411ae61881ab9cd25707d9de6444ec","title":"Compositional Attention: Disentangling Search and Retrieval"},{"paperId":"e528466e2aff981511d4ca6e063211297c0b4175","title":"The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization"},{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"90c1a63aada7704eadc4324c16a66ec793d4b698","title":"Compositional generalization with a broad-coverage semantic parser"},{"paperId":"20867dd60552674beac1edc7fa029d2d1b8fd03a","title":"MAQA: A Multimodal QA Benchmark for Negation Anonymous Author(s)"},{"paperId":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"DECOMPOSITION IN NEURAL PROGRAM SYNTHESIS"},{"paperId":"a77468f6bd4db7f8d761a0569d9cc29d5a8f0034","title":"L OGIC I NFERENCE : A N EW D ATASET FOR T EACHING L OGICAL I NFERENCE TO SEQ 2 SEQ M ODELS"},{"paperId":"42ab2e42221a7fbd66ba368cf90b5e63b5270010","title":"Improving Baselines in the Wild"},{"paperId":"2c33f2aed89a7b04c2509b897e5fcccccdb2b7b6","title":"Assistive Tele-op: Leveraging Transformers to Collect Robotic Task Demonstrations"},{"paperId":"fcf25e1affc2f8ee5bb49d156f174e9769234deb","title":"Systematic Generalization with Edge Transformers"},{"paperId":"bfd1752963697520ceb484a8b8c65b9dba99ca96","title":"TraVLR: Now You See It, Now You Don't! Evaluating Cross-Modal Transfer of Visio-Linguistic Reasoning"},{"paperId":"00050c15896e8ae6bb534f10d072351547993f72","title":"LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing"},{"paperId":"3d5699e7f7e085ad72102859b06fa4884d207e77","title":"Iterative Decoding for Compositional Generalization in Transformers"},{"paperId":"76c9558b3fa10baf0e094386a650015b29a8a4bc","title":"Compositional generalization in semantic parsing with pretrained transformers"},{"paperId":"af749e5dbde38914ca6fa1463fca17eac8f69ecc","title":"ReaSCAN: Compositional Reasoning in Language Grounding"},{"paperId":"b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347","title":"Sequence-to-Sequence Learning with Latent Neural Grammars"},{"paperId":"86589b6286ef3c55b8b4fccfb41a3b30b7afdf61","title":"Going Beyond Linear Transformers with Recurrent Fast Weight Programmers"},{"paperId":"72f207c777e4a17180cc54ccc6a743d5f43227af","title":"Choose a Transformer: Fourier or Galerkin"},{"paperId":"eaa88d697f92739f3569564329e9d037aabbe2d7","title":"A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics"},{"paperId":"269de1d1e26559613fa4b02320aefc07bb2d556b","title":"Enhancing the Transformer Decoder with Transition-based Syntax"},{"paperId":"c735740b26ceaa4db9d77233116434c0e8b311d8","title":"Learning Adaptive Control Flow in Transformers for Improved Systematic Generalization"},{"paperId":"6f0be1f9bda7530b1fa654cac84d595ca9d53740","title":"Revisit Systematic Generalization via Meaningful Learning"},{"paperId":"0dc5dd7c64ee016bdc33a5f32dc25747be5ca702","title":"From SCAN to Real Data: Systematic Generalization via Meaningful Learning"}],"references":[{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"86589b6286ef3c55b8b4fccfb41a3b30b7afdf61","title":"Going Beyond Linear Transformers with Recurrent Fast Weight Programmers"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":"1a703f08da01cf737cce3fb9064259b3f4b44e9c","title":"Linear Transformers Are Secretly Fast Weight Programmers"},{"paperId":"51f46cb42668cfe3745ecf029d032bf30253574f","title":"GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"649c758b0e59ddedaae37a3757e8eabdba664e5a","title":"Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"d9610589189e0821500516994dcee543a558b70c","title":"Learning advanced mathematical computations from examples"},{"paperId":null,"title":"Lexicon learning for few-shot neural sequence modeling"},{"paperId":"d642868ce4325ebf3026c0aa0c497a079f112a8d","title":"On the Binding Problem in Artificial Neural Networks"},{"paperId":"986cc3d0e3afb23f84564ea9588b7b8e9c3e1dd6","title":"Hierarchical Poset Decoding for Compositional Generalization in Language"},{"paperId":"227fe850a72fab24998c7e08d75db214715dc74e","title":"The EOS Decision and Length Extrapolation"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"e00484961fb2f30d2d48a5f9853fa3ebab140cac","title":"Improving Transformer Optimization Through Better Initialization"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"937fef6a786c4463a3bb19770c704945d1600b66","title":"Learning Compositional Rules via Neural Program Synthesis"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"ea415809bf87ef4b99966c6c50de6cb996a02a97","title":"Deep double descent: where bigger models and more data hurt"},{"paperId":"815a3d56401483b635cfad9468852cddb46350ee","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"80deaca65c2c155bd15718eeecff584841eb25b0","title":"CLOSURE: Assessing Systematic Generalization of CLEVR Models"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"d88f31a0091eee02c5a2aa2013914818cdef114e","title":"Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving"},{"paperId":"4cf963e5fd88825ac62ad6cce364447e5d2dfb2b","title":"Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention"},{"paperId":"07fa8c8a703abd7496f4781e9dee53d5de9c8717","title":"Neural Shuffle-Exchange Networks - Sequence Processing in O(n log n) Time"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"b39efed2e73357db4691f66935cf62e7b51f30e1","title":"Transcoding Compositionally: Using Attention to Find More Generalizable Solutions"},{"paperId":"d5535d4da15a7a8dfbeb34f61cddb4874bbc56e0","title":"Improving Differentiable Neural Computers Through Memory Masking, De-allocation, and Link Distribution Sharpness Control"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"c4744a7c2bb298e4a52289a1e085c71cc3d37bc6","title":"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"bb6d3644fa5675351a4a05fe8b925416dc091c3c","title":"Measuring Generalization and Overfitting in Machine Learning"},{"paperId":null,"title":"Here, we use a decomposed attention matrix of the following form: A"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","title":"Self-Attention with Relative Position Representations"},{"paperId":"08fbb1b4cfdc83977d2c8f08bdfb663f13c0e60a","title":"Memorize or generalize? Searching for a compositional RNN in a haystack"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":null,"title":"We never combine absolute with relative positional embedding. In case of a relative positional variant of any Transformer model, we do not add absolute positional encoding to the word"},{"paperId":"784ee73d5363c711118f784428d1ab89f019daa5","title":"Hybrid computing using a neural network with dynamic external memory"},{"paperId":"04cca8e341a5da42b29b0bc831cb25a0f784fa01","title":"Adaptive Computation Time for Recurrent Neural Networks"},{"paperId":"5e4eb58d5b47ac1c73f4cf189497170e75ae6237","title":"Neural GPUs Learn Algorithms"},{"paperId":"d6f2f611da110b5b5061731be3fc4c7f45d8ee23","title":"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f","title":"Sequence Transduction with Recurrent Neural Networks"},{"paperId":"152d82025f02916019e4cfcc943dceecc159cda4","title":"Self-Delimiting Neural Networks"},{"paperId":"b71ac1e9fb49420d13e084ac67254a0bbd40f83f","title":"Understanding the difficulty of training deep feedforward neural networks"},{"paperId":null,"title":"Learning to control fastweight memories: An alternative to recurrent nets"},{"paperId":"2b83ce7567bad29afe4753eea7e72521dffdb075","title":"Connectionism and the problem of systematicity: Why Smolensky's solution doesn't work"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"}],"id":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","summary":"By revisiting model configurations as basic as scaling of embeddings, early stopping, relative positional embedding, and Universal Transformer variants, this work can drastically improve the performance of Transformers on systematic generalization."},{"url":"https://www.semanticscholar.org/paper/557ebd17b7c7ac4e09bd167d7b8909b8d74d1153","title":"Compositional Generalization Requires Compositional Parsers","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/24/2022","authors":"Pia Weissenhorn,Yuekun Yao,L. Donatelli,Alexander Koller","citations":[],"references":[{"paperId":"ab72bccf6f3981537389510ecc609109e79595c3","title":"Disentangled Sequence to Sequence Learning for Compositional Generalization"},{"paperId":"76c9558b3fa10baf0e094386a650015b29a8a4bc","title":"Compositional generalization in semantic parsing with pretrained transformers"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"},{"paperId":"523745e29f6cb1890f18352d449fd3597910c485","title":"Improving Compositional Generalization in Classification Tasks via Structure Annotations"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"d34cac6a7101068f6ba6b9e08d169340b2589595","title":"Learning compositional structures for semantic graph parsing"},{"paperId":"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","title":"Lexicon Learning for Few Shot Sequence Modeling"},{"paperId":"25e7c9dcc294d77d184c4c1122c8304cdb58c69d","title":"One SPRING to Rule Them Both: Symmetric AMR Semantic Parsing and Generation without a Complex Pipeline"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"642492003112a47b0bf86d60fac5507bc3b35a49","title":"Are Pretrained Convolutions Better than Pretrained Transformers?"},{"paperId":null,"title":"Making transformers solve"},{"paperId":null,"title":"Unlike them we didn’t use the fixed-tree decoder (described in Groschwitz et al. 2018), but opted for the projective A* decoder (Lindemann"},{"paperId":"36d1b02b4cd504ae8641556457a673ba1044b6a9","title":"Compositionality"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"60d99e00f7f96efea3099b9491a93bb8060ff502","title":"Fast Semantic Parsing with Well-typedness Guarantees"},{"paperId":"b0ea633e0c22fbd8cbc531c7326376725d16ce25","title":"Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks"},{"paperId":null,"title":"BART: Denoising sequence-to-sequence pretraining for natural language"},{"paperId":"671a05535da65f9fc22800b5aa94795fc670ac45","title":"Compositional Semantic Parsing across Graphbanks"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"0f4a3b7f835737c7e004073f4a873e356c0063b4","title":"Incorporating Source Syntax into Transformer-Based Neural Machine Translation"},{"paperId":"3abc5ffb1757ec3f35cb7b4100410570b0b51e09","title":"LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better"},{"paperId":"25109699b098c786832c906e4b36fa76fb2b66a0","title":"AMR dependency parsing with a typed semantic algebra"},{"paperId":"efef34c1caef102ad5cc052642d75beaaf5adcaf","title":"Deep RNNs Encode Soft Hierarchical Syntax"},{"paperId":"928f9dccb806a3278d20d82cc53781c5f44e2bb1","title":"Constituency Parsing with a Self-Attentive Encoder"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"ebb222fff7b71b82d1a5971e198982858abcd03d","title":"Modeling Source Syntax for Neural Machine Translation"},{"paperId":"3aa52436575cf6768a0a1a476601825f6a62e58f","title":"Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies"},{"paperId":"eec3a236ecd185712ce65fb336141f8656eea13d","title":"Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations"},{"paperId":"bf0f141bae83bd6d5ca0c37839d53f0d06059b34","title":"Controlling Politeness in Neural Machine Translation via Side Constraints"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"e72e5ee5de14fd463ab58ce830474157258e3578","title":"Abstract Meaning Representation for Sembanking"},{"paperId":"6843890926bf0e5c887ffc78dcb1203135981bf1","title":"The compositionality papers"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":null,"title":"Syntactic Structures"},{"paperId":null,"title":"Agent NP to Unacc Subj 96"},{"paperId":null,"title":"Prim to Obj (proper noun) 10"}],"id":"557ebd17b7c7ac4e09bd167d7b8909b8d74d1153","summary":"The accuracy of different 005 parsers on the recent COGS corpus is analyzed and the role of syntactic generalization in compo- 016 sitional generalization is analyzed."},{"url":"https://www.semanticscholar.org/paper/6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing","venue":"ArXiv","year":2022,"referenceCount":83,"citationCount":3,"influentialCitationCount":1,"publicationDate":"05/24/2022","authors":"Linlu Qiu,Peter Shaw,Panupong Pasupat,Tianze Shi,Jonathan Herzig,Emily Pitler,Fei Sha,Kristina Toutanova","citations":[],"references":[{"paperId":"c140fe515de2f20d0c85c813c7b3ec1defc41f9d","title":"Binding Language Models in Symbolic Languages"},{"paperId":"b06c41432dc060c5591b91f8ed40ae15913a150d","title":"SUBS: Subtree Substitution for Compositional Semantic Parsing"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"15190e8b459bd85d546286f7d7da61b4f4f3f58a","title":"What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"011a4019aa0d0ce3edfa56bb2ca1e7586eb43fb2","title":"Training Compute-Optimal Large Language Models"},{"paperId":"51000d9f79be0eefd7972fe94e3c71dddc90d2c6","title":"Evaluating the Text-to-SQL Capabilities of Large Language Models"},{"paperId":"87e02a265606f31e65986f3c1c448a3e3a3a066e","title":"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"},{"paperId":"29b77089a0a40f46372ce2dca9c3bb2dd5d46b1d","title":"Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution"},{"paperId":"e404bdfaa858b3c25540aa5d2c5dfe20c16ead37","title":"Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments"},{"paperId":"53c0abe83fe9b4fdaf2208295d8504fcf5241694","title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models"},{"paperId":"9fe39e0f2cf3b6d5f70a379654f9c08ffa48ddee","title":"Unobserved Local Structures Make Compositional Generalization Hard"},{"paperId":"6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a","title":"Few-Shot Semantic Parsing with Language Models Trained on Code"},{"paperId":"f9838a3be5c94bb2674a0e224de349b50e18f3c4","title":"Learning To Retrieve Prompts for In-Context Learning"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"2f05cb3dd8194276aa26c4e71841a86edb51914f","title":"The Power of Prompt Tuning for Low-Resource Semantic Parsing"},{"paperId":"43a87867fe6bf4eb920f97fc753be4b727308923","title":"Towards a Unified View of Parameter-Efficient Transfer Learning"},{"paperId":"c206a6e7f51f5e1b6bfc479a174b66ad88ada2db","title":"Exploring the Limits of Large Scale Pre-training"},{"paperId":"2d4f66046bb436864cd6bf589e3a931c405f9f44","title":"Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers"},{"paperId":"de1fdaf92488f2f33ddc0272628c8543778d0da9","title":"Scaling Laws for Neural Machine Translation"},{"paperId":"9289826beb6206eeaf500105f7329d6d5a495d8a","title":"Robust fine-tuning of zero-shot models"},{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"0adec918885dff698acf359988ed79a543157f80","title":"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"},{"paperId":"68f141724814839d556a989646194be88641b143","title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"c764ecba2bace12b9bfb9c2b0651a12ff6888ea7","title":"Learning to Generalize Compositionally by Transferring Across Semantic Parsing Tasks"},{"paperId":"f3a332ff1b73acda482e5d83696b2c701f487819","title":"P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"},{"paperId":"3d5699e7f7e085ad72102859b06fa4884d207e77","title":"Iterative Decoding for Compositional Generalization in Transformers"},{"paperId":"bb0ab8591d6d57c7e2bd1ec35d806b3f25277752","title":"Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks"},{"paperId":"5fbcfccd3736969d95ed660d8e6962c86b7a9113","title":"PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models"},{"paperId":"676fa805bd715591f99bb17e36d673a6a14e92fe","title":"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"c0e059c46aea358872b4760aed53c4da3beaaeee","title":"Structured Reordering for Modeling Latent Alignments in Sequence Transduction"},{"paperId":"88dbde378e9ac5c25fc7d78f5da147223e8d34d4","title":"Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"64a1dbdd7653eaca25c78e87335ee156b6f6959e","title":"Constrained Language Models Yield Few-Shot Semantic Parsers"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":"56fa0b9cba4d9aee5ccc327365b3b3a721031c69","title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models"},{"paperId":"ac3cdb50606f7770eef8e4cd951840a4f71287a0","title":"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"},{"paperId":"6b2b5d3d9a2ca4bc4fbd81551a62370be2fbff1b","title":"Explaining Neural Scaling Laws"},{"paperId":"4383a975c09b72ba2f1a77cd779bb6965dbfb2fb","title":"Scaling Laws for Transfer"},{"paperId":"d3edc20ed4a07195f3663abc0ead4220266fd75b","title":"*-CFQ: Analyzing the Scalability of Machine Learning on a Compositional Task"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"cc0fc60e9b9d036acf53899c259574c8ce2aedfc","title":"Compositional Generalization via Semantic Tagging"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"1a272eb83fbf3082ca6d6009963186bb23254b03","title":"Value-Agnostic Conversational Semantic Parsing"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":null,"title":"2022) as intermediate representation. For CFQ, we use the reversible intermediate representation in Herzig et al. (2021)"},{"paperId":null,"title":"Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing"},{"paperId":"3efbcfeeb0ea1051a71101d3318da4411081f0b8","title":"Scaling Laws for Autoregressive Generative Modeling"},{"paperId":"227fe850a72fab24998c7e08d75db214715dc74e","title":"The EOS Decision and Length Extrapolation"},{"paperId":"37882abaec01eba1bf5bda8a36c904aaea0d5642","title":"Improving Compositional Generalization in Semantic Parsing"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"f430773cdc0fdd62192a48a372601d6aa40f3d7b","title":"Task-Oriented Dialogue as Dataflow Synthesis"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"dd377d9132b2d89532032d69c342927f7b4a00a2","title":"Small Data, Big Decisions: Model Selection in the Small-Data Regime"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"937fef6a786c4463a3bb19770c704945d1600b66","title":"Learning Compositional Rules via Neural Program Synthesis"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"d28c18a3c2a0afdc0a8634d18345af8d36e1f948","title":"A Constructive Prediction of the Generalization Error Across Scales"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"126b5958cc19b7e40a1f0c07b0a261c5c5094225","title":"Scaling description of generalization with number of parameters in deep learning"},{"paperId":null,"title":"BART: Denoising sequence-to-sequence pretraining for natural language"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"6e7dcd769636465af1d8dddf891e7580ca8bc228","title":"Don’t paraphrase, detect! Rapid and Effective Data Collection for Semantic Parsing"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"a1c922be467d1c0c64b963e65dae41778b81b2a0","title":"Deep Learning Scaling is Predictable, Empirically"},{"paperId":"b7eac64a8410976759445cce235469163d23ee65","title":"Data Recombination for Neural Semantic Parsing"},{"paperId":"7260c0692f8d265e11c4e9c4c8ef4c185bd587ad","title":"Building machines that learn and think like people"},{"paperId":"47ced790a563344efae66588b5fb7fe6cca29ed3","title":"The Probabilistic Relevance Framework: BM25 and Beyond"},{"paperId":"f60d8dd8ca3a7dfa7d0a14988af73084ad93619d","title":"Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing"},{"paperId":"b7c0e47f8b768258b7d536c21b218e6c46ab8791","title":"Learning to Parse Database Queries Using Inductive Logic Programming"}],"id":"6e10343767ab09dde83cf99ea3442907402a9810","summary":"Limits of current techniques for effectively leveraging model scale for compositional generalization are highlighted, while the analysis also suggests promising directions for future work."},{"url":"https://www.semanticscholar.org/paper/ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","title":"Compositional Generalisation with Structured Reordering and Fertility Layers","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/06/2022","authors":"Matthias Lindemann,Alexander Koller,Ivan Titov","citations":[],"references":[{"paperId":"fbde3eb4857f888e2b6b2ef1c99ecf7e2e2c8ecc","title":"Unobserved Local Structures Make Compositional Generalization Hard"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"ab72bccf6f3981537389510ecc609109e79595c3","title":"Disentangled Sequence to Sequence Learning for Compositional Generalization"},{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"a576512a7562597fd30719a834d5866d010ef6ab","title":"Compositional Generalization for Natural Language Interfaces to Web APIs"},{"paperId":"b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347","title":"Sequence-to-Sequence Learning with Latent Neural Grammars"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","title":"Lexicon Learning for Few Shot Sequence Modeling"},{"paperId":"c0e059c46aea358872b4760aed53c4da3beaaeee","title":"Structured Reordering for Modeling Latent Alignments in Sequence Transduction"},{"paperId":"2cedd0a1841c0bad42e7a7a6ed3a9d0ffbb7d979","title":"StylePTB: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":null,"title":"2021) and disregard the order of the parameters for computing accuracy"},{"paperId":"a1c4ce9de92338646c6ee93c7c2e5ee366784b1a","title":"Benchmarking Meaning Representations in Neural Semantic Parsing"},{"paperId":"227fe850a72fab24998c7e08d75db214715dc74e","title":"The EOS Decision and Length Extrapolation"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"37882abaec01eba1bf5bda8a36c904aaea0d5642","title":"Improving Compositional Generalization in Semantic Parsing"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"815a3d56401483b635cfad9468852cddb46350ee","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"ec070add234e7ef8f7b80b91702e54a37d3be483","title":"Exact Hard Monotonic Attention for Character-Level Transduction"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"520ddb38b59b8fae2209ddc7c6640462cf153eec","title":"Sparse and Constrained Attention for Neural Machine Translation"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"593e4e749bd2dbcaf8dc25298d830b41d435e435","title":"A Minimal Span-Based Neural Constituency Parser"},{"paperId":"76faaf292c6d9dc29d3a99300a7fdd7a35d6d107","title":"Online and Linear-Time Attention by Enforcing Monotonic Alignments"},{"paperId":"13d9323a8716131911bfda048a40e2cde1a76a46","title":"Structured Attention Networks"},{"paperId":"f61da0efbb38bd3e6b9a9855809f5288b829f1f0","title":"Online Segment to Segment Neural Transduction"},{"paperId":"ba30df190664193514d1d309cb673728ed48f449","title":"Incorporating Copying Mechanism in Sequence-to-Sequence Learning"},{"paperId":"33108287fbc8d94160787d7b2c7ef249d3ad6437","title":"Modeling Coverage for Neural Machine Translation"},{"paperId":"ada937c9f51316c6ac87f9d1d4509383d23e0c21","title":"Incorporating Structural Alignment Biases into an Attentional Neural Translation Model"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"0db6eb46ca9941660acc775e3ca39bf4434c18be","title":"Hierarchical Phrase-Based Translation"},{"paperId":"5e1cecd26a1f52d887cdfccbbed89c24a7e6b201","title":"Quasi-Synchronous Grammars: Alignment by Soft Projection of Syntactic Dependencies"},{"paperId":"687dce9ac01f5996601655035c34b449c27c3b6a","title":"Learning for Semantic Parsing with Statistical Machine Translation"},{"paperId":"8dd9fd6a45afd266d48255c398429e01ea4fd6db","title":"Learning to Transform Natural to Formal Languages"},{"paperId":"4c915c1eecb217c123a36dc6d3ce52d12c742614","title":"Simple statistical gradient-following algorithms for connectionist reinforcement learning"},{"paperId":"de2df29b0a0312de7270c3f5a0af6af5645cf91a","title":"A Systematic Comparison of Various Statistical Alignment Models"},{"paperId":"13b6eeb28328252a35cdcbe3ab8d09d2a9caf99d","title":"Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora"},{"paperId":"1e49be76322d6e9446ab9e38103cb98a27abcd73","title":"Pattern Matching for Permutations"},{"paperId":"ab7b5917515c460b90451e67852171a531671ab8","title":"The Mathematics of Statistical Machine Translation: Parameter Estimation"},{"paperId":"a1066659ec1afee9dce586f6f49b7d44527827e1","title":"A Statistical Approach to Machine Translation"},{"paperId":"539036ab9e8f038c8a948596e77cc0dfcfa91fb3","title":"An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process"},{"paperId":"2a9d6137c95cc7c106bf84f18fb13449eded7826","title":"Syntax-Directed Transduction"}],"id":"ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","summary":"This work presents a end-to-end differentiable neural model that composes two structural operations: a fertility step, which is introduced in this work, and a reordering step based on previous work, which outperforms seq2seq models by a wide margin on challenging compositional splits of realis-tic semantic parsing tasks."},{"url":"https://www.semanticscholar.org/paper/90c1a63aada7704eadc4324c16a66ec793d4b698","title":"Compositional generalization with a broad-coverage semantic parser","venue":"STARSEM","year":2022,"referenceCount":28,"citationCount":3,"influentialCitationCount":1,"publicationDate":null,"authors":"Pia Weissenhorn,L. Donatelli,Alexander Koller","citations":[{"paperId":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models"},{"paperId":"c6e4518dfd687a2a5bed4e78d5d9f999292a1746","title":"Counterfactual Recipe Generation: Exploring Compositional Generalization in a Realistic Scenario"},{"paperId":"1bd799cf462f926041dd2fc8fbe4af54bddbf5c5","title":"Translate First Reorder Later: Leveraging Monotonicity in Semantic Parsing"}],"references":[{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"ab72bccf6f3981537389510ecc609109e79595c3","title":"Disentangled Sequence to Sequence Learning for Compositional Generalization"},{"paperId":"76c9558b3fa10baf0e094386a650015b29a8a4bc","title":"Compositional generalization in semantic parsing with pretrained transformers"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"},{"paperId":"d34cac6a7101068f6ba6b9e08d169340b2589595","title":"Learning compositional structures for semantic graph parsing"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","title":"Lexicon Learning for Few Shot Sequence Modeling"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"25e7c9dcc294d77d184c4c1122c8304cdb58c69d","title":"One SPRING to Rule Them Both: Symmetric AMR Semantic Parsing and Generation without a Complex Pipeline"},{"paperId":null,"title":"2021)’s settings. For training on train (but not train100), we set the vocabulary threshold from 7 down to 1 to account for the fact that the lexical generalizations"},{"paperId":null,"title":"Are pretrained convolutions better"},{"paperId":"36d1b02b4cd504ae8641556457a673ba1044b6a9","title":"Compositionality"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"60d99e00f7f96efea3099b9491a93bb8060ff502","title":"Fast Semantic Parsing with Well-typedness Guarantees"},{"paperId":"d16cfdb62b734c7d09a0ad38d00a9e1841a1568c","title":"Saarland at MRP 2019: Compositional parsing across all graphbanks"},{"paperId":"671a05535da65f9fc22800b5aa94795fc670ac45","title":"Compositional Semantic Parsing across Graphbanks"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"25109699b098c786832c906e4b36fa76fb2b66a0","title":"AMR dependency parsing with a typed semantic algebra"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":null,"title":"Spider: A largescale human-labeled dataset for complex and cross"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"eec3a236ecd185712ce65fb336141f8656eea13d","title":"Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations"},{"paperId":"6843890926bf0e5c887ffc78dcb1203135981bf1","title":"The compositionality papers"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":"b7c0e47f8b768258b7d536c21b218e6c46ab8791","title":"Learning to Parse Database Queries Using Inductive Logic Programming"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":null,"title":"Syntactic Structures"}],"id":"90c1a63aada7704eadc4324c16a66ec793d4b698","summary":"It is shown how the AM parser, a compositional semantic parser (Groschwitz et al., 2018) can solve compositional generalization on the COGS dataset and is the first semantic parser that achieves high accuracy on both naturally occurring language and the syntheticCOGS dataset."},{"url":"https://www.semanticscholar.org/paper/5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation","venue":"NAACL","year":2021,"referenceCount":77,"citationCount":12,"influentialCitationCount":1,"publicationDate":"12/14/2021","authors":"Linlu Qiu,Peter Shaw,Panupong Pasupat,Pawel Krzysztof Nowak,Tal Linzen,Fei Sha,Kristina Toutanova","citations":[{"paperId":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models"},{"paperId":"1bd799cf462f926041dd2fc8fbe4af54bddbf5c5","title":"Translate First Reorder Later: Leveraging Monotonicity in Semantic Parsing"},{"paperId":"ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","title":"Compositional Generalisation with Structured Reordering and Fertility Layers"},{"paperId":"40047a74b707743157051d38f76061ba5ff9aab4","title":"Compositional Semantic Parsing with Large Language Models"},{"paperId":"01b2f7601ab3df0d2982a204e2fb309f6622646f","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models"},{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis"},{"paperId":"4c430e6c3a72626bd4cb1893960c7c26dfec6c79","title":"Structurally Diverse Sampling Reduces Spurious Correlations in Semantic Parsing Datasets"},{"paperId":"39f604fdd3ade5bd5a67d5284a6d9c12e535db85","title":"Compositionality as Lexical Symmetry"},{"paperId":"90c1a63aada7704eadc4324c16a66ec793d4b698","title":"Compositional generalization with a broad-coverage semantic parser"},{"paperId":"a143cac1bc440135b612132c89e603f364b8a3b7","title":"Combine to Describe: Evaluating Compositional Generalization in Image Captioning"},{"paperId":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"DECOMPOSITION IN NEURAL PROGRAM SYNTHESIS"}],"references":[{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"c764ecba2bace12b9bfb9c2b0651a12ff6888ea7","title":"Learning to Generalize Compositionally by Transferring Across Semantic Parsing Tasks"},{"paperId":"3d5699e7f7e085ad72102859b06fa4884d207e77","title":"Iterative Decoding for Compositional Generalization in Transformers"},{"paperId":"bb0ab8591d6d57c7e2bd1ec35d806b3f25277752","title":"Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks"},{"paperId":"676fa805bd715591f99bb17e36d673a6a14e92fe","title":"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization"},{"paperId":"b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347","title":"Sequence-to-Sequence Learning with Latent Neural Grammars"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"c0e059c46aea358872b4760aed53c4da3beaaeee","title":"Structured Reordering for Modeling Latent Alignments in Sequence Transduction"},{"paperId":"88dbde378e9ac5c25fc7d78f5da147223e8d34d4","title":"Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":"afd8fb26a7094ed3a8838000d50e2b22f815dc28","title":"Learning to Synthesize Data for Semantic Parsing"},{"paperId":"df66fcd3fd4f0d55bd96528cddb0e2f08ddb3385","title":"Revisiting Iterative Back-Translation from the Perspective of Compositional Generalization"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"cc0fc60e9b9d036acf53899c259574c8ce2aedfc","title":"Compositional Generalization via Semantic Tagging"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"e69e5953905b9b9ded4c07f0505ed401ec39babf","title":"Universal Grammar"},{"paperId":null,"title":"2021) which found that a disproportionate amount of the errors on the TMCD test set were in cases where an “atom"},{"paperId":"36d1b02b4cd504ae8641556457a673ba1044b6a9","title":"Compositionality"},{"paperId":"106fb432d2b62f3824a9d6f4a1b30e1f8b6ea9d7","title":"Sequence-level Mixed Sample Data Augmentation"},{"paperId":"9aa3b4492e5981ddf63c4c7c6cf6bc0e650e9ac6","title":"Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations"},{"paperId":"227fe850a72fab24998c7e08d75db214715dc74e","title":"The EOS Decision and Length Extrapolation"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"37882abaec01eba1bf5bda8a36c904aaea0d5642","title":"Improving Compositional Generalization in Semantic Parsing"},{"paperId":"c17198de805458564f43c2e8c2e37252009ca92f","title":"Grounded Adaptation for Zero-shot Executable Semantic Parsing"},{"paperId":"f430773cdc0fdd62192a48a372601d6aa40f3d7b","title":"Task-Oriented Dialogue as Dataflow Synthesis"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"937fef6a786c4463a3bb19770c704945d1600b66","title":"Learning Compositional Rules via Neural Program Synthesis"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"6e7dcd769636465af1d8dddf891e7580ca8bc228","title":"Don’t paraphrase, detect! Rapid and Effective Data Collection for Semantic Parsing"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"fd04e31c25451f9103a0ac2220ac8d7e7884c343","title":"Coarse-to-Fine Decoding for Neural Semantic Parsing"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"593e4e749bd2dbcaf8dc25298d830b41d435e435","title":"A Minimal Span-Based Neural Constituency Parser"},{"paperId":"b7eac64a8410976759445cce235469163d23ee65","title":"Data Recombination for Neural Semantic Parsing"},{"paperId":"7260c0692f8d265e11c4e9c4c8ef4c185bd587ad","title":"Building machines that learn and think like people"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"e4bf059af70c57ad0c68edfc8e5f798c5cebbfc2","title":"Learning Compact Lexicons for CCG Semantic Parsing"},{"paperId":"b2ac51e10a3510eadac5eac5e4fb828f086fab88","title":"Scaling Semantic Parsers with On-the-Fly Ontology Matching"},{"paperId":"96232dd74fad21f495a2f4563fd42869d968e6bf","title":"Lambda Dependency-Based Compositional Semantics"},{"paperId":"50c651e9f94f9d4927a726af0ef44818179d87da","title":"Semantic Parsing as Machine Translation"},{"paperId":"85cb4990378ebf79482d48ee9215147229a22720","title":"Unsupervised Transduction Grammar Induction via Minimum Description Length"},{"paperId":"c7a40c3ef180d847bb3db40fd01990e08a6264f7","title":"Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification"},{"paperId":"02bcc68113cff36226eb9d977f7367f14e2157e5","title":"A Discriminative Latent Variable Model for Statistical Machine Translation"},{"paperId":"c2ecc66c0e5f976b0e0d95c64ed2d1e283a2625d","title":"Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus"},{"paperId":"0db6eb46ca9941660acc775e3ca39bf4434c18be","title":"Hierarchical Phrase-Based Translation"},{"paperId":"774113732db34ce0b797fc3dcceded811fb6edbc","title":"Online Learning of Relaxed CCG Grammars for Parsing to Logical Form"},{"paperId":"5e1cecd26a1f52d887cdfccbbed89c24a7e6b201","title":"Quasi-Synchronous Grammars: Alignment by Soft Projection of Syntactic Dependencies"},{"paperId":"687dce9ac01f5996601655035c34b449c27c3b6a","title":"Learning for Semantic Parsing with Statistical Machine Translation"},{"paperId":"74fe7ec751cd50295b15cfd46389a8fefb37c414","title":"Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars"},{"paperId":"8dd9fd6a45afd266d48255c398429e01ea4fd6db","title":"Learning to Transform Natural to Formal Languages"},{"paperId":"d83c5f7b5de16aaeab6955c87cbfb468361a8ef3","title":"A tutorial introduction to the minimum description length principle"},{"paperId":"a600850ac0120cb09a0b7de7da80bb6a7a76de06","title":"Accurate Unlexicalized Parsing"},{"paperId":"f60d8dd8ca3a7dfa7d0a14988af73084ad93619d","title":"Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing"},{"paperId":"6c9f553e723a40a6713453b734b552c1928bf52b","title":"PCFG Models of Linguistic Tree Representations"},{"paperId":"b7c0e47f8b768258b7d536c21b218e6c46ab8791","title":"Learning to Parse Database Queries Using Inductive Logic Programming"},{"paperId":"215948d5e18400d3841a7a55ea0c031fdceaaa3e","title":"A Minimum Descriptipn Length Approach to Grammar Inference"},{"paperId":"ab7b5917515c460b90451e67852171a531671ab8","title":"The Mathematics of Statistical Machine Translation: Parameter Estimation"},{"paperId":"d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5","title":"Modeling By Shortest Data Description*"},{"paperId":"40dbb25a15b63af3faccb81c8e64a3f5d659e07e","title":"The Theory of Parsing, Translation, and Compiling"},{"paperId":"4561c20053add761b0fa28b0bea9c80c1bb81165","title":"Programming languages and their compilers: Preliminary notes"},{"paperId":"30da8ecce8b3ebc3e9344a79e5c2f8dc4c423bd2","title":"Recognition and Parsing of Context-Free Languages in Time n^3"},{"paperId":"af66165c454a0e94afbab36271fe3deaae0b421a","title":"An Efficient Recognition and Syntax-Analysis Algorithm for Context-Free Languages"},{"paperId":null,"title":"Online and Punta Cana, Dominican Republic"},{"paperId":null,"title":"Dev Source: Schedule a meeting on Thursday at 8 : 30 AM"},{"paperId":null,"title":"Example prediction errors for T5+CSL-Aug. and their closest training example if any for the SMCalFlow-CS dataset"}],"id":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","summary":"This work presents a more powerful data recombination method using a model called Compositional Structure Learner (CSL), a generative model with a quasi-synchronous context-free grammar backbone, which results in a model even stronger than a T5-CSL ensemble on two real world compositional generalization tasks."},{"url":"https://www.semanticscholar.org/paper/69078af65fc934f81fd340e9d1323d6c08194548","title":"Revisiting the Compositional Generalization Abilities of Neural Sequence Models","venue":"ACL","year":2022,"referenceCount":22,"citationCount":4,"influentialCitationCount":1,"publicationDate":"03/14/2022","authors":"Arkil Patel,S. Bhattamishra,P. Blunsom,Navin Goyal","citations":[{"paperId":"04ca71d9089b2e86f9e4a874fd66ee8bd0baba8e","title":"When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"e10ed48cceca216d8ac43113c0562cf340dbdce3","title":"Unveiling Transformers with LEGO: a synthetic reasoning task"},{"paperId":"6f0be1f9bda7530b1fa654cac84d595ca9d53740","title":"Revisit Systematic Generalization via Meaningful Learning"}],"references":[{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","title":"Lexicon Learning for Few Shot Sequence Modeling"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":null,"title":"2021) proposed a data augmentation method based on the theory of meaningful learning. Similar to our work, they also augment the train set by adding more primitives (e.g. ‘jump_0"},{"paperId":"106fb432d2b62f3824a9d6f4a1b30e1f8b6ea9d7","title":"Sequence-level Mixed Sample Data Augmentation"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"0dc5dd7c64ee016bdc33a5f32dc25747be5ca702","title":"From SCAN to Real Data: Systematic Generalization via Meaningful Learning"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"0dc092d33f7c71bc9d8d42b53ebb1fad101db4c8","title":"Linguistic generalization and compositionality in modern artificial neural networks"},{"paperId":"9d9b4cc02fc0ac6fe7eac649599db1a47cf99d89","title":"Human few-shot learning of compositional instructions"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"965c88e1afe9742eb06931a393cacf098c3c805d","title":"How Children Learn the Meanings of Words"},{"paperId":null,"title":"Implicit Word Learning Drawing analogy from human vocabulary acquisition (Bloom, 2000), our primitive generalization setting corresponds to the case when a child"}],"id":"69078af65fc934f81fd340e9d1323d6c08194548","summary":"It is demonstrated that modifying the training distribution in simple and intuitive ways enables standard seq-to-seq models to achieve near-perfect generalization performance, thereby showing that their compositional generalization abilities were previously underestimated."},{"url":"https://www.semanticscholar.org/paper/c764ecba2bace12b9bfb9c2b0651a12ff6888ea7","title":"Learning to Generalize Compositionally by Transferring Across Semantic Parsing Tasks","venue":"ArXiv","year":2021,"referenceCount":51,"citationCount":3,"influentialCitationCount":0,"publicationDate":"11/09/2021","authors":"Wang Zhu,Peter Shaw,Tal Linzen,Fei Sha","citations":[{"paperId":"01b2f7601ab3df0d2982a204e2fb309f6622646f","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models"},{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"}],"references":[{"paperId":"676fa805bd715591f99bb17e36d673a6a14e92fe","title":"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"88dbde378e9ac5c25fc7d78f5da147223e8d34d4","title":"Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":"789b5441743c2e38cf4c38749ed820c0671d81b1","title":"Muppet: Massive Multi-task Representations with Pre-Finetuning"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"cc0fc60e9b9d036acf53899c259574c8ce2aedfc","title":"Compositional Generalization via Semantic Tagging"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"37882abaec01eba1bf5bda8a36c904aaea0d5642","title":"Improving Compositional Generalization in Semantic Parsing"},{"paperId":"e71885cfa161b3fce024cb75887c06727abe8800","title":"Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"9e594ae4ae9c38b6495810a8872f513ae19be29c","title":"Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"d1206ccabd1980848f14472d6548251c2fab7963","title":"Exploring and Predicting Transferability across NLP Tasks"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"937fef6a786c4463a3bb19770c704945d1600b66","title":"Learning Compositional Rules via Neural Program Synthesis"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"815a3d56401483b635cfad9468852cddb46350ee","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":null,"title":"COGS Original input: Emma ate the ring beside a bed"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"93ad19fbc85360043988fa9ea7932b7fdf1fa948","title":"Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"f8de25118af2abc4c48afb947d6ec298e05ef1e5","title":"When Does Label Smoothing Help?"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"When does label smoothing help? In NeurIPS"},{"paperId":"b47381e04739ea3f392ba6c8faaf64105493c196","title":"Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"},{"paperId":"bb438e42ccc46b63766c59b2265f18a42d49d014","title":"Syntactic Scaffolds for Semantic Structures"},{"paperId":"a75869d69cc86f501939c237ae4711aa2885f6a6","title":"Meta-Learning for Low-Resource Neural Machine Translation"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"cd49acefc8d51e324aa562e5337e1c2aff067053","title":"An Overview of Multi-task Learning"},{"paperId":"6d431f835c06afdea45dff6b24486bf301ebdef0","title":"An Overview of Multi-Task Learning in Deep Neural Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"ade0c116120b54b57a91da51235108b75c28375a","title":"A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks"},{"paperId":"53bb7789be36a58f865f2ec84f6d8f816ddaae6a","title":"Learning to Learn"},{"paperId":"03ad06583c9721855ccd82c3d969a01360218d86","title":"Deep multi-task learning with low level tasks supervised at lower layers"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"8dd9fd6a45afd266d48255c398429e01ea4fd6db","title":"Learning to Transform Natural to Formal Languages"},{"paperId":"161ffb54a3fdf0715b198bb57bd22f910242eb49","title":"Multitask Learning"},{"paperId":"a43d7b8e5e1bcb7c3fbf82164cfc9d12737176e8","title":"Task Clustering and Gating for Bayesian Multitask Learning"},{"paperId":"f60d8dd8ca3a7dfa7d0a14988af73084ad93619d","title":"Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing"},{"paperId":null,"title":"Multitask learning. In Sebastian Thrun and Lorien Pratt (eds.), Learning to learn, pp. 95–133"},{"paperId":"7ab682d62af721cd93d2126db46277f9df6bda81","title":"Using inductive logic programming to automate the construction of natural language parsers"}],"id":"c764ecba2bace12b9bfb9c2b0651a12ff6888ea7","summary":"This work investigates learning representations that facilitate transfer learning from one compositional task to another: the representation and the task-specific layers of the models are strategically trained differently on a pre-finetuning task such that they generalize well on mismatched splits that require compositionality."},{"url":"https://www.semanticscholar.org/paper/04ca71d9089b2e86f9e4a874fd66ee8bd0baba8e","title":"When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks","venue":"","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/23/2022","authors":"Ankur Sikarwar,Arkil Patel,Navin Goyal","citations":[],"references":[{"paperId":"69078af65fc934f81fd340e9d1323d6c08194548","title":"Revisiting the Compositional Generalization Abilities of Neural Sequence Models"},{"paperId":"5d0db797a45ce2453f821f7ded0b547d3fdab054","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"3dcfa05a1c162e6cab927c5b08d0444f7b6691f4","title":"Probing Classifiers: Promises, Shortcomings, and Advances"},{"paperId":"d41417f22f898125c4d34f672938ebb2a3764961","title":"Systematic Generalization on gSCAN: What is Nearly Solved and What is Next?"},{"paperId":"af749e5dbde38914ca6fa1463fca17eac8f69ecc","title":"ReaSCAN: Compositional Reasoning in Language Grounding"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"39b492db00faead70bc3f4fb4b0364d94398ffdb","title":"Do Vision Transformers See Like Convolutional Neural Networks?"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"0735fb79bf34698c1df4461a05ed51c232c412e4","title":"Thinking Like Transformers"},{"paperId":"3268a9371aad1bc8ba1458338ab184ef000a9525","title":"Compositional Networks Enable Systematic Generalization for Grounded Language Understanding"},{"paperId":null,"title":"A mathematical framework for transformer circuits"},{"paperId":null,"title":"2021) compare the final position"},{"paperId":"36d1b02b4cd504ae8641556457a673ba1044b6a9","title":"Compositionality"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"ddf1b38b6ba885326e7f44721a135a7b2d7f415a","title":"Think before you act: A simple baseline for compositional generalization"},{"paperId":"3ecc8f61418f6afdbb600d9f6fbb286143e56026","title":"Systematic Generalization on gSCAN with Language Conditioned Embedding"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"9a21740d87976bf76f4a9668a9da631035302fb2","title":"Attention Is Not Only a Weight: Analyzing Transformers with Vector Norms"},{"paperId":"4bc2bb6584774b0d8ad0b4f5215dc2075487c192","title":"A Benchmark for Systematic Generalization in Grounded Language Understanding"},{"paperId":"80deaca65c2c155bd15718eeecff584841eb25b0","title":"CLOSURE: Assessing Systematic Generalization of CLEVR Models"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"335613303ebc5eac98de757ed02a56377d99e03a","title":"What Does BERT Learn about the Structure of Language?"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"9695676deace8c05d4e95274b92f20ed1e97470c","title":"CLEVR-Ref+: Diagnosing Visual Reasoning With Referring Expressions"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"03eb382e04cca8cca743f7799070869954f1402a","title":"CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"},{"paperId":"21c99706bb26e9012bfb4d8d48009a3d45af59b2","title":"Neural Module Networks"}],"id":"04ca71d9089b2e86f9e4a874fd66ee8bd0baba8e","summary":""},{"url":"https://www.semanticscholar.org/paper/bb0ab8591d6d57c7e2bd1ec35d806b3f25277752","title":"Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks","venue":"EMNLP","year":2021,"referenceCount":42,"citationCount":7,"influentialCitationCount":0,"publicationDate":"09/30/2021","authors":"Yichen Jiang,Mohit Bansal","citations":[{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"69df5b68fbf492341336b39b4cc9fcc74fff4d5f","title":"Improving Systematic Generalization Through Modularity and Augmentation"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"d129841cb2e30e25000dcd9edb83c880fc4babc1","title":"Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention"},{"paperId":"a143cac1bc440135b612132c89e603f364b8a3b7","title":"Combine to Describe: Evaluating Compositional Generalization in Image Captioning"},{"paperId":"6f0be1f9bda7530b1fa654cac84d595ca9d53740","title":"Revisit Systematic Generalization via Meaningful Learning"}],"references":[{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":"d3edc20ed4a07195f3663abc0ead4220266fd75b","title":"*-CFQ: Analyzing the Scalability of Machine Learning on a Compositional Task"},{"paperId":"df66fcd3fd4f0d55bd96528cddb0e2f08ddb3385","title":"Revisiting Iterative Back-Translation from the Perspective of Compositional Generalization"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"cc0fc60e9b9d036acf53899c259574c8ce2aedfc","title":"Compositional Generalization via Semantic Tagging"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":"e69e5953905b9b9ded4c07f0505ed401ec39babf","title":"Universal Grammar"},{"paperId":"37882abaec01eba1bf5bda8a36c904aaea0d5642","title":"Improving Compositional Generalization in Semantic Parsing"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"23835438889899885d9f33de2fb2356da10bbc0c","title":"Compositional Generalization by Factorizing Alignment and Translation"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"937fef6a786c4463a3bb19770c704945d1600b66","title":"Learning Compositional Rules via Neural Program Synthesis"},{"paperId":"4bc2bb6584774b0d8ad0b4f5215dc2075487c192","title":"A Benchmark for Systematic Generalization in Grounded Language Understanding"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"2621323502fc779c79bca7ba112bc4d0c1db1d3f","title":"CNNs found to jump around more skillfully than RNNs: Compositional Generalization in Seq2seq Convolutional Networks"},{"paperId":"33ecb49e7b1eb1f44790fb6ceca6eed82cb0c7cd","title":"Jump to better conclusions: SCAN both left and right"},{"paperId":"210feb22ff541920caa4884e73eaff1c09644114","title":"Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks"},{"paperId":"06354570d5f6be803d4a79bf59ecbb097bca8755","title":"On the Practical Computational Power of Finite Precision RNNs for Language Recognition"},{"paperId":"08fbb1b4cfdc83977d2c8f08bdfb663f13c0e60a","title":"Memorize or generalize? Searching for a compositional RNN in a haystack"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"3aa52436575cf6768a0a1a476601825f6a62e58f","title":"Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies"},{"paperId":"9e7ad19160313552175f7dc3e5acf94a430f66ac","title":"The Architecture of Cognition: Rethinking Fodor and Pylyshyn’s Systematicity Challenge"},{"paperId":"7260c0692f8d265e11c4e9c4c8ef4c185bd587ad","title":"Building machines that learn and think like people"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"06c10fcd7cfce4614f8460298820a65b5b8e1818","title":"Strong systematicity in sentence processing by simple recurrent networks"},{"paperId":"3ccaa9d20e1f16f6c818853a970755ce888df792","title":"Generalisation towards Combinatorial Productivity in Language Acquisition by Simple Recurrent Networks"},{"paperId":null,"title":"Recurrent neural networks can learn to implement symbolsensi - tive counting"},{"paperId":"6843890926bf0e5c887ffc78dcb1203135981bf1","title":"The compositionality papers"},{"paperId":"a6383f155fa9d3e9b15092bfefbf613f982eb263","title":"The Algebraic Mind: Integrating Connectionism and Cognitive Science"},{"paperId":"08dc7b19e679539f0f93db0192a8e8d11538b3dd","title":"Rethinking Eliminative Connectionism"},{"paperId":null,"title":"Recurrent neural networks can learn to implement symbolsensitive counting"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":null,"title":"Connec - tionism and cognitive architecture : A critical analy"},{"paperId":null,"title":"Connec - tionism and cognitive architecture : A critical analy"},{"paperId":null,"title":"Syntactic structures"}],"id":"bb0ab8591d6d57c7e2bd1ec35d806b3f25277752","summary":"It is argued that compositionality can be induced in Transformers given minimal but proper guidance, and a better result is achieved using less contextualized vectors as the attention’s query, providing insights into architecture choices in achieving systematic compositionality."},{"url":"https://www.semanticscholar.org/paper/76c9558b3fa10baf0e094386a650015b29a8a4bc","title":"Compositional generalization in semantic parsing with pretrained transformers","venue":"ArXiv","year":2021,"referenceCount":25,"citationCount":3,"influentialCitationCount":0,"publicationDate":"09/30/2021","authors":"A. Orhan","citations":[{"paperId":"470bd6813d303d4ea776def788d215caaa8e772f","title":"History Compression via Language Models in Reinforcement Learning"},{"paperId":"557ebd17b7c7ac4e09bd167d7b8909b8d74d1153","title":"Compositional Generalization Requires Compositional Parsers"},{"paperId":"90c1a63aada7704eadc4324c16a66ec793d4b698","title":"Compositional generalization with a broad-coverage semantic parser"}],"references":[{"paperId":"3d33c71af053b42c14ad8d476c9df9cf6dfc1e16","title":"Does Pretraining for Summarization Require Knowledge Transfer?"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"5b783ba4fa0c8f7b32139cd0f2bffa19dae83d62","title":"ProtTrans: Towards Cracking the Language of Lifes Code Through Self-Supervised Deep Learning and High Performance Computing."},{"paperId":"69bf5d3012c33c18e156aa654f4a6b5ec43b1e0b","title":"Learning to See by Looking at Noise"},{"paperId":"2365410a710b421b2295cdca0074946cb50bb1d4","title":"Are Pre-trained Convolutions Better than Pre-trained Transformers?"},{"paperId":"4e00843bc5f60d2b9116abc4320af6d184422291","title":"Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little"},{"paperId":"3544650f12a05cf4ed3bf2f7e22fc5c02fcabf50","title":"Pretrained Transformers as Universal Computation Engines"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","title":"Zero-Shot Text-to-Image Generation"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"88167f36dced91c279162d68af7225f2b4e2091c","title":"Pre-Training a Language Model Without Human Language"},{"paperId":"f4af3fe736b616452424d50cbd47d52f0a210582","title":"OPUS-MT – Building open translation services for the World"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"50d5e1d2cda879ee07a024d47b04d5f07b689575","title":"What Do Neural Networks Learn When Trained With Random Labels?"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0ebb1d1fbf488fba8c18a5a6057a6ccd9e87510f","title":"Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models"},{"paperId":"97f08c1ae8ca5ddf5948c66bfbbc0546ac154807","title":"Pretrained Transformers Improve Out-of-Distribution Robustness"},{"paperId":"20ba55ee3229db5cb190a00e788c59f08d2a767d","title":"Self-Training With Noisy Student Improves ImageNet Classification"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"c7f79fc1bfc0eaa4b1215fefd20d4c24ed2199c5","title":"Robustness properties of Facebook's ResNeXt WSL models"},{"paperId":"79fc892abaf44a84a758268efd4d1b9e6b64ecf5","title":"Leveraging Random Label Memorization for Unsupervised Pre-Training"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":null,"title":"GLUE : A multitask benchmark and analysis platform for natural language understanding"}],"id":"76c9558b3fa10baf0e094386a650015b29a8a4bc","summary":"It is shown that language models pretrained exclusively with nonEnglish corpora, or even with programming language corporA, significantly improve out-of-distribution generalization in these benchmarks, compared with models trained from scratch, even though both benchmarks are English-based."},{"url":"https://www.semanticscholar.org/paper/39f604fdd3ade5bd5a67d5284a6d9c12e535db85","title":"Compositionality as Lexical Symmetry","venue":"ArXiv","year":2022,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/30/2022","authors":"Ekin Akyürek,Jacob Andreas","citations":[],"references":[{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"},{"paperId":"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","title":"Lexicon Learning for Few Shot Sequence Modeling"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","title":"Zero-Shot Text-to-Image Generation"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"e69e5953905b9b9ded4c07f0505ed401ec39babf","title":"Universal Grammar"},{"paperId":null,"title":"1968 . 671 Syntax - directed transduction"},{"paperId":null,"title":"Learning algebraic re"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"b0ea633e0c22fbd8cbc531c7326376725d16ce25","title":"Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks"},{"paperId":"35bb704396a670da64e774e35f87e4cc482ec6cb","title":"A Group-Theoretic Framework for Data Augmentation"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"3b618878525c8bb5ea64c84cd16b902c29ccfea2","title":"Making a Point: Pointer-Generator Transformers for Disjoint Vocabularies"},{"paperId":null,"title":"Good-enough compositional data"},{"paperId":"3813b88a4ec3c63919df47e9694b577f4691f7e5","title":"A survey on Image Data Augmentation for Deep Learning"},{"paperId":"493fac37cea49afb98c52c2f5dd75c303a325b25","title":"Mitigating Gender Bias in Natural Language Processing: Literature Review"},{"paperId":"162cad5df347bdac469331df540440b320b5aa21","title":"EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks"},{"paperId":"9d9b4cc02fc0ac6fe7eac649599db1a47cf99d89","title":"Human few-shot learning of compositional instructions"},{"paperId":null,"title":"Eda: Easy data augmenta"},{"paperId":"12386a0d1be0877c8707aeef7196b1bc49628397","title":"On transfer learning using a MAC model variant"},{"paperId":"9d15ebe3f5aaf32a9f835f88703241461324c35b","title":"Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding"},{"paperId":"cdcdb52d89fb4c730c8ef360b41d3f19e1d20b9b","title":"Towards one-shot learning for rare-word translation with external experts"},{"paperId":"0ee468b9b709a2610c4b574d67218e7960350224","title":"SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation"},{"paperId":"d3707cf521e3596313af1f53acba6413d0d528a6","title":"Training Tips for the Transformer Model"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"7cfa5c97164129ce3630511f639040d28db1d4b7","title":"FiLM: Visual Reasoning with a General Conditioning Layer"},{"paperId":"f466157848d1a7772fb6d02cdac9a7a5e7ef982e","title":"Neural Discrete Representation Learning"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"668db48c6a79826456341680ee1175dfc4cced71","title":"Get To The Point: Summarization with Pointer-Generator Networks"},{"paperId":"03eb382e04cca8cca743f7799070869954f1402a","title":"CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"},{"paperId":null,"title":"Attention is all"},{"paperId":null,"title":"Clevr: A diagnostic dataset"},{"paperId":"bf4dc8112c00cddd87c5f6110dae5efc305b1a27","title":"Neural Shift-Reduce CCG Semantic Parsing"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"b7eac64a8410976759445cce235469163d23ee65","title":"Data Recombination for Neural Semantic Parsing"},{"paperId":"5c077b3ad4de4f2ea99561908aa9be1520f18a14","title":"Group Equivariant Convolutional Networks"},{"paperId":"f3b96ef2dc1fc5e14982f1b963db8db6a54183bb","title":"Improving Neural Machine Translation Models with Monolingual Data"},{"paperId":"21c99706bb26e9012bfb4d8d48009a3d45af59b2","title":"Neural Module Networks"},{"paperId":null,"title":"Long 626 short - term memory"},{"paperId":null,"title":"Polynomial 608 identification in the limit of substitutable context - free 609 languages"},{"paperId":null,"title":"2016. Data recombination"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":null,"title":"Symmetrybased semantic parsing"},{"paperId":null,"title":"Clevr : A diagnostic dataset 639 for compositional language and elementary visual 640 reasoning"},{"paperId":"cbf81e3c372019daf924270c7201670dd6cd2089","title":"Natural Logic and Natural Language Inference"},{"paperId":null,"title":"Symmetry - 644 based semantic parsing COGS : A compo - 647 sitional generalization challenge based on semantic 648 interpretation"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"7b5e31257f01aba987f16e175a3e49e00a5bd3bb","title":"A Simple, Fast, and Effective Reparameterization of IBM Model 2"},{"paperId":null,"title":"Recursive deep models for"},{"paperId":"36d69fec4884389c1709d3ca74394cac814ce4a4","title":"Lexical Generalization in CCG Grammar Induction for Semantic Parsing"},{"paperId":"358dd0063f5387b6a88f729c12fdb6610aaf2a3b","title":"Polynomial Identification in the Limit of Substitutable Context-free Languages"},{"paperId":null,"title":"602 The hiero machine translation system : Extensions , 603 evaluation , and analysis"},{"paperId":"08c0f93bcc64afddfdfb01532bb8369679f00e7b","title":"The Hiero Machine Translation System: Extensions, Evaluation, and Analysis"},{"paperId":"a4b828609b60b06e61bea7a4029cc9e1cad5df87","title":"Statistical Phrase-Based Translation"},{"paperId":null,"title":"Adam : A 652 method for stochastic optimization"},{"paperId":"4c6e99daaac3e78dc12864102f8072a5ef8278e1","title":"Learning from Imbalanced Data Sets: A Comparison of Various Strategies *"},{"paperId":null,"title":"Learning from imbal"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"a98a26a6f666f43494d1ad184bfda955bf168a36","title":"On Making a Point"},{"paperId":null,"title":"1970a. English as a formal lan"},{"paperId":null,"title":"Universal grammar. Theoria"},{"paperId":"2a9d6137c95cc7c106bf84f18fb13449eded7826","title":"Syntax-Directed Transduction"},{"paperId":null,"title":"Learning algebraic re - 676 combination for compositional generalization"},{"paperId":null,"title":"Improving composi - 719 tional generalization with latent structure and data 720 augmentation"},{"paperId":null,"title":"Group equivariant 612 convolutional networks"}],"id":"39f604fdd3ade5bd5a67d5284a6d9c12e535db85","summary":"This paper proves that for any task factorizable into a lex013 icon and a composition function, there exists a family of data transformation functions that are guaranteed to produce new, well-formed examples when applied to training data and shows that it is possible to identify these transformations even when the compositional function is unknown."},{"url":"https://www.semanticscholar.org/paper/45496cd0b256b75bfbe3bd95890b496069c7821c","title":"Multilingual Compositional Wikidata Questions","venue":"ArXiv","year":2021,"referenceCount":48,"citationCount":5,"influentialCitationCount":3,"publicationDate":null,"authors":"Ruixiang Cui,Rahul Aralikatte,Heather Christine Lent,Daniel Hershcovich","citations":[{"paperId":"787a7e59c7aea516ad52696b6e7cd2f58d44fa77","title":"Knowledge Graph Question Answering Datasets and Their Generalizability: Are They Enough for Future Research?"},{"paperId":"c2dfafae76b22403508941de6acb78d0c2564266","title":"Can Machine Translation be a Reasonable Alternative for Multilingual Question Answering Systems over Knowledge Graphs?"},{"paperId":"f5fc7773f40af5a16f130636ddec903f56266d06","title":"Enhancing Multilingual Accessibility of Question Answering over Knowledge Graphs"},{"paperId":"405277e9ed66ff916652f253f60fb28cf856b7bb","title":"Knowledge Graph Question Answering Leaderboard: A Community Resource to Prevent a Replication Crisis"},{"paperId":"377b3837569ee892a2c6ca1f0a6a0089f74d930b","title":"QALD-9-plus: A Multilingual Dataset for Question Answering over DBpedia and Wikidata Translated by Native Speakers"}],"references":[{"paperId":"850232f59e25fc9a37ba72f0738126503c4040bb","title":"Zero-Shot Cross-lingual Semantic Parsing"},{"paperId":"16529f7194bf7faee8a4e43fd54aefeb8730f236","title":"Database reasoning over text"},{"paperId":"dcf03c80c7eeac31401f6b39dd4b3d6d025679fd","title":"Knowledge-aware Named Entity Recognition with Alleviating Heterogeneity"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":"2f1b6164b8ee2f354a9ed3b7699e9f13985ba5b4","title":"CHOLAN: A Modular Approach for Neural Entity Linking on Wikipedia and Wikidata"},{"paperId":"d3edc20ed4a07195f3663abc0ead4220266fd75b","title":"*-CFQ: Analyzing the Scalability of Machine Learning on a Compositional Task"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"37882abaec01eba1bf5bda8a36c904aaea0d5642","title":"Improving Compositional Generalization in Semantic Parsing"},{"paperId":"f1bdd6afb1519a1930a9d7df29668287b3431ecc","title":"RuBQ: A Russian Dataset for Question Answering over Wikidata"},{"paperId":"c8b3ef5c29f3a960f4c601de5c4ba28c4b9dff46","title":"WikiBank: Using Wikidata to Improve Multilingual Frame-Semantic Parsing"},{"paperId":"9c49cdf0ac4665b320262156eb19bf2e39cb1bb4","title":"End-to-End Slot Alignment and Recognition for Cross-Lingual NLU"},{"paperId":"0e141942fa265142f41a2a26eb17b6005d3af29e","title":"The State and Fate of Linguistic Diversity and Inclusion in the NLP World"},{"paperId":"7907d06a0a8cd2d25480422944c88f66db950d3d","title":"FQuAD: French Question Answering Dataset"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":null,"title":"Compositional generalization in semantic parsing: Pretraining vs"},{"paperId":null,"title":"FQuAD: French question answering dataset. In Findings of the Association for Computational Linguistics: EMNLP"},{"paperId":"788d28e234fc69fb07b4a4da7fb1bcf05e5160b5","title":"Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base"},{"paperId":"2673c5eb6cda0f2d6e39a321fad91d90cb818332","title":"Rewarding Coreference Resolvers for Being Consistent with World Knowledge"},{"paperId":"2dd939fe52a336451c83b4e048660f2a5a048265","title":"FreebaseQA: A New Factoid QA Data Set Matching Trivia-Style Question-Answer Pairs with Freebase"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"16c844fd4d97f3c6eb38b0d6527c87d184efedc3","title":"The Evolved Transformer"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Antonios Anastasopoulos, Patrick Littell, and Graham Neubig"},{"paperId":null,"title":"Rewarding coreference resolvers"},{"paperId":"9283becd6596f9b4d0e0d753b7606edab5acfa8b","title":"Weakly-Supervised Neural Semantic Parsing with a Generative Ranker"},{"paperId":"c997d481606f0346164511cabe74c6d1ef3f6be5","title":"DRCD: a Chinese Machine Reading Comprehension Dataset"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"c8725f13be7434b69738491c66b45c9225258253","title":"The Web as a Knowledge-Base for Answering Complex Questions"},{"paperId":"e5a1d41e6212951cb6a831ed61a59d00b7ff6867","title":"Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"4f83e1b64f57ae0d546076279426e85c0e60298b","title":"9th Challenge on Question Answering over Linked Data (QALD-9) (invited paper)"},{"paperId":null,"title":"A call for clarity in report"},{"paperId":"d7d64d6d91c46c16e69141d0a79f165bde90e1ce","title":"Question Answering Benchmarks for Wikidata"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"f010affab57b5fcf1cd6be23df79d8ec98c7289c","title":"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"},{"paperId":"bc7fcefa3e333d50463d524406d107060c4a0cec","title":"Neural Semantic Parsing over Multiple Knowledge-bases"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"f896c4dffce831e41ae56070898b44d7a1e7813e","title":"From Freebase to Wikidata: The Great Migration"},{"paperId":"6e565308c8081e807709cb4a917443b737e6cdb4","title":"Large-scale Simple Question Answering with Memory Networks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"d2946a868682e4141beabc288d79253ae254c6e1","title":"DBpedia - A large-scale, multilingual knowledge base extracted from Wikipedia"},{"paperId":"b29447ba499507a259ae9d8f685d60cc1597d7d3","title":"Semantic Parsing on Freebase from Question-Answer Pairs"},{"paperId":"1976c9eeccc7115d18a04f1e7fb5145db6b96002","title":"Freebase: a collaboratively created graph database for structuring human knowledge"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":null,"title":"Bleu: a method for"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":null,"title":"Pavel Efimov, and Pavel Braslavski. 2021. Ru{bq} 2.0: An innovated russian question answering dataset"}],"id":"45496cd0b256b75bfbe3bd95890b496069c7821c","summary":"This work proposes a method for creating a multilingual, parallel dataset of question-query pairs, grounded in Wikidata, and introduces such a dataset called CompositionalWikidata Questions (CWQ), and utilizes this data to train and evaluate semantic parsers for Hebrew, Kannada, Chinese and English, to better understand the current strengths and weaknesses of multilingual semantic parsing."},{"url":"https://www.semanticscholar.org/paper/ad331dce175b1d38d6516455013c1ec0e26e606b","title":"Compositional Generalization in Multilingual Semantic Parsing over Wikidata","venue":"Transactions of the Association for Computational Linguistics","year":2021,"referenceCount":82,"citationCount":1,"influentialCitationCount":0,"publicationDate":"08/07/2021","authors":"Ruixiang Cui,Rahul Aralikatte,Heather Christine Lent,Daniel Hershcovich","citations":[{"paperId":"38e1a9c5599fc7597b7c5ffd37951ba5f528094c","title":"XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing"}],"references":[{"paperId":"8d0f755dea90f35f4b126a01fa3cce96b3bdd344","title":"Towards Climate Awareness in NLP Research"},{"paperId":"598231eb906b183f7a2a408ef4536127e11e3de9","title":"Challenges and Strategies in Cross-Cultural NLP"},{"paperId":"f62467f1ef82848338e014f2dd9b690e686ad1e3","title":"Enhancing the Accessibility of Knowledge Graph Question Answering Systems through Multilingualization"},{"paperId":"377b3837569ee892a2c6ca1f0a6a0089f74d930b","title":"QALD-9-plus: A Multilingual Dataset for Question Answering over DBpedia and Wikidata Translated by Native Speakers"},{"paperId":"8008348e87d3904842a2dd230c14b83112e8bf48","title":"Compositional Generalization in Dependency Parsing"},{"paperId":"850232f59e25fc9a37ba72f0738126503c4040bb","title":"Zero-Shot Cross-lingual Semantic Parsing"},{"paperId":"2aa1d4350e80613feed88d5a6337e79693f7aa57","title":"KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base"},{"paperId":"7cc74ffa1215321712d4a830bb9dee19d9f0fb47","title":"Grounded Graph Decoding Improves Compositional Generalization in Question Answering"},{"paperId":"eea16dfc29f0521dd547e67a84af4ff95a9c5529","title":"Visually Grounded Reasoning across Languages and Cultures"},{"paperId":"3e53ca0f1d08e5a0f3f014af3eb59dabe95b07e5","title":"Translate & Fill: Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data"},{"paperId":"16529f7194bf7faee8a4e43fd54aefeb8730f236","title":"Database reasoning over text"},{"paperId":"5a02e44c9057bab706f62c970806cbe611808746","title":"Universal Discourse Representation Structure Parsing"},{"paperId":"dcf03c80c7eeac31401f6b39dd4b3d6d025679fd","title":"Knowledge-aware Named Entity Recognition with Alleviating Heterogeneity"},{"paperId":"a07a94168608322600fd3cab54df1410b96852b6","title":"Case-based Reasoning for Natural Language Queries over Knowledge Bases"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":"2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"},{"paperId":"2f1b6164b8ee2f354a9ed3b7699e9f13985ba5b4","title":"CHOLAN: A Modular Approach for Neural Entity Linking on Wikipedia and Wikidata"},{"paperId":"d3edc20ed4a07195f3663abc0ead4220266fd75b","title":"*-CFQ: Analyzing the Scalability of Machine Learning on a Compositional Task"},{"paperId":"df66fcd3fd4f0d55bd96528cddb0e2f08ddb3385","title":"Revisiting Iterative Back-Translation from the Perspective of Compositional Generalization"},{"paperId":"b9c3e87bc09c4c6167a03a835c30b1c23bef7a40","title":"Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"b8ae6ae027039c675c07f2d660ae25e8ae0261b9","title":"Knowledge graphs"},{"paperId":"4f0340431007044777076a8c472b42c435eff03f","title":"RuBQ 2.0: An Innovated Russian Question Answering Dataset"},{"paperId":"65fc5e6657fa2e9fd1b15df6b61acbe3ab3993df","title":"Frustratingly Simple but Surprisingly Strong: Using Language-Independent Features for Zero-shot Cross-lingual Semantic Parsing"},{"paperId":"5e554da8889e608f3ec146cea55d150c376d5088","title":"Multi-level Alignment Pretraining for Multi-lingual Semantic Parsing"},{"paperId":"dff49c89b2d15704b7122c309e76bf7c545200b2","title":"MRP 2020: The Second Shared Task on Cross-Framework and Cross-Lingual Meaning Representation Parsing"},{"paperId":"c3a662b864673d8cc7469051419ab8819926d4b0","title":"Identifying Elements Essential for BERT’s Multilinguality"},{"paperId":"986cc3d0e3afb23f84564ea9588b7b8e9c3e1dd6","title":"Hierarchical Poset Decoding for Compositional Generalization in Language"},{"paperId":"37882abaec01eba1bf5bda8a36c904aaea0d5642","title":"Improving Compositional Generalization in Semantic Parsing"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"f1bdd6afb1519a1930a9d7df29668287b3431ecc","title":"RuBQ: A Russian Dataset for Question Answering over Wikidata"},{"paperId":"14489ec7893e373a0dcc9555c52b99b2b3a429f6","title":"Are All Languages Created Equal in Multilingual BERT?"},{"paperId":"c8b3ef5c29f3a960f4c601de5c4ba28c4b9dff46","title":"WikiBank: Using Wikidata to Improve Multilingual Frame-Semantic Parsing"},{"paperId":"26299d5fdc5137291dc6a091573b3d18aba1d1c2","title":"MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"},{"paperId":"085b360d3c08aaf997f45a78e27f2629f5625205","title":"Translation Artifacts in Cross-lingual Transfer Learning"},{"paperId":"297ad41c0e7264e67ae078921e2a57436293ce72","title":"XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation"},{"paperId":"0e141942fa265142f41a2a26eb17b6005d3af29e","title":"The State and Fate of Linguistic Diversity and Inclusion in the NLP World"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"4bc2bb6584774b0d8ad0b4f5215dc2075487c192","title":"A Benchmark for Systematic Generalization in Grounded Language Understanding"},{"paperId":"7907d06a0a8cd2d25480422944c88f66db950d3d","title":"FQuAD: French Question Answering Dataset"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"f0cc5fab483f036d0cc799ba819280899b04eff2","title":"A Robust Self-Learning Framework for Cross-Lingual Text Classification"},{"paperId":"788d28e234fc69fb07b4a4da7fb1bcf05e5160b5","title":"Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base"},{"paperId":"c58d6e040df970914f1ec5f356b826b681f03c98","title":"Ranking Knowledge Graphs By Capturing Knowledge about Languages and Labels"},{"paperId":"32c9a0acee8d236c553395052c29a6d853d8ea2d","title":"Compositional Generalization in Image Captioning"},{"paperId":"2673c5eb6cda0f2d6e39a321fad91d90cb818332","title":"Rewarding Coreference Resolvers for Being Consistent with World Knowledge"},{"paperId":"2dd939fe52a336451c83b4e048660f2a5a048265","title":"FreebaseQA: A New Factoid QA Data Set Matching Trivia-Style Question-Answer Pairs with Freebase"},{"paperId":"248824ec5d9b4ddf0c36cdc51b6b57af6e881328","title":"Choosing Transfer Languages for Cross-Lingual Learning"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"16c844fd4d97f3c6eb38b0d6527c87d184efedc3","title":"The Evolved Transformer"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Antonios Anastasopoulos, Patrick Littell, and Graham Neubig"},{"paperId":"9283becd6596f9b4d0e0d753b7606edab5acfa8b","title":"Weakly-Supervised Neural Semantic Parsing with a Generative Ranker"},{"paperId":"c997d481606f0346164511cabe74c6d1ef3f6be5","title":"DRCD: a Chinese Machine Reading Comprehension Dataset"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"c8725f13be7434b69738491c66b45c9225258253","title":"The Web as a Knowledge-Base for Answering Complex Questions"},{"paperId":"642c1b4a9da95ea4239708afc5929a5007a1870d","title":"Tensor2Tensor for Neural Machine Translation"},{"paperId":"e5a1d41e6212951cb6a831ed61a59d00b7ff6867","title":"Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"4f83e1b64f57ae0d546076279426e85c0e60298b","title":"9th Challenge on Question Answering over Linked Data (QALD-9) (invited paper)"},{"paperId":"d7d64d6d91c46c16e69141d0a79f165bde90e1ce","title":"Question Answering Benchmarks for Wikidata"},{"paperId":"e8bef413503471559d66146b651afc9c513b5b37","title":"Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources"},{"paperId":"eb8d32388012889ed438c3b0191f4ab614c5b80a","title":"Cross-Lingual Sentiment Analysis Without (Good) Translation"},{"paperId":"f010affab57b5fcf1cd6be23df79d8ec98c7289c","title":"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"},{"paperId":"bc7fcefa3e333d50463d524406d107060c4a0cec","title":"Neural Semantic Parsing over Multiple Knowledge-bases"},{"paperId":"bcab879f6b2d8b647f50b9878912d83d0fc84a7c","title":"Cross-Lingual Syntactic Transfer with Limited Resources"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"f896c4dffce831e41ae56070898b44d7a1e7813e","title":"From Freebase to Wikidata: The Great Migration"},{"paperId":"6e565308c8081e807709cb4a917443b737e6cdb4","title":"Large-scale Simple Question Answering with Memory Networks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"d2946a868682e4141beabc288d79253ae254c6e1","title":"DBpedia - A large-scale, multilingual knowledge base extracted from Wikipedia"},{"paperId":"b29447ba499507a259ae9d8f685d60cc1597d7d3","title":"Semantic Parsing on Freebase from Question-Answer Pairs"},{"paperId":"e086e4e1e4079354adb1f9bd156c4bf36fa23a59","title":"The myth of language universals: language diversity and its importance for cognitive science."},{"paperId":"1976c9eeccc7115d18a04f1e7fb5145db6b96002","title":"Freebase: a collaboratively created graph database for structuring human knowledge"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":null,"title":", Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander Rush . 2020 . Transformers : State - ofthe - art natural language processing"}],"id":"ad331dce175b1d38d6516455013c1ec0e26e606b","summary":"A method is proposed for creating a multilingual, parallel dataset of question-query pairs, grounded in Wikidata, and it is used to analyze the compositional generalization of semantic parsers in Hebrew, Kannada, Chinese, and English."},{"url":"https://www.semanticscholar.org/paper/e528466e2aff981511d4ca6e063211297c0b4175","title":"The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization","venue":"ICLR","year":2021,"referenceCount":60,"citationCount":7,"influentialCitationCount":2,"publicationDate":"10/14/2021","authors":"R. Csordás,Kazuki Irie,J. Schmidhuber","citations":[{"paperId":"b1f33e956e36bf25e118c0d537dcc519cfe52e60","title":"CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations"},{"paperId":"bee6ee50e75829bd1aa5561c659e98d857bf6b48","title":"A Generalist Neural Algorithmic Learner"},{"paperId":"6d72226ccdf1a883c4b866f14d3d1f06db871ee1","title":"SALSA: Attacking Lattice Cryptography with Transformers"},{"paperId":"00f1889ed3e0bb782db63a09779f8c19eda7f59f","title":"Block-Recurrent Transformers"},{"paperId":"4fd61f6b860acc9c5da8766b7c9064f0ec896301","title":"A Modern Self-Referential Weight Matrix That Learns to Modify Itself"},{"paperId":"7a8e40254435966e0568df52929d9779add28c7d","title":"Linear algebra with transformers"},{"paperId":"86589b6286ef3c55b8b4fccfb41a3b30b7afdf61","title":"Going Beyond Linear Transformers with Recurrent Fast Weight Programmers"}],"references":[{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"2fd1312b8507aae41bace1dd89712754a81fbc49","title":"PonderNet: Learning to Ponder"},{"paperId":"1bed382373aed687c045bb65bc7541b16fc7a6be","title":"Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN"},{"paperId":"0735fb79bf34698c1df4461a05ed51c232c412e4","title":"Thinking Like Transformers"},{"paperId":"86589b6286ef3c55b8b4fccfb41a3b30b7afdf61","title":"Going Beyond Linear Transformers with Recurrent Fast Weight Programmers"},{"paperId":"db016d2b6d2577c47d62f9de2a7d1ddbf226386a","title":"Modeling Hierarchical Structures with Continuous Recursive Neural Networks"},{"paperId":"8b133a09fd57e9703b45c4e64543180333f98931","title":"Reinforcement Learning of Implicit and Explicit Control Flow in Instructions"},{"paperId":"6fa1cfc4f97f03a8485692418c7aa1a06c574a85","title":"Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention"},{"paperId":"7e9ff94476f41041c75e253e84f487db00e9c861","title":"Long Range Arena: A Benchmark for Efficient Transformers"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"d642868ce4325ebf3026c0aa0c497a079f112a8d","title":"On the Binding Problem in Artificial Neural Networks"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"bdbf780dfd6b3eb0c9e980887feae5f23af15bc4","title":"GLU Variants Improve Transformer"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"d715b4a9282562b9d84fb66e04ee70e66b12e86d","title":"Location Attention for Extrapolation to Longer Sequences"},{"paperId":"59a916cdc943f0282908e6f3fa0360f4c5fb78d0","title":"Stabilizing Transformers for Reinforcement Learning"},{"paperId":"815a3d56401483b635cfad9468852cddb46350ee","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":"9222fd8b08c64c9bf677b4ce51d22444bd509fd3","title":"Advancing neural language modeling in automatic speech recognition"},{"paperId":"80deaca65c2c155bd15718eeecff584841eb25b0","title":"CLOSURE: Assessing Systematic Generalization of CLEVR Models"},{"paperId":"287e85aca777d6d3d73e1484ba9c0f09d40f578a","title":"Ordered Memory"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"b39efed2e73357db4691f66935cf62e7b51f30e1","title":"Transcoding Compositionally: Using Attention to Find More Generalizable Solutions"},{"paperId":"2621323502fc779c79bca7ba112bc4d0c1db1d3f","title":"CNNs found to jump around more skillfully than RNNs: Compositional Generalization in Seq2seq Convolutional Networks"},{"paperId":"3928b2177086532775fbf607ae3e05a0375a5061","title":"Language Modeling with Deep Transformers"},{"paperId":"d5535d4da15a7a8dfbeb34f61cddb4874bbc56e0","title":"Improving Differentiable Neural Computers Through Memory Masking, De-allocation, and Link Distribution Sharpness Control"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"b0e2fe0fe9f4fc4ce05d5f637baff96a7e966c01","title":"Cooperative Learning of Disjoint Syntax and Semantics"},{"paperId":"c4744a7c2bb298e4a52289a1e085c71cc3d37bc6","title":"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"2019) which we note here is that the computation of the final output o of column i effectively “halts"},{"paperId":null,"title":"We use the relative positional embedding variant of self-attention by Dai et al. (2019). Our attention matrix with the gated absolute/relative positional encodings"},{"paperId":null,"title":"Two variants of ACT are shown: “U"},{"paperId":"b1c169cb857d992449ae1bc00ff17f8cbb3ef285","title":"Learning compositionally through attentive guidance"},{"paperId":"8b354d76813bd5375e7e5c8d17f630bec5936a01","title":"ListOps: A Diagnostic Dataset for Latent Tree Learning"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","title":"Self-Attention with Relative Position Representations"},{"paperId":"08fbb1b4cfdc83977d2c8f08bdfb663f13c0e60a","title":"Memorize or generalize? Searching for a compositional RNN in a haystack"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"88caa4a0253a8b0076176745ebc072864eab66e1","title":"Language Modeling with Gated Convolutional Networks"},{"paperId":null,"title":"dhead are learned vectors. pi ∈ R is the standard sinusoidal embedding for position i (Vaswani et al., 2017). Softmax is applied to the second dimension"},{"paperId":"04cca8e341a5da42b29b0bc831cb25a0f784fa01","title":"Adaptive Computation Time for Recurrent Neural Networks"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"b92aa7024b87f50737b372e5df31ef091ab54e62","title":"Training Very Deep Networks"},{"paperId":"34f25a8704614163c4095b3ee2fc969b60de4698","title":"Dropout: a simple way to prevent neural networks from overfitting"},{"paperId":null,"title":"Self-delimiting neural networks. Technical Report IDSIA-08-12, arXiv:1210.0118v1"},{"paperId":"e23c34414e66118ecd9b08cf0cd4d016f59b0b85","title":"Bidirectional recurrent neural networks"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"50c770b425a5bb25c77387f687a9910a9d130722","title":"Learning Complex, Extended Sequences Using the Principle of History Compression"},{"paperId":"7826ff60d2dfb24d2af18c5bc565c357ef9db4c1","title":"A stochastic version of the delta rule"},{"paperId":"2b83ce7567bad29afe4753eea7e72521dffdb075","title":"Connectionism and the problem of systematicity: Why Smolensky's solution doesn't work"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"}],"id":"e528466e2aff981511d4ca6e063211297c0b4175","summary":"The novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on a simple arithmetic task and a new variant of ListOps testing for generalization across computational depths."},{"url":"https://www.semanticscholar.org/paper/dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models","venue":"","year":2022,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/24/2022","authors":"Yuekun Yao,Alexander Koller","citations":[],"references":[{"paperId":"9fe39e0f2cf3b6d5f70a379654f9c08ffa48ddee","title":"Unobserved Local Structures Make Compositional Generalization Hard"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"ab72bccf6f3981537389510ecc609109e79595c3","title":"Disentangled Sequence to Sequence Learning for Compositional Generalization"},{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"90c1a63aada7704eadc4324c16a66ec793d4b698","title":"Compositional generalization with a broad-coverage semantic parser"},{"paperId":null,"title":"2021) find that using an in-distribution development set can lead to inefficient model selection and they select their best model based on the accuracy on the generalization set"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"},{"paperId":"523745e29f6cb1890f18352d449fd3597910c485","title":"Improving Compositional Generalization in Classification Tasks via Structure Annotations"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","title":"Lexicon Learning for Few Shot Sequence Modeling"},{"paperId":"25e7c9dcc294d77d184c4c1122c8304cdb58c69d","title":"One SPRING to Rule Them Both: Symmetric AMR Semantic Parsing and Generation without a Complex Pipeline"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"b0ea633e0c22fbd8cbc531c7326376725d16ce25","title":"Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"815a3d56401483b635cfad9468852cddb46350ee","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":"b3564be8b79f25585acb035f3deaf4ae93c26d8f","title":"Theoretical Limitations of Self-Attention in Neural Sequence Models"},{"paperId":null,"title":"BART: Denoising sequence-to-sequence pretraining for natural language"},{"paperId":"50f623677c84ad8f96edac6ffea66ef6e86db298","title":"Learning the Dyck Language with Attention-based Seq2Seq Models"},{"paperId":"455a8838cde44f288d456d01c76ede95b56dc675","title":"A Structural Probe for Finding Syntax in Word Representations"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"e2587eddd57bc4ba286d91b27c185083f16f40ee","title":"What do you learn from context? Probing for sentence structure in contextualized word representations"},{"paperId":"0f4a3b7f835737c7e004073f4a873e356c0063b4","title":"Incorporating Source Syntax into Transformer-Based Neural Machine Translation"},{"paperId":"ac11062f1f368d97f4c826c317bf50dcc13fdb59","title":"Dissecting Contextual Word Embeddings: Architecture and Representation"},{"paperId":"928f9dccb806a3278d20d82cc53781c5f44e2bb1","title":"Constituency Parsing with a Self-Attentive Encoder"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"593e4e749bd2dbcaf8dc25298d830b41d435e435","title":"A Minimal Span-Based Neural Constituency Parser"},{"paperId":"ebb222fff7b71b82d1a5971e198982858abcd03d","title":"Modeling Source Syntax for Neural Machine Translation"},{"paperId":"3aa52436575cf6768a0a1a476601825f6a62e58f","title":"Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies"},{"paperId":"bf0f141bae83bd6d5ca0c37839d53f0d06059b34","title":"Controlling Politeness in Neural Machine Translation via Side Constraints"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"6843890926bf0e5c887ffc78dcb1203135981bf1","title":"The compositionality papers"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":null,"title":"Syntactic Structures"}],"id":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","summary":""},{"url":"https://www.semanticscholar.org/paper/00050c15896e8ae6bb534f10d072351547993f72","title":"LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing","venue":"ArXiv","year":2021,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/14/2021","authors":"Dora Jambor,Dzmitry Bahdanau","citations":[],"references":[{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"c0e059c46aea358872b4760aed53c4da3beaaeee","title":"Structured Reordering for Modeling Latent Alignments in Sequence Transduction"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"cc0fc60e9b9d036acf53899c259574c8ce2aedfc","title":"Compositional Generalization via Semantic Tagging"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":null,"title":"We sampled 1000 examples from the generalization"},{"paperId":null,"title":"Lexicon Learning for Few-Shot Neural Sequence Modeling"},{"paperId":"986cc3d0e3afb23f84564ea9588b7b8e9c3e1dd6","title":"Hierarchical Poset Decoding for Compositional Generalization in Language"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"e6e8a2c56243847b77b604259cde9e10a2daccb8","title":"Semantic Evaluation for Text-to-SQL with Distilled Test Suite"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"80deaca65c2c155bd15718eeecff584841eb25b0","title":"CLOSURE: Assessing Systematic Generalization of CLEVR Models"},{"paperId":"775113b55052994ddadea3cf9e316309a32c99e5","title":"Span-based Hierarchical Semantic Parsing for Task-Oriented Dialog"},{"paperId":"4d031258a66076187001b4d6182345198624d872","title":"The compositionality of neural networks: integrating symbolism and connectionism"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"63ef50238ba765edf47c86e3e3fe9f608d8ea00b","title":"AMR Parsing as Graph Prediction with Latent Alignment"},{"paperId":"fd04e31c25451f9103a0ac2220ac8d7e7884c343","title":"Coarse-to-Fine Decoding for Neural Semantic Parsing"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2","title":"Deep Biaffine Attention for Neural Dependency Parsing"},{"paperId":null,"title":"OpenNMT: Opensource toolkit for neural machine translation"},{"paperId":"f4fa16b9e15580db747110655a1e1df67ce888db","title":"Transforming Dependency Structures to Logical Forms for Semantic Parsing"},{"paperId":"eec3a236ecd185712ce65fb336141f8656eea13d","title":"Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations"},{"paperId":"d6f2f611da110b5b5061731be3fc4c7f45d8ee23","title":"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"b6a0f30260302a2001da9999096cfdd89bc1f7fb","title":"The Hungarian Method for the Assignment Problem"},{"paperId":"3bb5a439a0d610a7eac68f73068cdd278b8c9775","title":"Pattern Recognition and Machine Learning"},{"paperId":"5b60e4b182c1679eb788455586a3f8a4df300e3d","title":"Discriminative learning and spanning tree algorithms for dependency parsing"},{"paperId":"74fe7ec751cd50295b15cfd46389a8fefb37c414","title":"Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"8ff703240303808d2d54b0d7723550820d17a7ca","title":"Events in the Semantics of English: A Study in Subatomic Semantics"},{"paperId":null,"title":"Sesh Sadasivam"}],"id":"00050c15896e8ae6bb534f10d072351547993f72","summary":"This work shows that better systematic generalization can be achieved by producing the meaning representation directly as a graph and not as a sequence, and proposes LAGr (Label Aligned Graphs), a general framework to produce semantic parses by independently predicting node and edge labels for a complete multi-layer input-aligned graph."},{"url":"https://www.semanticscholar.org/paper/a122909a31acf41cb2d9eb602c01b24b9b85a061","title":"LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing","venue":"ACL","year":2022,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":"05/19/2022","authors":"Dora Jambor,Dzmitry Bahdanau","citations":[{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"}],"references":[{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"e51857aa0dae450c3bab2261c1aa0d9ca707d72e","title":"Events in Semantics"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"c0e059c46aea358872b4760aed53c4da3beaaeee","title":"Structured Reordering for Modeling Latent Alignments in Sequence Transduction"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"cc0fc60e9b9d036acf53899c259574c8ce2aedfc","title":"Compositional Generalization via Semantic Tagging"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":null,"title":"Lexicon Learning for Few-Shot Neural Sequence Modeling"},{"paperId":"986cc3d0e3afb23f84564ea9588b7b8e9c3e1dd6","title":"Hierarchical Poset Decoding for Compositional Generalization in Language"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"80deaca65c2c155bd15718eeecff584841eb25b0","title":"CLOSURE: Assessing Systematic Generalization of CLEVR Models"},{"paperId":"775113b55052994ddadea3cf9e316309a32c99e5","title":"Span-based Hierarchical Semantic Parsing for Task-Oriented Dialog"},{"paperId":"4d031258a66076187001b4d6182345198624d872","title":"The compositionality of neural networks: integrating symbolism and connectionism"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"63ef50238ba765edf47c86e3e3fe9f608d8ea00b","title":"AMR Parsing as Graph Prediction with Latent Alignment"},{"paperId":"fd04e31c25451f9103a0ac2220ac8d7e7884c343","title":"Coarse-to-Fine Decoding for Neural Semantic Parsing"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2","title":"Deep Biaffine Attention for Neural Dependency Parsing"},{"paperId":null,"title":"OpenNMT : Open - 696 source toolkit for neural machine translation"},{"paperId":"f4fa16b9e15580db747110655a1e1df67ce888db","title":"Transforming Dependency Structures to Logical Forms for Semantic Parsing"},{"paperId":"eec3a236ecd185712ce65fb336141f8656eea13d","title":"Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations"},{"paperId":"d6f2f611da110b5b5061731be3fc4c7f45d8ee23","title":"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"b6a0f30260302a2001da9999096cfdd89bc1f7fb","title":"The Hungarian method for the assignment problem"},{"paperId":"3bb5a439a0d610a7eac68f73068cdd278b8c9775","title":"Pattern Recognition and Machine Learning"},{"paperId":"5b60e4b182c1679eb788455586a3f8a4df300e3d","title":"Discriminative learning and spanning tree algorithms for dependency parsing"},{"paperId":"74fe7ec751cd50295b15cfd46389a8fefb37c414","title":"Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"8ff703240303808d2d54b0d7723550820d17a7ca","title":"Events in the Semantics of English: A Study in Subatomic Semantics"},{"paperId":null,"title":"1955b. The hungarian method for"}],"id":"a122909a31acf41cb2d9eb602c01b24b9b85a061","summary":"This work shows that better systematic generalization can be achieved by producing the meaning representation directly as a graph and not as a sequence, and proposes LAGr (Label Aligned Graphs), a general framework to produce semantic parses by independently predicting node and edge labels for a complete multi-layer input-aligned graph."},{"url":"https://www.semanticscholar.org/paper/559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review","venue":"ArXiv","year":2022,"referenceCount":690,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/06/2022","authors":"D. Hupkes,Mario Giulianelli,Verna Dankers,Mikel Artetxe,Yanai Elazar,Tiago Pimentel,Christos Christodoulopoulos,Karim Lasri,Naomi Saphra,Arabella J. Sinclair,Dennis Ulmer,Florian Schottmann,Khuyagbaatar Batsuren,Kaiser Sun,Koustuv Sinha,Leila Khalatbari,Maria Ryskina,Rita Frieske,Ryan Cotterell,Zhijing Jin","citations":[{"paperId":"b1a9104afd48f019aeb98c6be7a15736089959d2","title":"Assessing Out-of-Domain Language Model Performance from Few Examples"}],"references":[{"paperId":"f92d5dcc04b2f7713befce4e84e3084bfd4abd50","title":"Measuring Causal Effects of Data Statistics on Language Model's 'Factual' Predictions"},{"paperId":"0e638ce20f3e9b4dd1c10c32a29495c798425e63","title":"No Language Left Behind: Scaling Human-Centered Machine Translation"},{"paperId":"34503c0b6a615124eaf82cb0e4a1dab2866e8980","title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"6e697cc10bec83748a835e116bf5cb782446dd81","title":"ORCA: Interpreting Prompted Language Models via Locating Supporting Data Evidence in the Ocean of Pretraining Data"},{"paperId":"a122909a31acf41cb2d9eb602c01b24b9b85a061","title":"LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing"},{"paperId":"5cac32f85cb25299122c39b8b73eb01fdee710b6","title":"Are Prompt-based Models Clueless?"},{"paperId":"759cb417b66067c53cb7d59e0b0dc749b200f327","title":"Domain Adaptation in Multilingual and Multi-Domain Monolingual Settings for Complex Word Identification"},{"paperId":"a9b515d1ff3808574c79bea6cb65d4c923a358cb","title":"Naturalistic Causal Probing for Morpho-Syntax"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"03d73f3073ac0d73d60d5567fc1ed558367c8279","title":"Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks"},{"paperId":"dcca6811d71d043a85491c084eaf93ee4b5f73b3","title":"Improving the Generalizability of Depression Detection by Leveraging Clinical Questionnaires"},{"paperId":"dac6273cda7442bae3786bc8a72f7961859e63e0","title":"MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages"},{"paperId":"0bf8c1cab22c15d478f6ce431710065665b34e71","title":"Benchmarking Generalization via In-Context Instructions on 1, 600+ Language Tasks"},{"paperId":"0b6875dcfab398d56b9fc5c60f45665a1389cdd0","title":"CLUES: A Benchmark for Learning Classifiers using Natural Language Explanations"},{"paperId":"2c5163f07694f73e7ab842a3029d1e158156e6f6","title":"Does BERT really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task"},{"paperId":"2ecd43c95bc6f61229f6bb369c94cd6e02a2079d","title":"Towards Generalizeable Semantic Product Search by Text Similarity Pre-training on Search Click Logs"},{"paperId":"eaac903219301a31d12b52a981803f21fcf9dde2","title":"Using Linguistic Typology to Enrich Multilingual Lexicons: the Case of Lexical Gaps in Kinship"},{"paperId":"e7102d3fecfe2de5af672713347d805417c6d728","title":"“That Is a Suspicious Reaction!”: Interpreting Logits Variation to Detect NLP Adversarial Attacks"},{"paperId":"e058ac0c64e19d6f3952c275d23cf8a6dc710b6b","title":"Few-Shot Cross-lingual Transfer for Coarse-grained De-identification of Code-Mixed Clinical Texts"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"9cae815605a7c74c2e986933d8f913620011250f","title":"Efficient Argument Structure Extraction with Transfer Learning and Active Learning"},{"paperId":"011a4019aa0d0ce3edfa56bb2ca1e7586eb43fb2","title":"Training Compute-Optimal Large Language Models"},{"paperId":"a4c4093802e2084553781d700566c60a2dcc4e7a","title":"What is wrong with you?: Leveraging User Sentiment for Automatic Dialog Evaluation"},{"paperId":"98133933926a725fb18860383d52c0c60282637f","title":"Can Unsupervised Knowledge Transfer from Social Discussions Help Argument Mining?"},{"paperId":"a243f87588a2381257d325c40c9709b0e4d03ada","title":"The Change that Matters in Discourse Parsing: Estimating the Impact of Domain Shift on Parser Error"},{"paperId":"609e1c196fced582caf9113aa6a003b64d3cdcd6","title":"Better Language Model with Hypernym Class Prediction"},{"paperId":"8c25f38044b69f54233803c280677c3f8d547e9f","title":"Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models"},{"paperId":"2ff3b65e893afde3d98d0870129b1c7999fc1a43","title":"Multi-Stage Prompting for Knowledgeable Dialogue Generation"},{"paperId":"7a687bf250a9ec9013e91d4b436cc87943f71085","title":"Morphological Reinflection with Multiple Arguments: An Extended Annotation schema and a Georgian Case Study"},{"paperId":"214e8943b935c9c3e79b383191feebbdafe70fc5","title":"Multilingual Mix: Example Interpolation Improves Multilingual Neural Machine Translation"},{"paperId":"69078af65fc934f81fd340e9d1323d6c08194548","title":"Revisiting the Compositional Generalization Abilities of Neural Sequence Models"},{"paperId":"52bea123acbbefac6ebb3b40b6482465845ef014","title":"ClarET: Pre-training a Correlation-Aware Context-To-Event Transformer for Event-Centric Generation and Classification"},{"paperId":"f704ff871e1eb5d61c3ca2be39c9b1e171a49e76","title":"Continual Few-shot Relation Learning via Embedding Space Regularization and Data Augmentation"},{"paperId":"d3dd80269f2542cc173afb3a1df24b582a1e4af2","title":"Overcoming a Theoretical Limitation of Self-Attention"},{"paperId":"3fbe7f9a0ca640c915d1b4e2cca6e49a15ad710a","title":"Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction"},{"paperId":"833a2f1817cb9aeb292620454889cae78e26dda4","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning"},{"paperId":"9b1f4492a663c7f56f2b43ae1ed167d3857aacca","title":"PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts"},{"paperId":"7cbc2a7843411a1768ab762930707af0a3c33a19","title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"},{"paperId":"2558c42f6d24d3a85fe44ed4faef8a482785c174","title":"ZeroPrompt: Scaling Prompt-Based Pretraining to 1, 000 Tasks Improves Zero-Shot Generalization"},{"paperId":"c8efc90cde659ba22f48c96b486b45465c8e8bf9","title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models"},{"paperId":"6915f983d0d207ec9ce2bb12eefed86f3a14584c","title":"Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"},{"paperId":"40b4d98588719407fb72a014ab79e4145695654b","title":"Quantifying Adaptability in Pre-trained Language Models with 500 Tasks"},{"paperId":"cbf98ebe967e0f3f3236e7932f37013b98244e94","title":"ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning"},{"paperId":"7f2dd0a66a9e6570fc6123f0aab193084c1268fc","title":"Sharpness-Aware Minimization Improves Language Model Generalization"},{"paperId":"dbeff5429ff0caa85f9e02621928e787e789ca2b","title":"Hey AI, Can You Solve Complex Tasks by Talking to Agents?"},{"paperId":"e968a3c9590481cd13f2f86a7ac8839e3cf3455f","title":"Improving Compositional Generalization with Self-Training for Data-to-Text Generation"},{"paperId":"a84c5c4f0ef32c8a97f2f6c4059b5795f7172635","title":"ASPECTNEWS: Aspect-Oriented Summarization of News Documents"},{"paperId":"17dd3555fd1ccf1141cf984347fa1b3fd6b009ca","title":"Multitask Prompted Training Enables Zero-Shot Task Generalization"},{"paperId":"7d5c661fa9a4255ee087e861f820564ea2e2bd6b","title":"BBQ: A hand-built bias benchmark for question answering"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"a69d0b93d2ffcf25c964d93cc9b1adb73085232d","title":"Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations"},{"paperId":"8008348e87d3904842a2dd230c14b83112e8bf48","title":"Compositional Generalization in Dependency Parsing"},{"paperId":"ab72bccf6f3981537389510ecc609109e79595c3","title":"Disentangled Sequence to Sequence Learning for Compositional Generalization"},{"paperId":"043af12b66c2d598739921ac541d6d49c744aa07","title":"Situated Dialogue Learning through Procedural Environment Generation"},{"paperId":"692988f37e9ca79febea57099538790ebe7d3554","title":"LexGLUE: A Benchmark Dataset for Legal Language Understanding in English"},{"paperId":"9b2473c1d88f2300358dc8b11e826dc5ba81bbbc","title":"RNG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering"},{"paperId":"d031caface1f23995d4dc791e67454d7b4a19c1d","title":"CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"00ef52092ef3f109a09b66037707cd3227accb42","title":"Challenges in Generalization in Open Domain Question Answering"},{"paperId":"a1040e3d1aeb952b8a1d940e68f8632c34d58421","title":"LOT: A Story-Centric Benchmark for Evaluating Chinese Long Text Understanding and Generation"},{"paperId":"b3f644a5ea1fdd8cec1c34ebed69125838a50de3","title":"The Paradox of the Compositionality of Natural Language: A Neural Machine Translation Case Study"},{"paperId":"6dafc41e9bbd3aa476a0a1c15ca2c459eaef6b98","title":"(Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models’ Performance"},{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"eea7bca03bda3ee2448cd012bbcb2b33822861d8","title":"Noisy Channel Language Model Prompting for Few-Shot Text Classification"},{"paperId":"6692187d1835e767c5015ab24df8caf9f1d774da","title":"An Investigation of the (In)effectiveness of Counterfactually Augmented Data"},{"paperId":"9dc68f8362df16b2b7f17556a7728fe727055c6a","title":"AmericasNLI: Evaluating Zero-shot Natural Language Understanding of Pretrained Multilingual Models in Truly Low-resource Languages"},{"paperId":"0adec918885dff698acf359988ed79a543157f80","title":"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"},{"paperId":"cbdb45fc16b0885905b91d84281c310e6cb49e9c","title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions"},{"paperId":"eec41f0659b11d39fd606ac6cb8721c512a2ea6f","title":"Memorisation versus Generalisation in Pre-trained Language Models"},{"paperId":"fda76a1411e16722ebd2d8278c3143ca4363da6b","title":"Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models"},{"paperId":"bb774e72ee7ba8a3bb4db5ae76c4f2937b7437e9","title":"Improving Generalization of Hate Speech Detection Systems to Novel Target Groups via Domain Adaptation"},{"paperId":"18c8011df22875a20a67a6f3f8a27cfb3d5ea0a2","title":"Multilingualism Encourages Recursion: a Transfer Study with mBERT"},{"paperId":"4fb2132cbc3ac0fe025fc83519d6970cfc299681","title":"SIGMORPHON–UniMorph 2022 Shared Task 0: Generalization and Typologically Diverse Morphological Inflection"},{"paperId":"75ad544a694a2a6e3bf93930767fb21d251dde46","title":"Knowledge Distillation Meets Few-Shot Learning: An Approach for Few-Shot Intent Classification Within and Across Domains"},{"paperId":"6814ff8e9aa9cc28ff577023eea3f5167755721f","title":"An Empirical study to understand the Compositional Prowess of Neural Dialog Models"},{"paperId":"cb185d0cf894e550c0d612d8ddf6927e53021356","title":"How Can Cross-lingual Knowledge Contribute Better to Fine-Grained Entity Typing?"},{"paperId":"e24dcfa67258d1dc840143217c8dc6c525ee3c14","title":"Measuring the Language of Self-Disclosure across Corpora"},{"paperId":"0d7e687ed2ed2ab67a2539414b4c0c6143f8fb48","title":"Domain Generalisation of NMT: Fusing Adapters with Leave-One-Domain-Out Training"},{"paperId":"106468b5de8ef18a4612962d5a3b3e19272900fa","title":"Classification without (Proper) Representation: Political Heterogeneity in Social Media and Its Implications for Classification and Behavioral Analysis"},{"paperId":"b4db6695d7b41acd0bd42efe655b0d73e835f3ac","title":"Challenges to Open-Domain Constituency Parsing"},{"paperId":"8ab865ef15d57d5fa1b82bf010e721169dda54f6","title":"Leveraging Explicit Lexico-logical Alignments in Text-to-SQL Parsing"},{"paperId":"23756011261f20d411dc4d9bcd3a56b69884a72a","title":"Flooding-X: Improving BERT’s Resistance to Adversarial Attacks via Loss-Restricted Fine-Tuning"},{"paperId":"474c954231413f2a249f04272dbeda19cd8ff09b","title":"Bridging the Generalization Gap in Text-to-SQL Parsing with Schema Expansion"},{"paperId":"54f78090ec89493a0a49fdc598348db47fa08677","title":"Rare and Zero-shot Word Sense Disambiguation using Z-Reweighting"},{"paperId":"7869471766d5d73adaa37398a2f2fbaff9160221","title":"Bias Mitigation in Machine Translation Quality Estimation"},{"paperId":"4a180a03ca66bd19733d9273e9fc9dcb7582034e","title":"Learn to Adapt for Generalized Zero-Shot Text Classification"},{"paperId":"ae76a249bd12310a3a974e5a78da99d9b7eca4b2","title":"Cloze Evaluation for Deeper Understanding of Commonsense Stories in Indonesian"},{"paperId":null,"title":"Data-driven model generalizability in crosslinguistic lowresource morphological segmentation"},{"paperId":"30e3c4a3e38e42c515b9dcc7c4ac19f7db430a17","title":"On the cross-lingual transferability of multilingual prototypical models across NLU tasks"},{"paperId":"1403e6b9adf7712c35ae56327d52fe54603b87e1","title":"Few-shot Learning with Multilingual Language Models"},{"paperId":"68f141724814839d556a989646194be88641b143","title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"99278179243c3771440e6c3824f8aef2bf34ee07","title":"A Survey of Generalisation in Deep Reinforcement Learning"},{"paperId":"7cc74ffa1215321712d4a830bb9dee19d9f0fb47","title":"Grounded Graph Decoding Improves Compositional Generalization in Question Answering"},{"paperId":"76babaf07b884d7be6327d7cee0f9419df66159d","title":"A Unified Speaker Adaptation Approach for ASR"},{"paperId":"cbeff5462c08822f483bc7ed6678ce192e215d99","title":"Causal Transformers Perform Below Chance on Recursive Nested Constructions, Unlike Humans"},{"paperId":"d835d95e252c315103b435cc21a350ebc8d52616","title":"Cross-Lingual GenQA: Open-Domain Question Answering with Answer Sentence Generation"},{"paperId":"f3e14dfb76d23fd53d2140407e495b9263522bce","title":"Investigating the Effect of Natural Language Explanations on Out-of-Distribution Generalization in Few-shot NLI"},{"paperId":"ed4cd3c4f804c2d0d640558c01a2d7f1f807dff7","title":"Improving Multi-Party Dialogue Discourse Parsing via Domain Integration"},{"paperId":"abce205e296cda2256b7cc7f2229ac20566e28d5","title":"DMRST: A Joint Framework for Document-Level Multilingual RST Discourse Segmentation and Parsing"},{"paperId":"ce66fde956476755830e002fe3b7f5a048ef8bb5","title":"Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics"},{"paperId":"2618df03443da837f78ed935d2d97ced1964c93b","title":"JuriBERT: A Masked-Language Model Adaptation for French Legal Text"},{"paperId":"bb0ab8591d6d57c7e2bd1ec35d806b3f25277752","title":"Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks"},{"paperId":"67dc4ba4542d5895862c8b5af5023f659c14542c","title":"Single-dataset Experts for Multi-dataset Question Answering"},{"paperId":"a61aebbfe029c4b8eafae4042e6242cdca8f54b7","title":"Relation-Guided Pre-Training for Open-Domain Question Answering"},{"paperId":"420730f5f148fffd96aa0c011441bbf3f88bf8a4","title":"On Generalization in Coreference Resolution"},{"paperId":"396eade7a493413baff2da8e8b2aad70950a65f1","title":"Neural Unification for Logic Reasoning over Natural Language"},{"paperId":"3962f108081b22c7e54b413f47ba6f2c16f2cc05","title":"Frequency Effects on Syntactic Rule Learning in Transformers"},{"paperId":"669d98245819f8eee8966f855d69fa1b74c13893","title":"How Does Counterfactually Augmented Data Impact Models for Social Computing Constructs?"},{"paperId":"8e3b613f8a6d775a5bf97aa73328a2f4795dd407","title":"SituatedQA: Incorporating Extra-Linguistic Contexts into QA"},{"paperId":"c80436a9d7ec5d832e4875c89829294fc25841fd","title":"Effectiveness of Pre-training for Few-shot Intent Classification"},{"paperId":"df65efe264627303cbcbe89508d703fe8f9c716a","title":"Honey or Poison? Solving the Trigger Curse in Few-shot Event Detection via Causal Intervention"},{"paperId":"f45261b7b53043c316f45f613cb735907b93fb5a","title":"Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning"},{"paperId":"0b3863c21a7fb5ac61a447611cba0ec9ce1ab4a4","title":"Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization"},{"paperId":"238deab37e201c57505a4a47bb854e462af79bd7","title":"Entity-Based Knowledge Conflicts in Question Answering"},{"paperId":"4979495965d1a2239082b4b9fd5f335a288309b7","title":"DIALKI: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization"},{"paperId":"64f3a18921f7f3a384dca073cd6d2476b9af47f2","title":"Zero-Shot Dialogue State Tracking via Cross-Task Transfer"},{"paperId":"ed98ae2c7aec315488372476e964da3e3846360d","title":"Low-Resource Dialogue Summarization with Domain-Agnostic Multi-Source Pretraining"},{"paperId":"4c6f7fb5c2e1bd12899c3ec2788f9ce7eb2f8a5c","title":"How much pretraining data do language models need to learn syntax?"},{"paperId":"011dd2058a9e38ac91b1790377fdd7d0f99d2781","title":"Exploring Strategies for Generalizable Commonsense Reasoning with Pre-trained Models"},{"paperId":"4137ee5f7e831b77dcc5662dec83780b52d8adab","title":"Nearest Neighbour Few-Shot Learning for Cross-lingual Classification"},{"paperId":"676fa805bd715591f99bb17e36d673a6a14e92fe","title":"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization"},{"paperId":"14fa6eed1d77cfd8e40c9bf344cd5f0685f394ab","title":"Fight Fire with Fire: Fine-tuning Hate Detectors using Large Samples of Generated Hate Speech"},{"paperId":"fa7b8acd47631bada5b66049824bfd335ac6bf8f","title":"Towards Improving Adversarial Training of NLP Models"},{"paperId":"23d21bb495061564d2ce4540ea312959ab5e10f0","title":"Text AutoAugment: Learning Compositional Augmentation Policy for Text Classification"},{"paperId":"4caa5907965cb6d8e5985eea92e4b04c93a4ddb8","title":"Towards Out-Of-Distribution Generalization: A Survey"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"9a3da922eca7979c58faf39ba693831483a41079","title":"Not quite there yet: Combining analogical patterns and encoder-decoder networks for cognitively plausible inflection"},{"paperId":"03b7fe82dd7b08a9d86e39ed9a4d4000d90308b4","title":"Abstractive Text Summarization: Enhancing Sequence-to-Sequence Models Using Word Sense Disambiguation and Semantic Content Generalization"},{"paperId":"f75fd3288a60233bbc766037941f5b370fae9168","title":"Structural Guidance for Transformer Language Models"},{"paperId":"47cc6e97d71317052672e82e42154707b4485b49","title":"Did the Cat Drink the Coffee? Challenging Transformers with Generalized Event Knowledge"},{"paperId":"3397f25209666d30a8b797932e3197cc826fba18","title":"Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks"},{"paperId":"2ee03e28208a9310a9be4032c2b04ebdddb83cc7","title":"FLEX: Unifying Evaluation for Few-Shot NLP"},{"paperId":"f9ff3facd992951415e67ce08cd31c5ba8c04b4e","title":"Doing Good or Doing Right? Exploring the Weakness of Commonsense Causal Reasoning Models"},{"paperId":"1bed382373aed687c045bb65bc7541b16fc7a6be","title":"Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN"},{"paperId":"bb1839548939385aed7a8a8dfdf5e85c63a3e177","title":"Learning to Ask Conversational Questions by Optimizing Levenshtein Distance"},{"paperId":"0390383a49ec1feb7921d6f61f80b6988b3598f2","title":"Enhancing the generalization for Intent Classification and Out-of-Domain Detection in SLU"},{"paperId":"ecf5618b513aa5c4d5bf62ca251923a188251117","title":"XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages"},{"paperId":"9cd3d6eef7c574830be410598c3024191ee974d4","title":"KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers"},{"paperId":"523745e29f6cb1890f18352d449fd3597910c485","title":"Improving Compositional Generalization in Classification Tasks via Structure Annotations"},{"paperId":"378e2dc920f312fdb1c5ab095dfc2a0de9208199","title":"X-Fact: A New Benchmark Dataset for Multilingual Fact Checking"},{"paperId":"a51b35aa7488a51f516e4e40914333fc0b819e74","title":"On the proper role of linguistically-oriented deep net analysis in linguistic theorizing"},{"paperId":"453073c445bbf48bc17c37fc5ffbd91a20c8148d","title":"Domain-independent User Simulation with Transformers for Task-oriented Dialogue Systems"},{"paperId":"abbd05525ce06b009241da7a4734cdec39b4c3f0","title":"Generative Conversational Networks"},{"paperId":"f1b3c81604806ac2d7be3a4d48ebfd41289eb104","title":"Modeling Profanity and Hate Speech in Social Media with Semantic Subspaces"},{"paperId":"390f174d102c72172249254f3f1048721c0c3161","title":"Question Generation for Adaptive Education"},{"paperId":"3b8c8bbf2c1a69f2bf60b2434f2d1996aea2e740","title":"One Semantic Parser to Parse Them All: Sequence to Sequence Multi-Task Learning on Semantic Parsing Datasets"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"bb3425318de7eed5641cda147d61c9a057b9d054","title":"Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks"},{"paperId":"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","title":"Lexicon Learning for Few Shot Sequence Modeling"},{"paperId":"d1a2b23efdc7acd927403e2f573132d262340ac2","title":"X2Parser: Cross-Lingual and Cross-Domain Framework for Task-Oriented Compositional Semantic Parsing"},{"paperId":"74592257b9812f7d3a4a8a1cec31d6a1fd174c2f","title":"Meta-Learning with Variational Semantic Memory for Word Sense Disambiguation"},{"paperId":"dda0bce7baee7175f7b0e5f0ac81669ed1c13f07","title":"Syntax-augmented Multilingual BERT for Cross-lingual Transfer"},{"paperId":"f96e1494d714f6bbf496bdd137300c7db5500753","title":"A Dataset and Baselines for Multilingual Reply Suggestion"},{"paperId":"c8ff35e58c7d8db1024cd898cdf422452a11aefd","title":"Uncovering Constraint-Based Behavior in Neural Models via Targeted Fine-Tuning"},{"paperId":"ae14dd88917b732a41475a6cb5f37d3af30d82e1","title":"multiPRover: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning"},{"paperId":"c8f2dbb4addb9f8c531858f2aad80676c0787e95","title":"Content-based Stance Classification of Tweets about the 2020 Italian Constitutional Referendum"},{"paperId":"4e3a0aa2ce77563a7ff7d440ad2388aaa6fc92d0","title":"Anlirika: An LSTM–CNN Flow Twister for Spoken Language Identification"},{"paperId":"95d97288e3af9c891f172a8fc624983a3141e27a","title":"Improving the Performance of UDify with Linguistic Typology Knowledge"},{"paperId":"26da4d36dc5c404298cb9e0069a89b5964b0ffe7","title":"Detecting Multilingual COVID-19 Misinformation on Social Media via Contextualized Embeddings"},{"paperId":"d4c9fa8854d9d3ee8865656d67a26b51841d8ee3","title":"Do RNN States Encode Abstract Phonological Alternations?"},{"paperId":"68713fdc2b2e7182548c8d7d3a247bddb79c51b4","title":"Domain Adaptation for Arabic Cross-Domain and Cross-Dialect Sentiment Analysis from Contextualized Word Embedding"},{"paperId":"88dbde378e9ac5c25fc7d78f5da147223e8d34d4","title":"Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention"},{"paperId":"291016368158f28829c06c0a037e0ca1a6548cca","title":"HiddenCut: Simple Data Augmentation for Natural Language Understanding with Better Generalizability"},{"paperId":"d6218ce8a829c705678884d77ad791753755175e","title":"An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers"},{"paperId":"03ad126cfe495933f7bb769f27c03e5f31caedf8","title":"On Compositional Generalization of Neural Machine Translation"},{"paperId":"755038a42abf9a9fa6d40ac7c0eb9fbe6298d3f0","title":"Exploration and Exploitation: Two Ways to Improve Chinese Spelling Correction Models"},{"paperId":"2d3e4e5690b992d3099caa1606d4310a0d632868","title":"Language Models Use Monotonicity to Assess NPI Licensing"},{"paperId":"e638b9e6ee09ab4fa748b748099e0f03d471d803","title":"Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization"},{"paperId":"b58d8579ece27a60432e667bfbdb750590fa65d9","title":"True Few-Shot Learning with Language Models"},{"paperId":"6ff5ea40e0d1be8c71e5b675db64fe730018db03","title":"Learning Language Specific Sub-network for Multilingual Machine Translation"},{"paperId":"7ffc1b425026e916cd6db37c79df3e08e8f47ee6","title":"OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics"},{"paperId":"a293a01ddd639b25360cf4f23e2df8dd0d1caa8e","title":"Few-NERD: A Few-shot Named Entity Recognition Dataset"},{"paperId":"513e6d3ac68f170feebaee7c9b53a1b8e9533773","title":"Neural-Symbolic Commonsense Reasoner with Relation Predictors"},{"paperId":"4e9d4b3d89b4dec3d313c3295c70d3afff6ae50f","title":"Adversarial Learning for Zero-Shot Stance Detection on Social Media"},{"paperId":"1651a9a3fc460a53440503f1de065541eeafd1e4","title":"Encoding Explanatory Knowledge for Zero-shot Science Question Answering"},{"paperId":"e4cdfb01011ad4a10be1858aa7722632b42b9d23","title":"OCHADAI-KYOTO at SemEval-2021 Task 1: Enhancing Model Generalization and Robustness for Lexical Complexity Prediction"},{"paperId":"aed58f90d8a2e8e9dca35efee21e1b7e806600a8","title":"Generalising Multilingual Concept-to-Text NLG with Language Agnostic Delexicalisation"},{"paperId":"193054e7e3a83d3e36926268e9c3706721973c57","title":"Learning to Perturb Word Embeddings for Out-of-distribution QA"},{"paperId":"a4aa9b406ce8d648a4f9b871d1b1b2e1356551f6","title":"Learning to Learn to be Right for the Right Reasons"},{"paperId":"5dd431c010369169599067f6e24468b10a8edc82","title":"X-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering"},{"paperId":"f4b368e9b8ff32f4bd51080910cbab87c3ce460a","title":"Hidden Biases in Unreliable News Detection Datasets"},{"paperId":"7fa273f450251523e6b7fcc2eb3fdbdfd4a30493","title":"CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP"},{"paperId":"d8094a2fa4f3c7a3bf641d12f314a085d166f8a8","title":"Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation"},{"paperId":"b3f98f5baf3b7239ce0da45147bda4c4154fac9c","title":"Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning"},{"paperId":"c45c627aa40a982788e1a1e3b1b2a2091b2a71d0","title":"Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training"},{"paperId":"4a56f72b9c529810ba4ecfe9eac522d87f6db81d","title":"Explaining Answers with Entailment Trees"},{"paperId":"be9679047ff70d6147dbdb42521f62c5660f6f75","title":"Structure-Aware Abstractive Conversation Summarization via Discourse and Action Graphs"},{"paperId":"2ef4be35f8424ea768aa2e1b44392b3eddbc780b","title":"Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema"},{"paperId":"a2a7033a5a859e3a6e6f0a83018326400b4c5faa","title":"Retrieval Augmentation Reduces Hallucination in Conversation"},{"paperId":"2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"},{"paperId":"3bdbd9346c6e5a54fcdcb9c42811f4593542bba9","title":"Span Pointer Networks for Non-Autoregressive Task-Oriented Semantic Parsing"},{"paperId":"e8267569b7a1af840a3e0f6b58590549104d95fe","title":"Is Everything in Order? A Simple Way to Order Sentences"},{"paperId":"4e00843bc5f60d2b9116abc4320af6d184422291","title":"Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little"},{"paperId":"afd8fb26a7094ed3a8838000d50e2b22f815dc28","title":"Learning to Synthesize Data for Semantic Parsing"},{"paperId":"677a7a940dcff639bd066f25b395a361a08a60f9","title":"Continual Learning for Text Classification with Information Disentanglement Based Regularization"},{"paperId":"8e88f835acdabf16eda88ece9c5acedac64a6274","title":"Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach"},{"paperId":"c114db5f1c38cbe6797bc74ef98072cac71f6cc6","title":"ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser"},{"paperId":"4b0ec90dc10e51c1fc983edcd57bb86636d7b3ca","title":"Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections"},{"paperId":"6e91cfc05688330aa26395efa8e814e951191186","title":"TransWiC at SemEval-2021 Task 2: Transformer-based Multilingual and Cross-lingual Word-in-Context Disambiguation"},{"paperId":"77a096d80eb4dd4ccd103d1660c5a5498f7d026b","title":"Dynabench: Rethinking Benchmarking in NLP"},{"paperId":"1abe884af880135c80753dd3086234566bf0b8b9","title":"Disentangling Document Topic and Author Gender in Multiple Languages: Lessons for Adversarial Debiasing"},{"paperId":"32128a25b1ebf868f9b02590e361a98524bd371b","title":"AdaptSum: Towards Low-Resource Domain Adaptation for Abstractive Summarization"},{"paperId":"89c2dc86dd329e512f1a2cba25ee45f5b482752a","title":"Evaluating Document Coherence Modeling"},{"paperId":"24ac368d08765dfad920ceefb79fba7bfe81d83c","title":"The Effect of Domain and Diacritics in Yoruba–English Neural Machine Translation"},{"paperId":"9f7138a39df8dbe83b11699690b0c2ffe44523ae","title":"Learning Feature Weights using Reward Modeling for Denoising Parallel Corpora"},{"paperId":"a620d33208398950e633615c9adf06ba0bbfd35a","title":"Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU models"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"27a71682ccfc00650588482048077c3104f08801","title":"Conditional Adversarial Networks for Multi-Domain Text Classification"},{"paperId":"ba233d75aa403092bda0bffc026be7913673ad69","title":"Mind the Gap: Assessing Temporal Generalization in Neural Language Models"},{"paperId":"9eea59c34f139f3d2153226c8cf026e975622074","title":"Memorization vs. Generalization : Quantifying Data Leakage in NLP Performance Evaluation"},{"paperId":"80b0ee8b3f738e535dcf8b8c1223be5f8e3c25ba","title":"Language Modelling as a Multi-Task Problem"},{"paperId":"789b5441743c2e38cf4c38749ed820c0671d81b1","title":"Muppet: Massive Multi-task Representations with Pre-Finetuning"},{"paperId":"0d39d525f30609d0541330f933007025cd457a83","title":"Exploring Transitivity in Neural NLI Models through Veridicality"},{"paperId":"13f67bd8700d5cd869c07b4aa9cb92b9e79bb17e","title":"Model Compression for Domain Adaptation through Causal Effect Estimation"},{"paperId":"9e42cb2b133bc41600824a1002cb05844c6a46a9","title":"Learning to Generate Task-Specific Adapters from Task Description"},{"paperId":"fcfeaba7f8aacc4198d3028d0c7ab6d6b7679224","title":"Superbizarre Is Not Superb: Derivational Morphology Improves BERT’s Interpretation of Complex Words"},{"paperId":"cd02e0a094953077217e2e62f3557b36a365acff","title":"Optimizing Deeper Transformers on Small Datasets"},{"paperId":"e885821da51a4370b4eb8ecd9f62047f6ee604db","title":"RADDLE: An Evaluation Benchmark and Analysis Platform for Robust Task-oriented Dialog Systems"},{"paperId":"01a9102aa93b152f2d2978c568fb7061eb7152f1","title":"I like fish, especially dolphins: Addressing Contradictions in Dialogue Modeling"},{"paperId":"40848b41ed8c9c255ecd8a920006877691b52d03","title":"WILDS: A Benchmark of in-the-Wild Distribution Shifts"},{"paperId":"1d7f3297924a9dd90cfc0df522ebe9138c28b46f","title":"Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals"},{"paperId":"2f79aa196381d86ba938bf7c6503b3bf71a524dd","title":"Supertagging the Long Tail with Tree-Structured Decoding of Complex Categories"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"9d436d25981ea61db728bd490f0d54376d08953e","title":"Temporal Reasoning on Implicit Events from Distant Supervision"},{"paperId":"578d361b53fe5a7970bd6f96faf5b6d1d6ca1992","title":"Effective Distant Supervision for Temporal Relation Extraction"},{"paperId":"1a2bed6bed5f28095ebe5793f0c31d38690d6cd4","title":"Posterior Differential Regularization with f-divergence for Improving Model Robustness"},{"paperId":"19bd467b1c8de94b9bdaef1499788467937f594e","title":"Meta-Learning for Domain Generalization in Semantic Parsing"},{"paperId":"cc0fc60e9b9d036acf53899c259574c8ce2aedfc","title":"Compositional Generalization via Semantic Tagging"},{"paperId":"e4cb6bfe88a8ed729d34d5a9ff74a992932b70ce","title":"Scientific Claim Verification with VerT5erini"},{"paperId":"08f501ea41cafa359c99269612579f5cac43c744","title":"XOR QA: Cross-lingual Open-Retrieval Question Answering"},{"paperId":"04cea2d855c5bbc5e7ddeed47a3ad8ecc54d4f5c","title":"Explicitly Modeling Syntax in Language Models with Incremental Parsing and a Dynamic Oracle"},{"paperId":"4c33fe28dc1f4bafee41aeef2e8ba54ef8b5588c","title":"Self-Supervised Contrastive Learning for Efficient User Satisfaction Prediction in Conversational Agents"},{"paperId":"687b13c44f849d23c2496996b5da83e706094db9","title":"Beyond English-Centric Multilingual Machine Translation"},{"paperId":"f76d6d4a8f8d5264ec7ae9c3cb6e05a1477d6150","title":"Incorporating Commonsense Knowledge into Abstractive Dialogue Summarization via Heterogeneous Graph Networks"},{"paperId":"a29c55573839087490d7469fbbcfd06728566b44","title":"Paired Examples as Indirect Supervision in Latent Decision Models"},{"paperId":"aea909a04e4d848855afcf8ce8ae55a81a2623e4","title":"SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval"},{"paperId":"2dbf7d8b41095fa508029a6f6dc1e38e0d59f783","title":"On the Transferability of Minimal Prediction Preserving Inputs in Question Answering"},{"paperId":"4d766ae60fbbaea4f6c4ca8ad7f14569ec3189d1","title":"Critical Thinking for Language Models"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"a8a168d53e01b0c35d626cfced103656e22b8343","title":"MTOP: A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark"},{"paperId":"cb58542c94ce83b09f5d3809e69518ba52709c92","title":"Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets"},{"paperId":"750274c61a0936c9c9db32da659cddbcb227cf32","title":"MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering"},{"paperId":"6e3f8187f8fef3e11578a73f32da07d33dbf8235","title":"DART: Open-Domain Structured Data Record to Text Generation"},{"paperId":"8e4afdbc0dd6c10b630f5e5cc4559a4c6474e8f4","title":"Mechanisms for handling nested dependencies in neural-network language models and humans"},{"paperId":"4f0318290bc75294338fdeb450e4365929b3aa0c","title":"CausaLM: Causal Model Explanation Through Counterfactual Language Models"},{"paperId":"011869f932f89d047ce2bd36d73a95cc04888193","title":"RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms"},{"paperId":"d0fcdf47561ff742c9a72495102f16646eca43b7","title":"We Need To Talk About Random Splits"},{"paperId":"a66fd96f3ea9473a5e7ff30eb5c876850492e7e1","title":"Lexical Semantic Recognition"},{"paperId":"6606f7b9938dcc0e03dba45359dd04cb15018f13","title":"Robust Question Answering Through Sub-part Alignment"},{"paperId":"7718a9b2b33bc6b9c1d4c4b9a3e1d473ffe5c330","title":"DuReader_robust: A Chinese Dataset Towards Evaluating Robustness and Generalization of Machine Reading Comprehension in Real-World Applications"},{"paperId":"1849955aa9e439d99082bcf038550363e9622e35","title":"Parameter Space Factorization for Zero-Shot Learning across Tasks and Languages"},{"paperId":"8ae9a17c87a4518b513e860683a0ef7824be994d","title":"Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"},{"paperId":"e69e5953905b9b9ded4c07f0505ed401ec39babf","title":"Universal Grammar"},{"paperId":"47f3f165f05d9cd2eca29d1559cb65824641db6b","title":"Extend, don’t rebuild: Phrasing conditional graph modification as autoregressive sequence labelling"},{"paperId":null,"title":"Efficient large scale language modeling with mixtures of experts. CoRR, abs/2112.10684"},{"paperId":"b85e611e3be0f540343ad29491add893f3d97e6d","title":"Combining Shallow and Deep Representations for Text-Pair Classification"},{"paperId":"4fa1a3d26910458131536913e3f6db2cbeb79e4b","title":"Training Strategies for Neural Multilingual Morphological Inflection"},{"paperId":"17fab13f3b81efc8d02fde6c1d32bb847905ba7f","title":"Were We There Already? Applying Minimal Generalization to the SIGMORPHON-UniMorph Shared Task on Cognitively Plausible Morphological Inflection"},{"paperId":"8b383bd2eb054787d84636dab7b6ecf4a318c6c7","title":"Towards a Language Model for Temporal Commonsense Reasoning"},{"paperId":"136235d2a3dc4f1c995eaf977aec9c42114da850","title":"SIGMORPHON 2021 Shared Task on Morphological Reinflection: Generalization Across Languages"},{"paperId":"8d1b4088849c998cdadd653b67d38c4886663959","title":"How to Select One Among All ? An Empirical Study Towards the Robustness of Knowledge Distillation in Natural Language Understanding"},{"paperId":"266e16487ff6a4db3738629db76e77d8cb413817","title":"CNNBiF: CNN-based Bigram Features for Named Entity Recognition"},{"paperId":"307e3e00c96b3dc62a8258c95e62b3e41d183278","title":"Improving End-to-End Task-Oriented Dialog System with A Simple Auxiliary Task"},{"paperId":"e67af0f1bfde65d8387d5c5589d1373e8ce39a90","title":"Word Discriminations for Vocabulary Inventory Prediction"},{"paperId":"61cd4ffdaf2c0daa3d432ff9fecdd064d6e72886","title":"Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI"},{"paperId":"3b1de514ff3955034605a4a5d7121ecff694bc4a","title":"MassiveSumm: a very large-scale, very multilingual, news summarisation dataset"},{"paperId":"1a95eed753096bcf219fa5394623141899115851","title":"How Do Neural Sequence Models Generalize? Local and Global Cues for Out-of-Distribution Prediction"},{"paperId":"e01124ff4a3ba76fcac040597395b816cc074bc6","title":"Effective Fine-Tuning Methods for Cross-lingual Adaptation"},{"paperId":"983ff7ddb1b7c35be0b3929d20391216e6dde191","title":"On Classifying whether Two Texts are on the Same Side of an Argument"},{"paperId":"84aec29de31b56b3324c00667dfac62850f8dadf","title":"Few-Shot Named Entity Recognition: An Empirical Baseline Study"},{"paperId":"c71f40033cb0f8e76ef28e504240ae14317a5094","title":"Learning Universal Authorship Representations"},{"paperId":"056ccd104429263b5e52d1f2e59d2c1eb0f0af3d","title":"Automatic Classification of Attributes in German Adjective-Noun Phrases"},{"paperId":"1d49411a289eb0c74f68ea469134742f66796c97","title":"Predicate Representations and Polysemy in VerbNet Semantic Parsing"},{"paperId":"bcf2bc325e4a48b615efb9cad2da2ce3e2ecbec7","title":"Solving SCAN Tasks with Data Augmentation and Input Embeddings"},{"paperId":"30b2fcf292cd58d6df5813cc996f9c606276d961","title":"WIKIBIAS: Detecting Multi-Span Subjective Biases in Language"},{"paperId":"e3d9ff9cdbcef0c3e7675eb43661acbe44654e23","title":"Learning Cross-lingual Representations for Event Coreference Resolution with Multi-view Alignment and Optimal Transport"},{"paperId":"b67f488bc5e4e9629673c439f6878661d02987df","title":"Testing Cross-Database Semantic Parsers With Canonical Utterances"},{"paperId":"e239ea64487bab0d1b03259fede07cec9eb152e1","title":"Counterfactual Matters: Intrinsic Probing For Dialogue State Tracking"},{"paperId":"7b7b8531406dac284c577da10ad630115dc60d13","title":"Assessing the Generalization Capacity of Pre-trained Language Models through Japanese Adversarial Natural Language Inference"},{"paperId":"e92c5d8a15c16899777e075c73a6f1fc8531ef66","title":"Multi-Layer Random Perturbation Training for improving Model Generalization Efficiently"},{"paperId":"f5ec1ed1ec436123cc24ac73a354738658c6bce1","title":"Generalising to German Plural Noun Classes, from the Perspective of a Recurrent Neural Network"},{"paperId":"f6d78e7a56e1a39dc656a7c36570f7b9412ffac7","title":"Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda"},{"paperId":"e3737026564c8b64a79a6ee95f6bbb4a6eebd145","title":"Spurious Correlations in Cross-Topic Argument Mining"},{"paperId":"4302276c6fe5e01ddde123e97ffdc64bf8c33b81","title":"Semi-supervised Meta-learning for Cross-domain Few-shot Intent Classification"},{"paperId":"92fb9e98468a5e7c0890e284ce6ff435094a2fc2","title":"Multi-Pair Text Style Transfer for Unbalanced Data via Task-Adaptive Meta-Learning"},{"paperId":"1c59de25af45cef20d846ec7454251e8237d45d1","title":"A Statistical Extension of Byte-Pair Encoding"},{"paperId":"439246df6d82bf95d2006bc027cdc1ef3c604f74","title":"Evaluating Text Generation from Discourse Representation Structures"},{"paperId":"c9e58cdcd05ab00f48953238a159dc8b1943c047","title":"Attribute Value Generation from Product Title using Language Models"},{"paperId":"fd8f89e592d06af710b8350349769dbd8979f49f","title":"Fine-grained Event Classification in News-like Text Snippets - Shared Task 2, CASE 2021"},{"paperId":"c77e874b18852738acbccf17f9e585a78973d924","title":"Guideline Bias in Wizard-of-Oz Dialogues"},{"paperId":"525d5835b66612b11c2714645d92297b221e9daf","title":"QA-Driven Zero-shot Slot Filling with Weak Supervision Pretraining"},{"paperId":"dffd39f36deaf68e9916b945800757993895baf8","title":"How Helpful is Inverse Reinforcement Learning for Table-to-Text Generation?"},{"paperId":"0df9c19659388e55745c290ace520491c2985d9b","title":"MulDA: A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual NER"},{"paperId":"26db43bc04dd8ca827f7700e3eaac04b5e5b7bad","title":"BanditMTL: Bandit-based Multi-task Learning for Text Classification"},{"paperId":"2f34693c39aaca2617db8965f82515003e359ebf","title":"Improving Model Generalization: A Chinese Named Entity Recognition Case Study"},{"paperId":"07d9bab660fc42aafc2681539ef6362d23a843ff","title":"Synthetic Examples Improve Cross-Target Generalization: A Study on Stance Detection on a Twitter corpus."},{"paperId":"4c436f1fb70611e3bab1c182b3789c9b02ec28d7","title":"Adversarial Training for News Stance Detection: Leveraging Signals from a Multi-Genre Corpus."},{"paperId":"14cab7aa7ce37911c481df3af63ff1ba6fabec8c","title":"Using a Frustratingly Easy Domain and Tagset Adaptation for Creating Slavic Named Entity Recognition Systems"},{"paperId":"6df985d08020f468f58d065ea2802904b581ce52","title":"Multidomain Pretrained Language Models for Green NLP"},{"paperId":"1f0e1657063ea38cf225eaf1c1187ae7b2e4a0e0","title":"Increasing Robustness to Spurious Correlations using Forgettable Examples"},{"paperId":"cb3767751f64ac159e637d1694f110d811ea6928","title":"Cross-Topic Rumor Detection using Topic-Mixtures"},{"paperId":"431ad023149287abc496d61570ba167fb014cf54","title":"Maximal Multiverse Learning for Promoting Cross-Task Generalization of Fine-Tuned Language Models"},{"paperId":"9d4e23f9fe66a82c60e21664f9d720e996a45699","title":"Keep Learning: Self-supervised Meta-learning for Learning from Inference"},{"paperId":"355d3e4bf5430ba16f95496513b69d5f4753bcef","title":"Zero-shot Cross-lingual Content Filtering: Offensive Language and Hate Speech Detection"},{"paperId":"c4db295d5a73649bdeafc52833f2ac748e277bc6","title":"IA On Learning the Past Tenses of English Verbs"},{"paperId":null,"title":"Bme submission for sigmorphon 2021 shared task 0"},{"paperId":null,"title":"Leveraging paradigmatic information in inflection acceptability prediction: The JHU-SFU submission to SIGMORPHON shared task"},{"paperId":"373bc164d7b552f8782988e7da6b0d00092a20b0","title":"Continual Lifelong Learning in Natural Language Processing: A Survey"},{"paperId":"9d1f9406ed676171d9975e27606c95633ca898b1","title":"On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in BERT"},{"paperId":"c66a5c3fd98d0cd87b710488b0612e7c11e53f14","title":"Hy-NLI: a Hybrid system for Natural Language Inference"},{"paperId":"fc33bbbe296b34bf8f52f0d7a4816725e0607502","title":"Event Coreference Resolution with their Paraphrases and Argument-aware Embeddings"},{"paperId":"3f2749398436423dc48289008f99c6d386793e42","title":"Noise Isn’t Always Negative: Countering Exposure Bias in Sequence-to-Sequence Inflection Models"},{"paperId":"852e2545d8a7617129800b200fcc2f89a42235aa","title":"Monolingual Adapters for Zero-Shot Neural Machine Translation"},{"paperId":"4b58367375466e653751a0c258b2f50bd3551408","title":"Sequence-to-Sequence Networks Learn the Meaning of Reflexive Anaphora"},{"paperId":"c204d40384d39c59cd7249bde4cd8615972acaac","title":"Findings of the WMT 2020 Shared Task on Machine Translation Robustness"},{"paperId":"3f7e5cdfb420a6a42da82dc7ac4c257babebcd75","title":"DATAMAFIA at WNUT-2020 Task 2: A Study of Pre-trained Language Models along with Regularization Techniques for Downstream Tasks"},{"paperId":"327b12c21e7b8f2126bd7d64ebc2ede0db220c88","title":"Do We Need to Create Big Datasets to Learn a Task?"},{"paperId":"ed005193b3050770155c79ea51ffb1ece1dfd56c","title":"Efficient Estimation of Influence of a Training Instance"},{"paperId":"78958b0e88cb3b2c18e1ac1b34516bc370926599","title":"Not a cute stroke: Analysis of Rule- and Neural Network-based Information Extraction Systems for Brain Radiology Reports"},{"paperId":"6432f3a3ebd5a64c21c63ba8c593015af9dd3306","title":"Natural Language Response Generation from SQL with Generalization and Back-translation"},{"paperId":"f950beb2624ef0fd701a8da19a45294ffdddb3a3","title":"Enhancing Generalization in Natural Language Inference by Syntax"},{"paperId":"91075e07f1ffcd7ee0b3ed927925cfca05f5e41e","title":"Do Models of Mental Health Based on Social Media Data Generalize?"},{"paperId":"d12687e6fe1e9f58c6d9fe9a04e65bba7b4fc935","title":"Unsupervised Domain Adaptation for Cross-lingual Text Labeling"},{"paperId":"969150e36d16e4295326c7a231b97a86ad9c1ba9","title":"Event Extraction as Multi-turn Question Answering"},{"paperId":"0f4f27bb267b238d6044375863335db7fe69d661","title":"This is a BERT. Now there are several of them. Can they generalize to novel words?"},{"paperId":"7b404cca77f8eecd85ab2df24101023e8c40c505","title":"The Thieves on Sesame Street Are Polyglots — Extracting Multilingual Models from Monolingual APIs"},{"paperId":"106fb432d2b62f3824a9d6f4a1b30e1f8b6ea9d7","title":"Sequence-level Mixed Sample Data Augmentation"},{"paperId":"016ca039d9f5220c96b26f15d90d82064c361bfa","title":"Learning from Task Descriptions"},{"paperId":"6eb3804a3b7f1f36fedb564b28b0d2332bd4e091","title":"Modeling Content Importance for Summarization with Pre-trained Language Models"},{"paperId":"800d0e41bdab9dc8150ae31831ce5b5dfc16f899","title":"A Joint Multiple Criteria Model in Transfer Learning for Cross-domain Chinese Word Segmentation"},{"paperId":"04a7d9f0388ded93c1ec16e36a6df3cd44cb95b0","title":"Entity Linking in 100 Languages"},{"paperId":"7ade8a61f267136a5e68316a6ba39d382c90857a","title":"Investigating Novel Verb Learning in BERT: Selectional Preference Classes and Alternation-Based Syntactic Generalization"},{"paperId":"10391eed628dfece8a9136f76c5df53b5704422d","title":"Social Chemistry 101: Learning to Reason about Social and Moral Norms"},{"paperId":"c4546e343a3212ce98a2af090807432af2f9f79d","title":"Target Word Masking for Location Metonymy Resolution"},{"paperId":"ed8f03230b8f38b6e3822efeacf258fe67d908c4","title":"Strongly Incremental Constituency Parsing with Graph Neural Networks"},{"paperId":"77d4bd7ffd1aa1e704822a214af3d6e454da0398","title":"Compressive Summarization with Plausibility and Salience Modeling"},{"paperId":"bf653319c217af18debb1c2ebebfbf1485dcaba8","title":"Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses"},{"paperId":"624a9920ec98179d34a882710cccc71f5b50cd14","title":"On Cross-Dataset Generalization in Automatic Detection of Online Abuse"},{"paperId":"227fe850a72fab24998c7e08d75db214715dc74e","title":"The EOS Decision and Length Extrapolation"},{"paperId":"1665977dd646ed8ebbbaf848b0aa694e7fee32ed","title":"XL-WiC: A Multilingual Benchmark for Evaluating Semantic Contextualization"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"df8108f1f803c92e6d00d1244a355a35c3d64fa6","title":"Structural Supervision Improves Few-Shot Learning and Syntactic Generalization in Neural Language Models"},{"paperId":"37882abaec01eba1bf5bda8a36c904aaea0d5642","title":"Improving Compositional Generalization in Semantic Parsing"},{"paperId":"613effb92cae57fb3850a0b4cb4c84873e1c66ca","title":"An Empirical Study of Cross-Dataset Evaluation for Neural Summarization Systems"},{"paperId":"055fac05cd424e7b1bdcd359ff7980ca8d938ef3","title":"Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually)"},{"paperId":"3fd45fc420a882ab2fba3166ef08f376cc758ad0","title":"On Long-Tailed Phenomena in Neural Machine Translation"},{"paperId":"63897018266ce8b8df9a63845c561aa63c90285a","title":"Counterfactually-Augmented SNLI Training Data Does Not Yield Better Generalization Than Unaugmented Data"},{"paperId":"91bad6519095404998f4ce23592547b409cdb60a","title":"Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data"},{"paperId":"9c860cdbd0a29551f39aee022c9064ec0e89d30d","title":"Zero-Shot Stance Detection: A Dataset and Model Using Generalized Topic Representations"},{"paperId":"e71885cfa161b3fce024cb75887c06727abe8800","title":"Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing"},{"paperId":"f00f2d4b8ddd55aa2cc202f44053e5f97a254175","title":"WikiLingua: A New Benchmark Dataset for Cross-Lingual Abstractive Summarization"},{"paperId":"38d9db97d82a96e7bf39a299cfcdf9de1f033107","title":"Improving QA Generalization by Concurrent Modeling of Multiple Biases"},{"paperId":"553028f7f7c850371379c621e40d7d00e75303a6","title":"On Negative Interference in Multilingual Models: Findings and A Meta-Learning Treatment"},{"paperId":"e2d38543bd3cf813c63df336b21b003156ed48a8","title":"Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start"},{"paperId":"6f33bd4e62955f4d40424f8ae4ec83af4e97862c","title":"KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation"},{"paperId":"7144fc4dd1688056468c30add521fc3a5fbe2bbb","title":"Learning to Generalize for Sequential Decision Making"},{"paperId":"68ee4b7ef5163836c62d51a928efb23fd07d9fff","title":"X-SRL: A Parallel Cross-Lingual Semantic Role Labeling Dataset"},{"paperId":"89cb62dc83c1b1895267bd28639fbf5bb7ed21a4","title":"Measuring Systematic Generalization in Neural Proof Generation with Transformers"},{"paperId":"8a195d8ea613ffed341990fb757604ebe67f99ec","title":"TaxiNLI: Taking a Ride up the NLU Hill"},{"paperId":"ee5fff85d3ec62698eddba162f054b7e73670b2a","title":"Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics"},{"paperId":"67f343b5212d3a71965d2c217bb567f8af0bbcdb","title":"SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness"},{"paperId":"be1ca892708727324fba9ca9c218702a652e8fcc","title":"Composed Variational Natural Language Generation for Few-shot Intents"},{"paperId":"89f1710e84aadcace515171a1280af8aeafbfed9","title":"NEU at WNUT-2020 Task 2: Data Augmentation To Tell BERT That Death Is Not Necessarily Informative"},{"paperId":"59467dd8020d493a52116c5be18c958fb1925717","title":"Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks"},{"paperId":"04422085a52050516b9741e0fd1fda964b73dd53","title":"An Empirical Study on Robustness to Spurious Correlations using Pre-trained Language Models"},{"paperId":"b3f1765a12bae7b44c929a5cae5ddf6b886b5358","title":"Joint Training with Semantic Role Labeling for Better Generalization in Natural Language Inference"},{"paperId":"86c74c1028cca22541b6004213ad93abbd35fa30","title":"Cross-Lingual Disaster-related Multi-label Tweet Classification with Manifold Mixup"},{"paperId":"23835438889899885d9f33de2fb2356da10bbc0c","title":"Compositional Generalization by Factorizing Alignment and Translation"},{"paperId":"509b42fc150a057a64c4608f64e779ef04fdff47","title":"Temporally-Informed Analysis of Named Entity Recognition"},{"paperId":"bd044d307b75a0b35adfa62fa54c2c9a836e0547","title":"Exploiting the Syntax-Model Consistency for Neural Relation Extraction"},{"paperId":"7c41e58832f3af5fd9e09674924d6b5f822e8eac","title":"Exploring Unexplored Generalization Challenges for Cross-Database Semantic Parsing"},{"paperId":"b044395ed89de33b43d304c008d3c5a7727d423d","title":"SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection"},{"paperId":"cb693ce346f44e6b89ee814c6bb9f0e5cc2fa9d2","title":"Selective Question Answering under Domain Shift"},{"paperId":"fd90e543e0ae7ba01d63132d443ea5fdb131c81f","title":"Translating Natural Language Instructions for Behavioral Robot Navigation with a Multi-Head Attention Mechanism"},{"paperId":"01f85e44c566594b48a9757af12263097ad56f34","title":"LRG at SemEval-2020 Task 7: Assessing the Ability of BERT and Derivative Models to Perform Short-Edits Based Humor Grading"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"97d3c0a402097a8c03ed66bd0e73ac4d4ac70b31","title":"Towards Open Domain Event Trigger Identification using Adversarial Domain Adaptation"},{"paperId":"45a8e28cc7bc7e52c275315a534aeff7e535e822","title":"Text-to-Text Pre-Training for Data-to-Text Tasks"},{"paperId":"02d3b7c6e887391a38dd4b2839f998fa02ee324a","title":"Inflecting When There’s No Majority: Limitations of Encoder-Decoder Neural Networks as Cognitive Models for German Plurals"},{"paperId":"18629f4edead8bcf07e5ef694914ba83dd1e9666","title":"Entity-Enriched Neural Models for Clinical Question Answering"},{"paperId":"7e0000b7e15c117f48eeeeff76973a7f75c3f196","title":"Schema-Guided Natural Language Generation"},{"paperId":"4dc005ea288c50d57222122903edf87f21689781","title":"Probing Linguistic Systematicity"},{"paperId":"33ec7eb2168e37e3007d1059aa96b9a63254b4da","title":"Beyond Accuracy: Behavioral Testing of NLP Models with CheckList"},{"paperId":"427973cbf535187c95cd174adce64c20292a0c78","title":"A Systematic Assessment of Syntactic Generalization in Neural Language Models"},{"paperId":"d0cda85c030711aaa5383c80d5928a4d22f8d3bf","title":"How Can We Accelerate Progress Towards Human-like Linguistic Generalization?"},{"paperId":"3cc2f69951cd24fe61be4cf32d62afbac297bc2b","title":"Social Biases in NLP Models as Barriers for Persons with Disabilities"},{"paperId":"7eebe26af39dfd513a78be3483b9506dcf321606","title":"Teaching Machine Comprehension with Compositional Explanations"},{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"645bc7a5347a299a1e8aa965867bd097f6f4bddd","title":"RMM: A Recursive Mental Model for Dialog Navigation"},{"paperId":"60be3eb53539c56bc336bb9e978f72dff4754289","title":"Analyzing ELMo and DistilBERT on Socio-political News Classification"},{"paperId":"1717d77a222db35e67092691f7a7a67cbc1f58d1","title":"A Multi-Platform Arabic News Comment Dataset for Offensive Language Detection"},{"paperId":"b929fc4ea07199d8e876f5c897aa1f47f9578e1f","title":"AccentDB: A Database of Non-Native English Accents to Assist Neural Speech Recognition"},{"paperId":"ec51c9be66fef4321e45a4904c7368287ec0321c","title":"Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset"},{"paperId":"f527bcef68aeda601aae314fe5c75185c716e579","title":"KLEJ: Comprehensive Benchmark for Polish Language Understanding"},{"paperId":"d97e7561fa7710213ccd4f8128044ea6849be377","title":"XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning"},{"paperId":"0f86a865e52dd6c41b3fec4b87f3794fff1f224c","title":"Self-Supervised Knowledge Triplet Learning for Zero-shot Question Answering"},{"paperId":"601b438b4a7129641e314b4ad731834e81d7dd30","title":"Why Overfitting Isn’t Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries"},{"paperId":"0ebb1d1fbf488fba8c18a5a6057a6ccd9e87510f","title":"Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models"},{"paperId":"c30b457fdfb0623b87379de79ffaa570a7f3bb48","title":"Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation"},{"paperId":"25316018268ed68c7096fe8b2d8fbd66f998a201","title":"Representations of Syntax [MASK] Useful: Effects of Constituency and Dependency Structure in Recursive LSTMs"},{"paperId":"24e4d3370dc366d6b353d1d6818a0df266bb31b9","title":"MLSUM: The Multilingual Summarization Corpus"},{"paperId":"1197a3d963b8016ff5a9c975d361bd28edf2ad16","title":"Universal Dependencies according to BERT: both more specific and more general"},{"paperId":"04b515b69d32eec58272d7dd77e2dfb4e6297d6f","title":"General Purpose Text Embeddings from Pre-trained Language Models for Scalable Inference"},{"paperId":"84059eba69c02bd57b6b227710ba62168ade827e","title":"Syntactic Data Augmentation Increases Robustness to Inference Heuristics"},{"paperId":"7600bc8922c225b299658417f83d54e450a28642","title":"Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation"},{"paperId":"2171eb88c80fd7344890c3b394eedcdf3e5ca58d","title":"Generative Data Augmentation for Commonsense Reasoning"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"b805693c17961af2cc7f859c1a54320b26036f46","title":"Universal Dependencies v2: An Evergrowing Multilingual Treebank Collection"},{"paperId":"32c5332cd586d5f401981114251d58902ea88955","title":"Avoiding the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training"},{"paperId":"360956507ca1169dde233dd09b26e409c7c5731e","title":"TriggerNER: Learning with Entity Triggers as Explanations for Named Entity Recognition"},{"paperId":"97f08c1ae8ca5ddf5948c66bfbbc0546ac154807","title":"Pretrained Transformers Improve Out-of-Distribution Robustness"},{"paperId":"6121617a944c9b273466c55721115bd847cfbfd8","title":"Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension"},{"paperId":"6f66e5260574727cfb46c097bfcaf1763e488bf0","title":"From Machine Reading Comprehension to Dialogue State Tracking: Bridging the Gap"},{"paperId":"5ca5c40661eeec6dda1d80d65dca1b3d8a5f4132","title":"Designing Precise and Robust Dialogue Response Evaluators"},{"paperId":"15e0a71bd655d7c6c7a1b1a9e9eb7a4f650531cd","title":"More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction"},{"paperId":"35e6783307f82d1faa39be0653431305abec7271","title":"Evaluating Models’ Local Decision Boundaries via Contrast Sets"},{"paperId":"00696ba295d66f049d70272219f7fea4266171be","title":"Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space"},{"paperId":"297ad41c0e7264e67ae078921e2a57436293ce72","title":"XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation"},{"paperId":"387b5988331f8fe779c323f8a88df23daa715a8a","title":"Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?"},{"paperId":"9c49cdf0ac4665b320262156eb19bf2e39cb1bb4","title":"End-to-End Slot Alignment and Recognition for Cross-Lingual NLU"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"937fef6a786c4463a3bb19770c704945d1600b66","title":"Learning Compositional Rules via Neural Program Synthesis"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"080872cc51dffa3b72d3870f968fc296d33922a7","title":"Few-shot Natural Language Generation for Task-Oriented Dialog"},{"paperId":"3fafc51835f7247220ab8b29c19785133ccc7220","title":"Undersensitivity in Neural Reading Comprehension"},{"paperId":"0119a57cf88ef16e6dc291252fae340bb6b3953c","title":"CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning"},{"paperId":"693cce5d9764f9e9e0c9c583bf840ac019e2179f","title":"Beat the AI: Investigating Adversarial Human Annotation for Reading Comprehension"},{"paperId":"b0ea633e0c22fbd8cbc531c7326376725d16ce25","title":"Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks"},{"paperId":"5e0cffc51e8b64a8f11326f955fa4b4f1803e3be","title":"oLMpics-On What Language Model Pre-training Captures"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"80e797968a59e1281be95ddb02ba53d653880c90","title":"Reading the Manual: Event Extraction as Definition Comprehension"},{"paperId":"b56e2e7b93be127c953b6ad18230d5905051d23b","title":"BLiMP: A Benchmark of Linguistic Minimal Pairs for English"},{"paperId":"81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85","title":"How Can We Know What Language Models Know?"},{"paperId":"8ed9254d93f540fdc922718fa4df47108f6f9df2","title":"Rethinking Self-Attention: Towards Interpretability in Neural Parsing"},{"paperId":"0c5bc409e62e65f86838968a2a7cdae5fa0b288b","title":"RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers"},{"paperId":"d715b4a9282562b9d84fb66e04ee70e66b12e86d","title":"Location Attention for Extrapolation to Longer Sequences"},{"paperId":"6b2f425da3c8756ee777987b487526077b50a90e","title":"Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks"},{"paperId":"ab70853cd5912c470f6ff95e95481980f0a2a41b","title":"SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization"},{"paperId":"48689c4bb52a45c0bc97d1421d72d11bab6c346b","title":"BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"1d9adfeca5715ec82e2c1aa149a861af93d2f504","title":"Localization of Fake News Detection via Multitask Transfer Learning"},{"paperId":"2e347a977f14eca7cc5bbbb4c71145b75637340c","title":"MLQA: Evaluating Cross-lingual Extractive Question Answering"},{"paperId":"47f1eb0dc42189ba7cf21b76598c8217eb1b6e05","title":"Learning the Difference that Makes a Difference with Counterfactually-Augmented Data"},{"paperId":"72a1d0256b38dea6c3e7d10a63eacc51abdc96da","title":"End-to-End Bias Mitigation by Modelling Biases in Corpora"},{"paperId":"3161e2b6787d304c29dddb7d5fc188ca41be7bda","title":"LAMOL: LAnguage MOdeling for Lifelong Language Learning"},{"paperId":"815a3d56401483b635cfad9468852cddb46350ee","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":"401dc39c2c8c910253d47980cfa3b4d2f7790d9b","title":"WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale"},{"paperId":"4095018e41da90f623af8be7c6f56f597b9cc136","title":"Few-Shot NLG with Pre-Trained Language Model"},{"paperId":"bc5249c2040d187e9dce01c76aa4687fb13d1ce7","title":"Supervised Adaptation of Sequence-to-Sequence Speech Recognition Systems using Batch-Weighting"},{"paperId":"435e7cf5d80f016bf7a6bbb287e8a09575fe22b4","title":"Cloze Distillation: Improving Neural Language Models with Human Next-Word Prediction"},{"paperId":"4b9fcefb1e4ea895a967ce4fdd8035c25904e0f5","title":"How does Punctuation Affect Neural Models in Natural Language Inference"},{"paperId":"732d4c0a2744e731aeacd669b8ea42b70f57a220","title":"Multi-Action Dialog Policy Learning with Interactive Human Teaching"},{"paperId":null,"title":"Hierarchy and interpretability in neural models of language processing"},{"paperId":"3dc34991b758deb0b30dce6653027daa77e0a91b","title":"Cost-Sensitive BERT for Generalisable Sentence Classification on Imbalanced Data"},{"paperId":"72e0d7e374937374897cab79306079f6f436a9d4","title":"On Compositionality in Neural Machine Translation"},{"paperId":"309ae92c32670ac9b3fa6136b2de23ebb5ad8eda","title":"Generic and Specialized Word Embeddings for Multi-Domain Machine Translation"},{"paperId":"4de8867295789d50ae411d44a982f5b0b04eb023","title":"Robustness to Capitalization Errors in Named Entity Recognition"},{"paperId":"db95ba221305a3aa2ec8bbc78de0f42485c63c12","title":"Generalizing Question Answering System with Pre-trained Language Model Fine-tuning"},{"paperId":"853f3f5b556c7268ab088f8f51d6d90a737221f7","title":"CLER: Cross-task Learning with Expert Representation to Generalize Reading and Understanding"},{"paperId":"b7cc89edcc6bc6d29e55cc014d001a08f758fc37","title":"Improving the Robustness of Deep Reading Comprehension Models by Leveraging Syntax Prior"},{"paperId":"cf117373e5d9d0eba1f41057c1321292c69dad02","title":"Graph Enhanced Cross-Domain Text-to-SQL Generation"},{"paperId":"ac3b99729a511d83ebb155361ed13c56ac0c6309","title":"Self-Adaptive Scaling for Learnable Residual Structure"},{"paperId":"dfb443f6ea3a7d388b0b93f6d44563526c06bcba","title":"Posing Fair Generalization Tasks for Natural Language Inference"},{"paperId":"39f81aef964e076d7a089f597e8028b43b5675cf","title":"The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection"},{"paperId":"1be21e96eaac56f626e7b41c1f332b6b46131608","title":"MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension"},{"paperId":"fe5ce0d118ec60a886600331927fc7fbc0379b09","title":"Domain-agnostic Question-Answering with Adversarial Training"},{"paperId":"4c9a8caf940627126aaa9bd3ac813d07065c86a0","title":"Diversify Your Datasets: Analyzing Generalization via Controlled Variance in Adversarial Datasets"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"aa2ba55f349ee5eff02497057c53eae29be67a74","title":"Book QA: Stories of Challenges and Opportunities"},{"paperId":"e29e43d9c0772d44cff53044484970599db30d5f","title":"Domain Differential Adaptation for Neural Machine Translation"},{"paperId":"808b7bf6d9245e46865cd2d172d7a7128fc7a1f3","title":"Global Voices: Crossing Borders in Automatic News Summarization"},{"paperId":"91b5d03050e416df04872361d8d9f35ad7091246","title":"Generalization in Generation: A closer look at Exposure Bias"},{"paperId":"1c8863f1ce6e78bc83b375c1b1e091bec8a91fbc","title":"Look-up and Adapt: A One-shot Semantic Parser"},{"paperId":"46b968d82c4709e74419828b2767a9218a3ebcc8","title":"Learning Invariant Representations of Social Media Users"},{"paperId":"f206899d0064626f8c939935fd209cb51c3a3843","title":"Robust Semantic Parsing with Adversarial Learning for Domain Generalization"},{"paperId":"6b4d97881c279a7b7fee2a9860c255031cb2d7b2","title":"Multilingual Language Models for Named Entity Recognition in German and English"},{"paperId":"85ea337f6c0ba48bd238287068548fa9f83a8429","title":"Finding Generalizable Evidence by Learning to Convince Q&A Models"},{"paperId":"a6af5e3766e510fb67fc4632b34bb9ef702ebdb1","title":"Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs"},{"paperId":"38c1f516c3f8a4c3df6eacee7fc3702599b542cd","title":"Don’t Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction"},{"paperId":"3cd331c997e90f737810aad6fcce4d993315189f","title":"Investigating BERT’s Knowledge of Language: Five Analysis Methods with NPIs"},{"paperId":"5019dbe8d1da5f128f4f373d6849095cf18fd519","title":"The Woman Worked as a Babysitter: On Biases in Language Generation"},{"paperId":"6b5cb3b85fb247256b264c2732916cf129015a92","title":"Learning Dense Representations for Entity Retrieval"},{"paperId":"de073a0b29ccce85fb6026363c8f53b21ae601c4","title":"Topics to Avoid: Demoting Latent Confounds in Text Classification"},{"paperId":"e0f41a30fe692c76e9a27396b9494f2e017dd333","title":"Discourse-Aware Semantic Self-Attention for Narrative Reading Comprehension"},{"paperId":"33fbaf34fa0119e5249dff3267795e13fa0eaa37","title":"Neural data-to-text generation: A comparison between pipeline and end-to-end architectures"},{"paperId":"0abcbdf40f872e6baf1c082811d4ae93df787698","title":"Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets"},{"paperId":"635cb6fb865e86c108c5d1d895aeac0e759eb199","title":"MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance"},{"paperId":"7e870b2feda462bc4d11ddbb78fab911d02d61dc","title":"Out-of-Domain Detection for Low-Resource Text Classification Tasks"},{"paperId":"04a7021fe6be6bddcfae476493fcc7571e7c613c","title":"PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification"},{"paperId":"f4a425499128d53d97a86f2cd625990653101356","title":"Improved Generalization of Arabic Text Classifiers"},{"paperId":"f6d4392a5df9a7265d7939c6ac003946c199086a","title":"An Empirical Study on Pre-trained Embeddings and Language Models for Bot Detection"},{"paperId":"91663fbb9257b1ee5d22d7b9e796e03f374a7a6a","title":"hULMonA: The Universal Language Model in Arabic"},{"paperId":"1d73839a843b70b0a59c566d8faa9e9a3c01a44e","title":"Learning Multilingual Meta-Embeddings for Code-Switching Named Entity Recognition"},{"paperId":"5e35895fc4731858f0b286cb5a1613a819cc2367","title":"CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"a2aa642db090b3aa28a44ccbc3c51fdb0be8335b","title":"Cross-Domain Generalization of Neural Constituency Parsers"},{"paperId":"9f2d1fccd76d5b3a438e8e67cb717ec8b40a9e9e","title":"Knowledge-aware Pronoun Coreference Resolution"},{"paperId":"451788cf6329f3258676118bc188c5bd69803d80","title":"Multi-Task Learning for Coherence Modeling"},{"paperId":"56ed530013dfc77bef2564d2b8f72a92e474cfb3","title":"Do You Know That Florence Is Packed with Visitors? Evaluating State-of-the-art Models of Speaker Commitment"},{"paperId":"435799b6bb553b535f62d1fb796960a824cfc5cf","title":"Reinforced Training Data Selection for Domain Adaptation"},{"paperId":"09e1277e555ef87d53648be72454e2e6bc6150c8","title":"Multilingual and Cross-Lingual Graded Lexical Entailment"},{"paperId":"f95f6392dd5aac975574ffd5ace4376927c79831","title":"XQA: A Cross-lingual Open-domain Question Answering Dataset"},{"paperId":"3bb3a018bf8c0defd7eec3d98001aa9103fcb24a","title":"HEIDL: Learning Linguistic Expressions with Deep Learning and Human-in-the-Loop"},{"paperId":"94befec2a6d96e3a60fb8b77f2e161666743c1a5","title":"We Need to Talk about Standard Splits"},{"paperId":"e3ca15ccbec5ee2309031d8d90cec4d87ae11327","title":"Dual Adversarial Neural Transfer for Low-Resource Named Entity Recognition"},{"paperId":"2bd840084332c78af7c19363197db50a61074c12","title":"Zero-shot Word Sense Disambiguation using Sense Definition Embeddings"},{"paperId":"ed6793d15cf94945adec0dad8bcaf9290ecbae13","title":"Neural Temporality Adaptation for Document Classification: Diachronic Word Embeddings and Domain Adaptation Models"},{"paperId":"45fe966219595e3a6d771c15f273efa171a9f53a","title":"Zero-Shot Entity Linking by Reading Entity Descriptions"},{"paperId":"61daa30becda503c55217ef19a618d6a26b1dabd","title":"Can Neural Networks Understand Monotonicity Reasoning?"},{"paperId":"ed8ff2bef83103f3229e80e92d89d02d94e02f61","title":"Learning to Predict Novel Noun-Noun Compounds"},{"paperId":"d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","title":"Energy and Policy Considerations for Deep Learning in NLP"},{"paperId":"b39efed2e73357db4691f66935cf62e7b51f30e1","title":"Transcoding Compositionally: Using Attention to Find More Generalizable Solutions"},{"paperId":"809cc93921e4698bde891475254ad6dfba33d03b","title":"How Multilingual is Multilingual BERT?"},{"paperId":"74c512bfb67d9cda71836bbd853bb68d4a82793e","title":"Are we there yet? Encoder-decoder neural networks as cognitive models of English past tense inflection"},{"paperId":"fc5d79301a0876201c95954a764ec374b8eb236e","title":"Domain Adaptation of Neural Machine Translation by Lexicon Induction"},{"paperId":"e1619756f071559e020c8c95cb4f149f8b0c7172","title":"Improving Generalization in Coreference Resolution via Adversarial Training"},{"paperId":"f79af5e9e96f9c777e8759d791e33e9da83ffa65","title":"SParC: Cross-Domain Semantic Parsing in Context"},{"paperId":"82dac30bc25eb1470e07ff5bd1ec000f28f4c6d8","title":"Siamese recurrent networks learn first-order logic reasoning and exhibit zero-shot compositional generalization"},{"paperId":"636904d91d9dd1a641a595d9578ba7640f35aa74","title":"MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension"},{"paperId":"66a3ec99a9eef1d14c54b3e39b83a94a523a428a","title":"A Cross-Domain Transferable Neural Coherence Model"},{"paperId":"4888102774ad93140391f3a26af0f54cfba5ec34","title":"Human vs. Muppet: A Conservative Estimate of Human Performance on the GLUE Benchmark"},{"paperId":"2621323502fc779c79bca7ba112bc4d0c1db1d3f","title":"CNNs found to jump around more skillfully than RNNs: Compositional Generalization in Seq2seq Convolutional Networks"},{"paperId":"50170a78f22e03b08e62a20e6cf0e36133a8bdad","title":"A logical-based corpus for cross-lingual evaluation"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"86a1e98860b2554c7d826d96efd022c42ed28295","title":"GumDrop at the DISRPT2019 Shared Task: A Model Stacking Approach to Discourse Unit Segmentation and Connective Detection"},{"paperId":"2fa3f7ce620a1c7155daef6620dd6bb0e01934f3","title":"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT"},{"paperId":"09ba232886fb2aaca21b16fd9b684c6ce29de1b6","title":"Something’s Brewing! Early Prediction of Controversy-causing Posts from Discussion Features"},{"paperId":"6d78d0ec6e0887a4124b258fd59f305c83800d29","title":"Learning to Decipher Hate Symbols"},{"paperId":"f26f17ec49f2593bcc926051394871480a80c0c2","title":"Density Matching for Bilingual Word Embedding"},{"paperId":"85bcccd887a8f5b164745e1c0f0ae4e4dc88cd80","title":"Consistency by Agreement in Zero-Shot Neural Machine Translation"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"fc09d6486be1c9bbfbef4165ce3c1ab664e5d084","title":"PAWS: Paraphrase Adversaries from Word Scrambling"},{"paperId":"7c5f943ce2e9c12bf1423424732b06dcf95f3e8a","title":"A Probabilistic Generative Model of Linguistic Typology"},{"paperId":"cca6f4177921be0dd0ea2794d9d788f265b44da0","title":"The emergence of number and syntax units in LSTM language models"},{"paperId":"b611a8095630557229dc5fb6b07c272f1cd614da","title":"Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification"},{"paperId":"28a0ac83233d123b38917706e4ea4e34eee43482","title":"Structural Supervision Improves Learning of Non-Local Grammatical Dependencies"},{"paperId":"88bd75ce3ce22ed85bf9271877aa85da7b7bb312","title":"Massively Multilingual Neural Machine Translation"},{"paperId":"a73051e08af289a50ef8ed53e69f91c189dd01e5","title":"Induction Networks for Few-Shot Text Classification"},{"paperId":"42ed4a9994e6121a9f325f5b901c5b3d7ce104f5","title":"Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"},{"paperId":"e99e2bd4812b30e104db0feddb681f32acd88758","title":"Massively Multilingual Transfer for NER"},{"paperId":"43676a155b857b747e7489329517c0ea9dfe7286","title":"Adversarial Training for Satire Detection: Controlling for Confounding Variables"},{"paperId":"b0e2fe0fe9f4fc4ce05d5f637baff96a7e966c01","title":"Cooperative Learning of Disjoint Syntax and Semantics"},{"paperId":"19281b9ecdb5c07a93423a506627ab9d9b0cf039","title":"Learning and Evaluating General Linguistic Intelligence"},{"paperId":"07e10c90a193f1b2959e8bf00702317b336ed67e","title":"Learning Relational Representations by Analogy using Hierarchical Siamese Networks"},{"paperId":"96b4f3633d9544593aa6c50949e345d4016c8b48","title":"DIAG-NRE: A Neural Pattern Diagnosis Framework for Distantly Supervised Neural Relation Extraction"},{"paperId":"86f00f3619626bf3aa9664b17bcaebc18a4b6531","title":"Testing the Generalization Power of Neural Network Models across NLI Benchmarks"},{"paperId":"5f6a87289ef0977073e49aa4460f6018de89e14c","title":"pair2vec: Compositional Word-Pair Embeddings for Cross-Sentence Inference"},{"paperId":"a925f818f787e142c5f6bcb7bbd7ede2deb34860","title":"WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations"},{"paperId":"357cc9bfc7b41702ffc22ebe1662d66afd661d6a","title":"GILE: A Generalized Input-Label Embedding for Text Classification"},{"paperId":"522aa8b08fb1793dfaa91fdbc597196cd8108598","title":"Instance-based Inductive Deep Transfer Learning by Cross-Dataset Querying with Locality Sensitive Hashing"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9b80479d57d47d6354adb713dbca20e2e27723aa","title":"Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Neural network acceptability judgments. TACL"},{"paperId":"8456a5ed15b465e82bba3b974ff4e25c3b652826","title":"Zero-Shot Open Entity Typing as Type-Compatible Grounding"},{"paperId":"44fc8d79fb8e0f8c6c6f680179b5803a789c6227","title":"Measuring and Mitigating Unintended Bias in Text Classification"},{"paperId":"b47381e04739ea3f392ba6c8faaf64105493c196","title":"Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"},{"paperId":"0ca1b123151492ed3384b8201a9dc92424de0650","title":"Using context to identify the language of face-saving"},{"paperId":"3833217cdee7ee3796e2732001313d7f41cf2132","title":"Sequence-to-Sequence Models for Data-to-Text Natural Language Generation: Word- vs. Character-based Processing and Output Diversity"},{"paperId":"083b95dd57e52e832b402df66ae09bc1b2b0d518","title":"Cross-Domain Detection of Abusive Language Online"},{"paperId":"51d8bb97e17bc2408e4bec504c7f20a579fba4f5","title":"Sentiment analysis under temporal shift"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"33ecb49e7b1eb1f44790fb6ceca6eed82cb0c7cd","title":"Jump to better conclusions: SCAN both left and right"},{"paperId":"d2131a523deb983c06326717203bb6b185f63527","title":"Can Neural Generators for Dialogue Learn Sentence Planning and Discourse Structuring?"},{"paperId":"b8ff343d528f57bbdd1396b500959165b9af527c","title":"Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification"},{"paperId":"ce89ee7aaeeea2c9d474707690f3ea9d948776a3","title":"MTNT: A Testbed for Machine Translation of Noisy Text"},{"paperId":"6be08083902942f9f142920fdbabe747996b304d","title":"Do Language Models Understand Anything? On the Ability of LSTMs to Understand Negative Polarity Items"},{"paperId":"a2ce385fc8d5068e8c87ebe4699c8f9b295cad5e","title":"Adapting Word Embeddings to New Languages with Morphological and Phonological Subword Representations"},{"paperId":"192a943ad1895f0d77af7931135e5292f18a6649","title":"Reducing Gender Bias in Abusive Language Detection"},{"paperId":"af5c4b80fbf847f69a202ba5a780a3dd18c1a027","title":"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"},{"paperId":"f2a9d16e852e6008b11244df899672231efb7a12","title":"Aggression Identification Using Deep Learning and Data Augmentation"},{"paperId":"7fedd981f2769bd009f749a3dff7044d8378c9b4","title":"Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information"},{"paperId":"e0c66240239263f16159eef166a391d3939ae2d5","title":"How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks"},{"paperId":"210feb22ff541920caa4884e73eaff1c09644114","title":"Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks"},{"paperId":"862e55cfe2627fad81adc8c015aa7c824b89d0cf","title":"A Helping Hand: Transfer Learning for Deep Sentiment Analysis"},{"paperId":"df6dadfd989b5623c928cad17809b1ff3fff081e","title":"Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate"},{"paperId":"a93a3799ba977aa2393cad8cd260d6f778a495e0","title":"Character-level Supervision for Low-resource POS Tagging"},{"paperId":"312eb723aa76c979ebac68e5ded726eb8a7d200c","title":"RECIPE: Applying Open Domain Question Answering to Privacy Policies"},{"paperId":"6d0c0649d51f0245cef1de9d5f27012a1a27d958","title":"diaNED: Time-Aware Named Entity Disambiguation for Diachronic Corpora"},{"paperId":"ce40a8dfb72c69c5c61d8c161784551f670c10fc","title":"Time-evolving Text Classification with Deep Neural Networks"},{"paperId":"4f9f3699c0972bd93a2b81dd2d690f61a4c1495c","title":"Examining Temporality in Document Classification"},{"paperId":"9784fbf77295860b2e412137b86356d70b25e3c0","title":"The Natural Language Decathlon: Multitask Learning as Question Answering"},{"paperId":"3ce9c44c6505e133bb5656cad566d326e43a76ff","title":"A Retrospective Analysis of the Fake News Challenge Stance-Detection Task"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"48fa98589c719f4afa9239c2fc9b358afff111e4","title":"Cross-lingual complex word identification with multitask learning"},{"paperId":"ddc08ac13fc77c156a95473a117d852a56a42f5a","title":"Estimating Linguistic Complexity for Science Texts"},{"paperId":"39af58c32e76e875d667804707ea110323207988","title":"A Survey of Domain Adaptation for Neural Machine Translation"},{"paperId":"d323d011a3214116a18d623501bca9a31d33cf4c","title":"Are All Languages Equally Hard to Language-Model?"},{"paperId":"41b3180745068934bd9f7f2fbc2efc00c64d534b","title":"Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting"},{"paperId":"69c4e3bbc2970a42504a55df824836032c86233d","title":"Leveraging Orthographic Similarity for Multilingual Neural Transliteration"},{"paperId":"587b5f701ebcd91a0ee903541bd34a112c2f4362","title":"Cross-Target Stance Classification with Self-Attention Networks"},{"paperId":"3395d5766969ed64fcadd633759019ddd63232fb","title":"Behavior Analysis of NLI Models: Uncovering the Influence of Three Factors on Robustness"},{"paperId":"8433807f872f8edc697e79a03db1f8882fe802c6","title":"Towards Inference-Oriented Reading Comprehension: ParallelQA"},{"paperId":"c44582c96790e397fc2014a19daf88636d9358c2","title":"Paraphrase to Explicate: Revealing Implicit Noun-Compound Relations"},{"paperId":"413a03a146e6f7b16c11e73243d83e6f1a6627a3","title":"Breaking NLI Systems with Sentences that Require Simple Lexical Inferences"},{"paperId":"15be357a094d22dd7895961cc2fab83f577a9602","title":"The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models"},{"paperId":"8c6427cc1f4e1bbe5d6da34a4511842361f4fbb6","title":"Hypothesis Only Baselines in Natural Language Inference"},{"paperId":"5f8c6e540710d0f54d0fbf8575d103c60c932409","title":"What’s in a Domain? Learning Domain-Robust Text Representations using Adversarial Training"},{"paperId":"b57b69a9b5dcec38532c0869c345b294f2795959","title":"Zero-Shot Dialog Generation with Cross-Domain Latent Actions"},{"paperId":"7ada8577807aefcad4f8120e8a031cceba065ec9","title":"Multitask Parsing Across Semantic Representations"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"ca008695bc1163d36270ca20c466930f7d86854b","title":"Evaluating Historical Text Normalization Systems: How Well Do They Generalize?"},{"paperId":"0d406275af277ea94dd10f024bf76ca5c513906e","title":"End-Task Oriented Textual Entailment via Deep Explorations of Inter-Sentence Interactions"},{"paperId":"c7a46543829b59a9423a067ca798c63b872eb613","title":"Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing"},{"paperId":"fb46d79829d19e9e87fc1bed7f2f846b73f9a43c","title":"Massively Parallel Cross-Lingual Learning in Low-Resource Target Language Translation"},{"paperId":"3d42ddf7c5ce59ae04d1d27085be9f736d1be04b","title":"Colorless Green Recurrent Networks Dream Hierarchically"},{"paperId":"2997b26ffb8c291ce478bd8a6e47979d5a55c466","title":"Annotation Artifacts in Natural Language Inference Data"},{"paperId":"08fbb1b4cfdc83977d2c8f08bdfb663f13c0e60a","title":"Memorize or generalize? Searching for a compositional RNN in a haystack"},{"paperId":"d25e9351d3d1be7709f0d2fdf19991c0882554ce","title":"Neural Compositional Denotational Semantics for Question Answering"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"772b3d86d63e2b738534fae9a6650159f831364f","title":"Investigating the Working of Text Classifiers"},{"paperId":"1e077413b25c4d34945cc2707e17e46ed4fe784a","title":"Universal Language Model Fine-tuning for Text Classification"},{"paperId":"5e2bb96c47ccaa16a4e7192e8fadb3b3e1c3acdc","title":"Deep Learning: A Critical Appraisal"},{"paperId":"83cc0d20275fdc3a97cceacdc41fbb19953fa901","title":"Mapping to Declarative Knowledge for Word Problem Solving"},{"paperId":"97856a4c31fec7b189446a130aab4cbfa8d6a3e8","title":"Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"66814fe66801d797a515eaf042bfa90ee44896b4","title":"Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers"},{"paperId":"5ded2b8c64491b4a67f6d39ce473d4b9347a672e","title":"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"},{"paperId":"cfff3c785674d0455abe1a860dcf3b9211abe64d","title":"Out-of-domain Detection based on Generative Adversarial Network"},{"paperId":"67c851762f0c4c12ae38917d84c2ba9a68a2f9ca","title":"Bridging the Gaps: Multi Task Learning for Domain Transfer of Hate Speech Detection"},{"paperId":"38e24643800118f7a5208f81404e84777ba0fc61","title":"Seq2Seq Models with Dropout can Learn Generalizable Reduplication"},{"paperId":"c522fb56871a23acc4bbebafba7340d116bcb3b8","title":"A Neural Morphological Analyzer for Arapaho Verbs Learned from a Finite State Transducer"},{"paperId":"97d798fb89b46ccfdc6e7e9add827f95afa2b53b","title":"Unit Segmentation of Argumentative Texts"},{"paperId":"c80725ad0c0cd06416f3c01a78b7c419359d3fe2","title":"Instance Weighting for Neural Machine Translation Domain Adaptation"},{"paperId":"088bf4be2e8e3d723685dd51dbb1597e56cecacb","title":"A Simpler and More Generalizable Story Detector using Verb and Character Features"},{"paperId":"53f18d3bee0abdab40e3264862b3febbcd2cfe64","title":"A Dataset and Classifier for Recognizing Social Media English"},{"paperId":"db53e79b0ab8375e33c172867cb0e2d5eeeef368","title":"Abstractive morphological learning with a recurrent neural network"},{"paperId":"ceb7dddbd0c51f511c4ba97d328b48fd10d2a7fc","title":"Recurrent Neural Network-Based Sentence Encoder with Gated Attention for Natural Language Inference"},{"paperId":"ffb949d3493c3b2f3c9acf9c75cb03938933ddf0","title":"Adversarial Examples for Evaluating Reading Comprehension Systems"},{"paperId":"78e592524e336afdf586c51ac228488a71e31340","title":"Sentence Embedding for Neural Machine Translation Domain Adaptation"},{"paperId":"785bd8ff24188829f8522e2be58574a1df1a7841","title":"Domain Attention with an Ensemble of Experts"},{"paperId":"fa025e5d117929361bcf798437957762eb5bb6d4","title":"Zero-Shot Relation Extraction via Reading Comprehension"},{"paperId":"ab65d7fa065ad8347bf9de1a4bfbf26a2f245594","title":"Natural Language Generation for Spoken Dialogue System using RNN Encoder-Decoder Networks"},{"paperId":"2a9cda48c1a57b738ea54abe96ff7e54bf95579b","title":"Bandit Structured Prediction for Neural Sequence-to-Sequence Learning"},{"paperId":"152cf9f57150b00fd21de55c5c571c2ff0302371","title":"Lexical Features in Coreference Resolution: To be Used With Caution"},{"paperId":"b427919579628a1ff887c38d254184e2b8c23791","title":"Detecting negation scope is easy, except when it isn’t"},{"paperId":"8ccf69a8cb3f040c1b7742ce42bb24b9b2cc7286","title":"Neural sentence embedding using only in-domain sentences for out-of-domain sentence detection in dialog systems"},{"paperId":"0c2a8c7c54f2f3094c4d5c09cf8f6f3c037ee120","title":"Neural Structural Correspondence Learning for Domain Adaptation"},{"paperId":"efbd381493bb9636f489b965a2034d529cd56bcd","title":"Pointer Sentinel Mixture Models"},{"paperId":"0daf48d0ce4e4200a753c28519f5761b160944fb","title":"Multi-task Domain Adaptation for Sequence Tagging"},{"paperId":"e51ad63414138ce5999c192bb1d6f2e410e9463b","title":"Authorship Attribution with Convolutional Neural Networks and POS-Eliding"},{"paperId":"256365e6007b2aa1e4356f9979a1ba83b9041d1e","title":"An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation"},{"paperId":"616253f6b1e83ede361457de2f51b0bf70555b13","title":"Cross-lingual Name Tagging and Linking for 282 Languages"},{"paperId":"502c5bee958a4ad5d97658d2c2d6697f22dd7c23","title":"Supervised and unsupervised approaches to measuring usage similarity"},{"paperId":"a1a0c17dba3bb90b9ea3d25cd16eeda49a1c8ece","title":"Fast Domain Adaptation for Neural Machine Translation"},{"paperId":"1863f4c3cb86737e0cc232a62e021548a4b48c05","title":"Broad Twitter Corpus: A Diverse Named Entity Recognition Resource"},{"paperId":"5516b99b30d93e58d628f5af671ca50a07c2effb","title":"Leveraging Multiple Domains for Sentiment Classification"},{"paperId":"3aa52436575cf6768a0a1a476601825f6a62e58f","title":"Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies"},{"paperId":"c809f53348f66ae89cb16a018033476ca79cc966","title":"Modal Sense Classification At Large: Paraphrase-Driven Sense Projection, Semantically Enriched Classification Models and Cross-Genre Evaluations"},{"paperId":"87f714f3534c7a3ca2bf41ce5825139ddc8247bf","title":"What to do about non-standard (or non-canonical) language in NLP"},{"paperId":"7a4f3a0cfc0cc2aafa4ed1a2924380e82d5e3e4c","title":"Demographic Dialectal Variation in Social Media: A Case Study of African-American English"},{"paperId":"85b68477a6e031d88b963833e15a4b4fc6855264","title":"A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories"},{"paperId":"7260c0692f8d265e11c4e9c4c8ef4c185bd587ad","title":"Building machines that learn and think like people"},{"paperId":"bb238163cfcad848dbbd022d03a683069aa713d8","title":"Sarcastic Soulmates: Intimacy and irony markers in social media messaging"},{"paperId":"595c45a6c4fe895e00742f8316710e1177896deb","title":"Diagnostic Classifiers Revealing how Neural Networks Process Hierarchical Structure"},{"paperId":"6f481937c38abbb3b71b2f39ced56dc8894eaa0e","title":"How to Avoid Unwanted Pregnancies: Domain Adaptation using Neural Network Models"},{"paperId":"f04df4e20a18358ea2f689b4c129781628ef7fc1","title":"A large annotated corpus for learning natural language inference"},{"paperId":"04d1a26c2516dc14a765112a63ec60dc3cb3de72","title":"Tree-Structured Composition in Neural Networks without Tree-Structured Architectures"},{"paperId":"2826f9dccdcceb113b33ccf2841d488f1419bb30","title":"Stanford Neural Machine Translation Systems for Spoken Language Domains"},{"paperId":"0df165969045b5adc2c575063eef33b379abc6cb","title":"Crowdsourcing and annotating NER for Twitter #drift"},{"paperId":"c333778104f648c385b4631f7b4a859787e9d3d3","title":"A SICK cure for the evaluation of compositional distributional semantic models"},{"paperId":"ac50b590dcefef497411418fbfca9043b453e808","title":"A unifying view on dataset shift in classification"},{"paperId":"396aabd694da04cdb846cb724ca9f866f345cbd5","title":"Domain Adaptation via Pseudo In-Domain Data Selection"},{"paperId":"857c82948e57fee02ea77b12ad76782cc56fa974","title":"Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP"},{"paperId":"105ed573024e9a31eddc766b6018297ab4383bb9","title":"On Achieving and Evaluating Language-Independence in NLP"},{"paperId":"40cbaf4106ab60586e8ffa7624fc779172cfd490","title":"Domain Adaptation for Statistical Machine Translation with Monolingual Resources"},{"paperId":"fc252ca4c573c4e69758d5544ce3b55d244cd80f","title":"When Training and Test Sets Are Different: Characterizing Learning Transfer"},{"paperId":"57458bc1cffe5caa45a885af986d70f723f406b4","title":"A unified architecture for natural language processing: deep neural networks with multitask learning"},{"paperId":"195df0de3c4c181d26391dd73746c7aefe709ab6","title":"Experiments in Domain Adaptation for Statistical Machine Translation"},{"paperId":"3ccaa9d20e1f16f6c818853a970755ce888df792","title":"Generalisation towards Combinatorial Productivity in Language Acquisition by Simple Recurrent Networks"},{"paperId":"d895647b4a80861703851ef55930a2627fe19492","title":"Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification"},{"paperId":"9f62067945d991cd78a62cf647de17f01d1b54d3","title":"Frustratingly Easy Domain Adaptation"},{"paperId":"6e1b12dd7ca3443ba6e0fb38dfa3587a4eb5d539","title":"Improved Inference for Unlexicalized Parsing"},{"paperId":"730cb37dc464f9b8e226fc34dafa4d932315e1da","title":"Out-of-Domain Utterance Detection Using Classification Confidences of Multiple Topics"},{"paperId":"9fa8d73e572c3ca824a04a5f551b602a17831bc5","title":"Domain Adaptation with Structural Correspondence Learning"},{"paperId":"04df7d50ffc522d752116070a5cea0d3a16405b2","title":"A Maximum Entropy Approach to Adaptive Statistical Language Modeling"},{"paperId":"a6383f155fa9d3e9b15092bfefbf613f982eb263","title":"The Algebraic Mind: Integrating Connectionism and Cognitive Science"},{"paperId":"30bb9c55b77218d404112edab44559bf9bebca7d","title":"Connectionism: with or without rules? Response to J.L. McClelland and D.C. Plaut (1999)"},{"paperId":"787e1077d2f4d8db1692aa3f37ed1cffacd2b07d","title":"Does generalization in infant learning implicate abstract algebra-like rules?"},{"paperId":"08dc7b19e679539f0f93db0192a8e8d11538b3dd","title":"Rethinking Eliminative Connectionism"},{"paperId":"3764baa7465201f054083d02b58fa75f883c4461","title":"A New Statistical Parser Based on Bigram Lexical Dependencies"},{"paperId":"d6689ba53cd7e09cf1c640564fb083f9f2171370","title":"German Inflection: The Exception That Proves the Rule"},{"paperId":"9548ac30c113562a51e603dbbc8e9fa651cfd3ab","title":"Improved backing-off for M-gram language modeling"},{"paperId":"f0a14be7e7f5614b91d0f648ae5f2baafc6d7036","title":"Statistical Decision-Tree Models for Parsing"},{"paperId":"0b44fcbeea9415d400c5f5789d6b892b6f98daff","title":"Building a Large Annotated Corpus of English: The Penn Treebank"},{"paperId":"550e406b84e46cf8c77baa70a61704e93fd963bc","title":"Towards Compositional Learning in Dynamic NetworksTechnical Report"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":"60cbf6ce76f13b575ed3d41edfe8c5ee28fb455b","title":"The Child's Learning of English Morphology"}],"id":"559bfba3bee31f6061a5d5c7061f22794de47e39","summary":"A taxonomy for characterising and understanding generalisation research in NLP is presented, a taxonomy is used to present a comprehensive map of published generalisation studies, and recommendations for which areas might deserve attention in the future are made."},{"url":"https://www.semanticscholar.org/paper/6f0be1f9bda7530b1fa654cac84d595ca9d53740","title":"Revisit Systematic Generalization via Meaningful Learning","venue":"","year":2020,"referenceCount":81,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/14/2020","authors":"Ning Shi,Boxin Wang,Wei Wang,Xiangyu Liu,Zhouhan Lin","citations":[],"references":[{"paperId":"ce975a743822a1bc89ef6bf182388f41866225b5","title":"Can Transformer be Too Compositional? Analysing Idiom Processing in Neural Machine Translation"},{"paperId":"69078af65fc934f81fd340e9d1323d6c08194548","title":"Revisiting the Compositional Generalization Abilities of Neural Sequence Models"},{"paperId":"b3f644a5ea1fdd8cec1c34ebed69125838a50de3","title":"The Paradox of the Compositionality of Natural Language: A Neural Machine Translation Case Study"},{"paperId":"845b4941d8c016aa5f8967da2f86d38ef6c18fa3","title":"A Survey on Knowledge Graphs: Representation, Acquisition, and Applications"},{"paperId":"bb0ab8591d6d57c7e2bd1ec35d806b3f25277752","title":"Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks"},{"paperId":"b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347","title":"Sequence-to-Sequence Learning with Latent Neural Grammars"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"1bed382373aed687c045bb65bc7541b16fc7a6be","title":"Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","title":"Lexicon Learning for Few Shot Sequence Modeling"},{"paperId":"03ad126cfe495933f7bb769f27c03e5f31caedf8","title":"On Compositional Generalization of Neural Machine Translation"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":"e69e5953905b9b9ded4c07f0505ed401ec39babf","title":"Universal Grammar"},{"paperId":"bcf2bc325e4a48b615efb9cad2da2ce3e2ecbec7","title":"Solving SCAN Tasks with Data Augmentation and Input Embeddings"},{"paperId":"1012ee9d7f33dfed330f9dd8030decc0bea17c12","title":"Do Word Embeddings Capture Spelling Variation?"},{"paperId":"179633ce3e47780a940c5716fbaabcca3c71e385","title":"SynET: Synonym Expansion using Transitivity"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"37882abaec01eba1bf5bda8a36c904aaea0d5642","title":"Improving Compositional Generalization in Semantic Parsing"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"5e11ee60242b07d4cdcff339f6c671564314f99a","title":"BiRRE: Learning Bidirectional Residual Relation Embeddings for Supervised Hypernymy Detection"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"2b5d553cb2f298f36aff1a1519f7f2f6be4db5da","title":"STEAM: Self-Supervised Taxonomy Expansion with Mini-Paths"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"937fef6a786c4463a3bb19770c704945d1600b66","title":"Learning Compositional Rules via Neural Program Synthesis"},{"paperId":"828ca45d90cccd09693ef7765469da0b55040a91","title":"Hypernym Detection Using Strict Partial Order Networks"},{"paperId":"815a3d56401483b635cfad9468852cddb46350ee","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":"0feea94f89d395436bf41bd10c797447eecbc128","title":"Unsupervised Data Augmentation for Consistency Training"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"8c6a9e0bc0ab0f26210bf755167c7b542949a0e5","title":"Incorporating Bilingual Dictionaries for Low Resource Semi-Supervised Neural Machine Translation"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"5e35895fc4731858f0b286cb5a1613a819cc2367","title":"CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"4b344351fe43544317efc9adaebe6791c4242814","title":"Pay Less Attention with Lightweight and Dynamic Convolutions"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"33ecb49e7b1eb1f44790fb6ceca6eed82cb0c7cd","title":"Jump to better conclusions: SCAN both left and right"},{"paperId":"210feb22ff541920caa4884e73eaff1c09644114","title":"Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"32ce5467ff884d2f90a233f4d9606c6e18b1a9d6","title":"Learning a Neural Semantic Parser from User Feedback"},{"paperId":"d30f2eca7e1a0b085743a3df1c9a2a0bdacb3dfd","title":"Detecting spelling variants in non-standard texts"},{"paperId":"dc984ea8be018a0244b40468d13f7b734ab55bac","title":"Incorporating Discrete Translation Lexicons into Neural Machine Translation"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"753e30826f1908a62a8d251fc6b1b598f86d2bb2","title":"Shared Tasks of the 2015 Workshop on Noisy User-generated Text: Twitter Lexical Normalization and Named Entity Recognition"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"b54268e3b8d148c0695ca52bebb0f80e26a4b987","title":"The IWSLT 2015 Evaluation Campaign"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"34f25a8704614163c4095b3ee2fc969b60de4698","title":"Dropout: a simple way to prevent neural networks from overfitting"},{"paperId":"81aace0e90c6a962059b117c24db0d856f340f41","title":"Report on the 11th IWSLT evaluation campaign"},{"paperId":"0157dcd6122c20b5afc359a799b2043453471f7f","title":"Exploiting Similarities among Languages for Machine Translation"},{"paperId":"330da625c15427c6e42ccfa3b747fb29e5835bf0","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"84069287da0a6b488b8c933f3cb5be759cb6237e","title":"On the difficulty of training recurrent neural networks"},{"paperId":"15c460439979d1ddf617ad343308359532f316d2","title":"Connectionist semantic systematicity"},{"paperId":"f040887e99010c19a9e1a754860f7f1be3b43828","title":"COGNITIVE SYNONYMY: A GENERAL OVERVIEW"},{"paperId":"06c10fcd7cfce4614f8460298820a65b5b8e1818","title":"Strong systematicity in sentence processing by simple recurrent networks"},{"paperId":"945d60a4ced5b05d2a2fed26922cb4b8d505443b","title":"Rote Versus Meaningful Learning"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"6843890926bf0e5c887ffc78dcb1203135981bf1","title":"The compositionality papers"},{"paperId":"a6383f155fa9d3e9b15092bfefbf613f982eb263","title":"The Algebraic Mind: Integrating Connectionism and Cognitive Science"},{"paperId":null,"title":"How to teach grammar, volume 3"},{"paperId":"08dc7b19e679539f0f93db0192a8e8d11538b3dd","title":"Rethinking Eliminative Connectionism"},{"paperId":null,"title":"Early stopping-but when? In Neural Networks: Tricks of the trade, pages 55–69"},{"paperId":"e23c34414e66118ecd9b08cf0cd4d016f59b0b85","title":"Bidirectional recurrent neural networks"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"b7c0e47f8b768258b7d536c21b218e6c46ab8791","title":"Learning to Parse Database Queries Using Inductive Logic Programming"},{"paperId":"153a9f34c6c76392cbdd808569b084747d570f49","title":"Linguistic Semantics: An Introduction"},{"paperId":"0736a262b557c86e8d14cc7577bb94de5067d65e","title":"Systematicity in Connectionist Language Learning"},{"paperId":"52a33c269aeb8e82de1985dc3745c09e2338debb","title":"A Comparison of Inductive and Deductive Approaches to Teaching Foreign Languages"},{"paperId":"ce9a21b93ba29d4145a8ef6bf401e77f261848de","title":"A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"},{"paperId":"c4b4ce25f645b5743affab62e9b017d0e08d7d9d","title":"Cognitive preference and learning mode as determinants of meaningful learning through concept mapping"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":"83b3943965f4475e46bc763df3a8b1480a01dfba","title":"The Deduction/Induction Controversy"},{"paperId":"7ef3ac14cdb484aaa2b039850093febd5cf73a21","title":"Contextual correlates of synonymy"},{"paperId":"4256e407ad928f5ccba548a933a56c02c402b376","title":"The psychology of meaningful verbal learning."},{"paperId":"988800917eda494963e28b62f6821d0f51000b84","title":"The social motivation of a sound change"},{"paperId":"decd9bc0385612bdf936928206d83730718e737e","title":"Distributional Structure"}],"id":"6f0be1f9bda7530b1fa654cac84d595ca9d53740","summary":"It is suggested that models can successfully one-shot generalize to novel concepts and compositions through semantic linking, either inductively or deductively, and it is demonstrated that prior knowledge plays a key role as well."},{"url":"https://www.semanticscholar.org/paper/9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"C OMPOSITIONAL G ENERALIZATION AND D ECOMPOSITION IN N EURAL P ROGRAM S YNTHESIS","venue":"","year":null,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"","citations":[],"references":[{"paperId":"c347093e2dca530ce347526380b0b7aedf03a6b2","title":"CrossBeam: Learning to Search in Bottom-Up Program Synthesis"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-Level Code Generation with AlphaCode"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"40c25232bc3a3f36ac856ff517d5c70704f14965","title":"TF-Coder: Program Synthesis for Tensor Manipulations"},{"paperId":null,"title":"CS106A: Programming methodologies"},{"paperId":"676fa805bd715591f99bb17e36d673a6a14e92fe","title":"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"946179bd263d46d70422cdff7e6657b97b81230c","title":"Learning to Combine Per-Example Solutions for Neural Program Synthesis"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"88dbde378e9ac5c25fc7d78f5da147223e8d34d4","title":"Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":"afd8fb26a7094ed3a8838000d50e2b22f815dc28","title":"Learning to Synthesize Data for Semantic Parsing"},{"paperId":"b9c3e87bc09c4c6167a03a835c30b1c23bef7a40","title":"Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"19bd467b1c8de94b9bdaef1499788467937f594e","title":"Meta-Learning for Domain Generalization in Semantic Parsing"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"30a156f17ca8f54aa14d01d32c2315c11fcbe723","title":"BUSTLE: Bottom-up program-Synthesis Through Learning-guided Exploration"},{"paperId":"5f818ecbfce3bc44325a4f8ef2d744bc94006d6c","title":"Learning to Execute Programs with Instruction Pointer Attention Graph Neural Networks"},{"paperId":"7cdfc1fefa54eb6e02373120a13ab87290c89338","title":"Just-in-time learning for bottom-up enumerative synthesis"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"37882abaec01eba1bf5bda8a36c904aaea0d5642","title":"Improving Compositional Generalization in Semantic Parsing"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"ef2bbcd928749978b4395460a96c9869833c9c89","title":"DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning"},{"paperId":"e41452747ac0674a7b6534e78be33134fe8ef650","title":"Learning to Represent Programs with Property Signatures"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"80deaca65c2c155bd15718eeecff584841eb25b0","title":"CLOSURE: Assessing Systematic Generalization of CLEVR Models"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"f669b5406320a83e94b4dd439806211c8b1e95fc","title":"Write, Execute, Assess: Program Synthesis with a REPL"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"6c72cff44294e050f75b47f1027889f539e11347","title":"Learning to Infer Program Sketches"},{"paperId":"72cdcdb69e1b5c7dc0e71f480cc820217cde7055","title":"FrAngel: component-based synthesis with control structures"},{"paperId":"6c41bedc4637f3fd504c68baa3b3d8881e056ac1","title":"Execution-Guided Neural Program Synthesis"},{"paperId":"980e4fe84fe5feec42c2a2eea7cc738e1af8acdf","title":"Automatic Program Synthesis of Long Programs with a Learned Garbage Collector"},{"paperId":"62fb54f5736be4a55b2da17010a115cee701d72c","title":"Accelerating search-based program synthesis using learned probabilistic models"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"cf6c1aca36156137c5491737dd7f334f09df7357","title":"The three pillars of machine programming"},{"paperId":"c8725f13be7434b69738491c66b45c9225258253","title":"The Web as a Knowledge-Base for Answering Complex Questions"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","title":"Self-Attention with Relative Position Representations"},{"paperId":"dea6aeb514b1969ab879c793d46a0d2eceaa2cbf","title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":null,"title":"Introduction to program synthesis"},{"paperId":null,"title":"Expression e := s | m | o | ConstStr(c) Compose o := m1(m2) | m(s) Substring s := SubStr(k1, k2) | GetSpan"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":"644ca74f80463415613847ab01cff067fb58f0ad","title":"Neuro-Symbolic Program Synthesis"},{"paperId":null,"title":"Program synthesis. Foundations and Trends® in Programming Languages"},{"paperId":null,"title":"A syntactic neural model for General-Purpose code generation"},{"paperId":null,"title":"Program Synthesis. Foundations and Trends(r) in Programming Languages Series. Now Publishers, 2017a. ISBN 9781680832921"},{"paperId":null,"title":"Program Synthesis. Foundations and Trends(r) in Programming Languages Series"},{"paperId":"05d44eae378940377e4ef3c99158fe9ad7dc59f2","title":"Foundations and Trends R © in Programming Languages"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a","title":"Learning to Execute"},{"paperId":"69c42a8da4c52b90ee27f9b6c0df37f2731ac890","title":"Growing solver-aided languages with rosette"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"1ef301c1b275091b6a50d620b41df4722f2108f0","title":"Combinatorial sketching for finite programs"},{"paperId":null,"title":"Syntactic Structures. De Gruyter Reference Global. Mouton de Gruyter"},{"paperId":null,"title":"Syntactic Structures"},{"paperId":"a6383f155fa9d3e9b15092bfefbf613f982eb263","title":"The Algebraic Mind: Integrating Connectionism and Cognitive Science"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"}],"id":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","summary":"A suite of generalization tasks, which measure different types of compositional generalization that are desirable for program synthesis and are particularly difﬁcult for current sequence to sequence models, are proposed."},{"url":"https://www.semanticscholar.org/paper/6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis","venue":"ArXiv","year":2022,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/07/2022","authors":"Kensen Shi,Joey Hong,M. Zaheer,Pengcheng Yin,Charles Sutton","citations":[],"references":[{"paperId":"c347093e2dca530ce347526380b0b7aedf03a6b2","title":"CrossBeam: Learning to Search in Bottom-Up Program Synthesis"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-Level Code Generation with AlphaCode"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"40c25232bc3a3f36ac856ff517d5c70704f14965","title":"TF-Coder: Program Synthesis for Tensor Manipulations"},{"paperId":null,"title":"CS106A: Programming methodologies"},{"paperId":"676fa805bd715591f99bb17e36d673a6a14e92fe","title":"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"946179bd263d46d70422cdff7e6657b97b81230c","title":"Learning to Combine Per-Example Solutions for Neural Program Synthesis"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"88dbde378e9ac5c25fc7d78f5da147223e8d34d4","title":"Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":"afd8fb26a7094ed3a8838000d50e2b22f815dc28","title":"Learning to Synthesize Data for Semantic Parsing"},{"paperId":"b9c3e87bc09c4c6167a03a835c30b1c23bef7a40","title":"Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"19bd467b1c8de94b9bdaef1499788467937f594e","title":"Meta-Learning for Domain Generalization in Semantic Parsing"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"30a156f17ca8f54aa14d01d32c2315c11fcbe723","title":"BUSTLE: Bottom-up program-Synthesis Through Learning-guided Exploration"},{"paperId":"5f818ecbfce3bc44325a4f8ef2d744bc94006d6c","title":"Learning to Execute Programs with Instruction Pointer Attention Graph Neural Networks"},{"paperId":"7cdfc1fefa54eb6e02373120a13ab87290c89338","title":"Just-in-time learning for bottom-up enumerative synthesis"},{"paperId":"37882abaec01eba1bf5bda8a36c904aaea0d5642","title":"Improving Compositional Generalization in Semantic Parsing"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"ef2bbcd928749978b4395460a96c9869833c9c89","title":"DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning"},{"paperId":"e41452747ac0674a7b6534e78be33134fe8ef650","title":"Learning to Represent Programs with Property Signatures"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"80deaca65c2c155bd15718eeecff584841eb25b0","title":"CLOSURE: Assessing Systematic Generalization of CLEVR Models"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"f669b5406320a83e94b4dd439806211c8b1e95fc","title":"Write, Execute, Assess: Program Synthesis with a REPL"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"6c72cff44294e050f75b47f1027889f539e11347","title":"Learning to Infer Program Sketches"},{"paperId":"72cdcdb69e1b5c7dc0e71f480cc820217cde7055","title":"FrAngel: component-based synthesis with control structures"},{"paperId":"6c41bedc4637f3fd504c68baa3b3d8881e056ac1","title":"Execution-Guided Neural Program Synthesis"},{"paperId":"980e4fe84fe5feec42c2a2eea7cc738e1af8acdf","title":"Automatic Program Synthesis of Long Programs with a Learned Garbage Collector"},{"paperId":"62fb54f5736be4a55b2da17010a115cee701d72c","title":"Accelerating search-based program synthesis using learned probabilistic models"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"cf6c1aca36156137c5491737dd7f334f09df7357","title":"The three pillars of machine programming"},{"paperId":"c8725f13be7434b69738491c66b45c9225258253","title":"The Web as a Knowledge-Base for Answering Complex Questions"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","title":"Self-Attention with Relative Position Representations"},{"paperId":"dea6aeb514b1969ab879c793d46a0d2eceaa2cbf","title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":null,"title":"Introduction to program synthesis"},{"paperId":null,"title":"Expression e := s | m | o | ConstStr(c) Compose o := m1(m2) | m(s) Substring s := SubStr(k1, k2) | GetSpan"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":"644ca74f80463415613847ab01cff067fb58f0ad","title":"Neuro-Symbolic Program Synthesis"},{"paperId":null,"title":"A syntactic neural model for General-Purpose code generation"},{"paperId":null,"title":"Program synthesis. Foundations and Trends® in Programming Languages"},{"paperId":null,"title":"Program Synthesis. Foundations and Trends(r) in Programming Languages Series. Now Publishers, 2017a. ISBN 9781680832921"},{"paperId":null,"title":"Program Synthesis. Foundations and Trends(r) in Programming Languages Series"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a","title":"Learning to Execute"},{"paperId":"69c42a8da4c52b90ee27f9b6c0df37f2731ac890","title":"Growing solver-aided languages with rosette"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"1ef301c1b275091b6a50d620b41df4722f2108f0","title":"Combinatorial sketching for finite programs"},{"paperId":null,"title":"Syntactic Structures. De Gruyter Reference Global. Mouton de Gruyter"},{"paperId":null,"title":"Syntactic Structures"},{"paperId":"a6383f155fa9d3e9b15092bfefbf613f982eb263","title":"The Algebraic Mind: Integrating Connectionism and Cognitive Science"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"}],"id":"6a250b904965732840a75b6a13e35ac15f5cce4d","summary":"A suite of generalization tasks, which measure different types of compositional generalization that are desirable for program synthesis and are particularly difﬁcult for current sequence to sequence models, are proposed."},{"url":"https://www.semanticscholar.org/paper/a143cac1bc440135b612132c89e603f364b8a3b7","title":"Combine to Describe: Evaluating Compositional Generalization in Image Captioning","venue":"ACL","year":2022,"referenceCount":57,"citationCount":1,"influentialCitationCount":0,"publicationDate":null,"authors":"G. Pantazopoulos,Alessandro Suglia,Arash Eshghi","citations":[{"paperId":"fb762e3e3db1e9c06395e04d2452bf27ce499ff7","title":"DALL-E 2 Fails to Reliably Capture Common Syntactic Processes"}],"references":[{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"bb0ab8591d6d57c7e2bd1ec35d806b3f25277752","title":"Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"6be64445935dcdf4053a6e78b623b80a314d9bbc","title":"Separating Skills and Concepts for Novel Visual Question Answering"},{"paperId":"88dbde378e9ac5c25fc7d78f5da147223e8d34d4","title":"Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention"},{"paperId":"7c936b504abbec7b191a876cfcdb545cd21bc47a","title":"Incremental Composition in Distributional Semantics"},{"paperId":"38b0567e83386ddc294d6c81b541deacbd8e3c2a","title":"CLIPScore: A Reference-free Evaluation Metric for Image Captioning"},{"paperId":"10161db52bfa53bdab84ae97b47cef2f22119131","title":"The Role of Syntactic Planning in Compositional Image Captioning"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"0b4d5b7cef06b66182db80803f783d077e3637b6","title":"Improving Image Captioning Evaluation by Considering Inter References Variance"},{"paperId":"fc78991050e355477f9d0ba51a241947e8bc9b9d","title":"CompGuessWhat?!: A Multi-task Evaluation Framework for Grounded Language Learning"},{"paperId":"4bc2bb6584774b0d8ad0b4f5215dc2075487c192","title":"A Benchmark for Systematic Generalization in Grounded Language Understanding"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"d5caec8107da41ec71fc0bb36d60fc2d8834846e","title":"Meshed-Memory Transformer for Image Captioning"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"815a3d56401483b635cfad9468852cddb46350ee","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":null,"title":"2020) we use 3 encoding and decoding layers, 8 attention heads, and 40 memory vectors"},{"paperId":"32c9a0acee8d236c553395052c29a6d853d8ea2d","title":"Compositional Generalization in Image Captioning"},{"paperId":"5e35895fc4731858f0b286cb5a1613a819cc2367","title":"CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"82dac30bc25eb1470e07ff5bd1ec000f28f4c6d8","title":"Siamese recurrent networks learn first-order logic reasoning and exhibit zero-shot compositional generalization"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"0dc092d33f7c71bc9d8d42b53ebb1fad101db4c8","title":"Linguistic generalization and compositionality in modern artificial neural networks"},{"paperId":"6c7494a47cc5421a7b636c244e13586dc2dab007","title":"Systematic Generalization: What Is Required and Can It Be Learned?"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"3bf09b2e2639add154a9fe6ff98cc373d3e90e4e","title":"Neural Baby Talk"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8","title":"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"a3d071d2a5c11329aa324b2cae6b7b6ca7800213","title":"C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset"},{"paperId":"03eb382e04cca8cca743f7799070869954f1402a","title":"CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"},{"paperId":"6c8353697cdbb98dfba4f493875778c4286d3e3a","title":"Self-Critical Sequence Training for Image Captioning"},{"paperId":"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88","title":"Deep Visual-Semantic Alignments for Generating Image Descriptions"},{"paperId":"e4dd95c4341ec7d14317a3d97022773a0822906c","title":"Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models"},{"paperId":"936227f7483938097cc1cdd3032016df54dbd5b6","title":"Learning to generalize to new compositions in image understanding"},{"paperId":"651e5bcc14f14605a879303e97572a27ea8c7956","title":"A Diversity-Promoting Objective Function for Neural Conversation Models"},{"paperId":"595c45a6c4fe895e00742f8316710e1177896deb","title":"Diagnostic Classifiers Revealing how Neural Networks Process Hierarchical Structure"},{"paperId":"258986132bf17755fe8263e42429fe73218c1534","title":"CIDEr: Consensus-based image description evaluation"},{"paperId":"c3823aacea60bc1f2cabb9283144690a3d015db5","title":"Neural Turing Machines"},{"paperId":"7c05a4ffee7e159e34b2efea7e44d994333ec628","title":"Recursive Neural Networks Can Learn Logical Semantics"},{"paperId":"26adb749fc5d80502a6d889966e50b31391560d3","title":"Meteor Universal: Language Specific Translation Evaluation for Any Target Language"},{"paperId":"52f86811b57034ba5c0478b37cab101d9a84024a","title":"Comparing Automatic Evaluation Measures for Image Description"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"9eccbe51227af5199144c047b7908cde41649fbf","title":"The case for compositionality"},{"paperId":"70309c07d192907c81665e701758554bc0417928","title":"A Compositional Distributional Semantics, Two Concrete Constructions, and Some Experimental Evaluations"},{"paperId":"37efe2ef1b9d27cc598361a8013ec888a6f7c4d8","title":"Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space"},{"paperId":"228d9e4b69926594fd26080f4cfaa9ecfca44eb3","title":"Mathematical Foundations for a Compositional Distributional Model of Meaning"},{"paperId":"b5d67d1dc671bce42a9daac0c3605adb3fcfc697","title":"Vector-based Models of Semantic Composition"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"399790217357cfb0bcd4ef2559055017bf302559","title":"Communication and Strong Compositionality"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"6843890926bf0e5c887ffc78dcb1203135981bf1","title":"The compositionality papers"},{"paperId":"9438172bfbb74a6a4ea4242b180d4335bb1f18b7","title":"Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":"eeae92442aaac000562bb4a2ea0ba6db2af8e9d6","title":"The foundations of arithmetic : a logico-mathematical enquiry into the concept of number"},{"paperId":null,"title":"Dhruv Batra, and Devi Parikh. 2017. C-vqa: A compositional split of the visual question answering (vqa)"}],"id":"a143cac1bc440135b612132c89e603f364b8a3b7","summary":"It is demonstrated that the models studied here do not compositionally generalize in terms of systematicity and productivity, however, they are robust to some degree to synonym substitutions."}]}