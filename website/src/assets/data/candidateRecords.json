{"papers":[{"url":"https://www.semanticscholar.org/paper/4c430e6c3a72626bd4cb1893960c7c26dfec6c79","title":"Structurally Diverse Sampling Reduces Spurious Correlations in Semantic Parsing Datasets","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":2,"influentialCitationCount":0,"publicationDate":null,"authors":"Shivanshu Gupta,Sameer Singh,Matt Gardner","id":"4c430e6c3a72626bd4cb1893960c7c26dfec6c79","summary":"This work proposes a novel algorithm for sampling a structurally diverse set of instances from a labeled instance pool with structured outputs that leads to better generalization and uses information theory to show that reduction in spurious correlations between substructures may be one reason why diverse training sets improve generalization.","score":5},{"url":"https://www.semanticscholar.org/paper/1bd799cf462f926041dd2fc8fbe4af54bddbf5c5","title":"Translate First Reorder Later: Leveraging Monotonicity in Semantic Parsing","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Francesco Cazzaro,Davide Locatelli,A. Quattoni,X. Carreras","id":"1bd799cf462f926041dd2fc8fbe4af54bddbf5c5","summary":"By means of the monotonic translations, TP OL can learn reliable lexico-logical patterns from aligned data, improving compositional generalization both over conventional seq2seq models, as well as over a recently proposed approach that exploits gold alignments.","score":5},{"url":"https://www.semanticscholar.org/paper/fcf25e1affc2f8ee5bb49d156f174e9769234deb","title":"Systematic Generalization with Edge Transformers","venue":"Neural Information Processing Systems","year":2021,"referenceCount":43,"citationCount":5,"influentialCitationCount":0,"publicationDate":"12/01/2021","authors":"Leon Bergen,T. O’Donnell,Dzmitry Bahdanau","id":"fcf25e1affc2f8ee5bb49d156f174e9769234deb","summary":"The Edge Transformer is a new model that combines inspiration from Transformers and rulebased symbolic AI that outperforms Relation-aware, Universal and classical Transformer baselines on compositional generalization benchmarks in relational reasoning, semantic parsing, and dependency parsing.","score":5},{"url":"https://www.semanticscholar.org/paper/b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347","title":"Sequence-to-Sequence Learning with Latent Neural Grammars","venue":"Neural Information Processing Systems","year":2021,"referenceCount":150,"citationCount":13,"influentialCitationCount":3,"publicationDate":"09/02/2021","authors":"Yoon Kim","id":"b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347","summary":"This work develops a neural parameterization of the grammar which enables parameter sharing over the combinatorial space of derivation rules without the need for manual feature engineering, and applies it to a diagnostic language navigation task and to small-scale machine translation.","score":4},{"url":"https://www.semanticscholar.org/paper/b1f33e956e36bf25e118c0d537dcc519cfe52e60","title":"CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations","venue":"ArXiv","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/12/2022","authors":"R'obert Csord'as,Kazuki Irie,J. Schmidhuber","id":"b1f33e956e36bf25e118c0d537dcc519cfe52e60","summary":"CTL++ is introduced, a new diagnostic dataset based on compositions of unary symbolic functions designed to test systematicity of NNs, that is, their capability to generalize to unseen compositions of known functions.","score":4},{"url":"https://www.semanticscholar.org/paper/676fa805bd715591f99bb17e36d673a6a14e92fe","title":"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":47,"citationCount":14,"influentialCitationCount":4,"publicationDate":"09/06/2021","authors":"I. Oren,Jonathan Herzig,Jonathan Berant","id":"676fa805bd715591f99bb17e36d673a6a14e92fe","summary":"This work investigates automatic generation of synthetic utterance-program pairs for improving compositional generalization in semantic parsing and selects a subset of synthetic examples that are structurally-diverse and uses them to improve compositionalgeneralization.","score":4},{"url":"https://www.semanticscholar.org/paper/c735740b26ceaa4db9d77233116434c0e8b311d8","title":"Learning Adaptive Control Flow in Transformers for Improved Systematic Generalization","venue":"","year":2021,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"R. Csordás,Kazuki Irie,J. Schmidhuber","id":"c735740b26ceaa4db9d77233116434c0e8b311d8","summary":"The novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the compositional table lookup task, and its attention and gating patterns tend to be interpretable as an intuitive form of neural routing.","score":4},{"url":"https://www.semanticscholar.org/paper/61d56ece2d19f4bfeb322c92085fb28521e169da","title":"Neural-Symbolic Recursive Machine for Systematic Generalization","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/04/2022","authors":"Qing Li,Yixin Zhu,Yitao Liang,Y. Wu,Song-Chun Zhu,Siyuan Huang","id":"61d56ece2d19f4bfeb322c92085fb28521e169da","summary":"The proposed Neural-Symbolic Recursive Machine (NSR) demonstrates stronger generalization than pure neural networks due to its symbolic representation and inductive biases, and demonstrates better transferability than existing neural-symbolic approaches due to less domain-speciﬁc knowledge required.","score":4},{"url":"https://www.semanticscholar.org/paper/b49ebf36a29cf9734313066129ab0d7092d4041e","title":"Categorizing Semantic Representations for Neural Machine Translation","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/13/2022","authors":"Yongjing Yin,Yafu Li,Fandong Meng,Jie Zhou,Yue Zhang","id":"b49ebf36a29cf9734313066129ab0d7092d4041e","summary":"The main idea is to enhance generalization by reducing sparsity and overfitting, which is achieved by finding prototypes of token representations over the training set and integrating their embeddings into the source encoding.","score":4},{"url":"https://www.semanticscholar.org/paper/da09949d0c89aca711de0f00e84138c62df623e1","title":"FROM SCAN TO REAL DATA: SYSTEMATIC GENER-","venue":"","year":2021,"referenceCount":49,"citationCount":2,"influentialCitationCount":0,"publicationDate":null,"authors":"Ning Shi,Boxin Wang,Wei Wang,Xiangyu Liu,Rong Zhang,Hui Xue,Xinbing Wang,Zhouhan Lin","id":"da09949d0c89aca711de0f00e84138c62df623e1","summary":"This paper revisits systematic generalization from the perspective of meaningful learning, an exceptional capability of humans to learn new concepts by connecting them with other previously known knowledge, and proposes to augment a training dataset in either an inductive or deductive manner to build semantic links between new and old concepts.","score":3},{"url":"https://www.semanticscholar.org/paper/a406701b5fb05be55244d4f940db7be55fce85c6","title":"Semantic Systematicity in Connectionist Language Production","venue":"Inf.","year":2021,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"Jesús Calvillo,Harm Brouwer,M. Crocker","id":"a406701b5fb05be55244d4f940db7be55fce85c6","summary":"A novel connectionist model of sentence production that employs rich situation model representations originally proposed for modeling systematicity in comprehension, which provides a sufficient structure from which the neural network can interpret novel inputs.","score":3},{"url":"https://www.semanticscholar.org/paper/eaa88d697f92739f3569564329e9d037aabbe2d7","title":"A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics","venue":"","year":2021,"referenceCount":105,"citationCount":1,"influentialCitationCount":1,"publicationDate":"03/02/2021","authors":"Qing Li,Siyuan Huang,Yining Hong,Yixin Zhu,Y. Wu,Song-Chun Zhu","id":"eaa88d697f92739f3569564329e9d037aabbe2d7","summary":"Models show a gap toward human-level generalization when tested with new concepts in a few-shot setting, and the results suggest that current models still struggle in extrapolation to long-range syntactic dependency and semantics.","score":3},{"url":"https://www.semanticscholar.org/paper/d129841cb2e30e25000dcd9edb83c880fc4babc1","title":"Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":35,"citationCount":2,"influentialCitationCount":0,"publicationDate":null,"authors":"Ayush K Chakravarthy,Jacob Russin,R. O’Reilly","id":"d129841cb2e30e25000dcd9edb83c880fc4babc1","summary":"This work develops a novel modification to the transformer by implementing two separate input streams: a role stream controls the attention distributions at each layer, and a filler stream determines the values.","score":3},{"url":"https://www.semanticscholar.org/paper/69df5b68fbf492341336b39b4cc9fcc74fff4d5f","title":"Improving Systematic Generalization Through Modularity and Augmentation","venue":"ArXiv","year":2022,"referenceCount":55,"citationCount":3,"influentialCitationCount":2,"publicationDate":"02/22/2022","authors":"Laura Ruis,B. Lake","id":"69df5b68fbf492341336b39b4cc9fcc74fff4d5f","summary":"This work investigates how two well-known modeling principles— modularity and data augmentation—affect systematic generalization of neural networks in grounded language learning and analyzes how large the vocabulary needs to be to achieve system- atic generalization and how similar the augmented data needs toBe to the problem at hand.","score":3},{"url":"https://www.semanticscholar.org/paper/66f3f0e8ebc780e570770986f50bf9cb9cd53ec1","title":"WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series Tasks","venue":"ArXiv","year":2022,"referenceCount":128,"citationCount":5,"influentialCitationCount":1,"publicationDate":"03/18/2022","authors":"Jean-Christophe Gagnon-Audet,Kartik Ahuja,Mohammad Javad Darvishi Bayazi,G. Dumas,I. Rish","id":"66f3f0e8ebc780e570770986f50bf9cb9cd53ec1","summary":"WOODS: eight challenging open-source time series benchmarks covering a diverse range of data modalities, such as videos, brain recordings, and sensor signals is presented, underscoring the new challenges posed by time series tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/bc16284f517dd0011dcf64ea1c8fe6d6576494a4","title":"Is the Computation of Abstract Sameness Relations Human-Like in Neural Language Models?","venue":"ArXiv","year":2022,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/12/2022","authors":"Lukas Thoma,Benjamin Roth","id":"bc16284f517dd0011dcf64ea1c8fe6d6576494a4","summary":"This work explores one facet of the question whether state-of-the-art NLP models exhibit elementary mechanisms known from human cognition by de-signed experimental settings in which each element from the original studies was mapped to a component of language models.","score":3},{"url":"https://www.semanticscholar.org/paper/108c25905be36b2a7a0fc7256ac314985ecd9699","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models","venue":"ArXiv","year":2022,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/24/2022","authors":"M. Bueno,Carlos Gemmel,Jeffrey Stephen Dalton,R. Lotufo,Rodrigo Nogueira","id":"108c25905be36b2a7a0fc7256ac314985ecd9699","summary":"It is demonstrated that large language models can succeed in extrapolation without modifying their architecture or training procedure, and a limitation of current architectures to effectively generalize without explicit surface form guidance is highlighted.","score":3},{"url":"https://www.semanticscholar.org/paper/1ed29beb55b10de8553c926ce6da2625ec2c8776","title":"Benchmarking Long-tail Generalization with Likelihood Splits","venue":"ArXiv","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/13/2022","authors":"Ameya Godbole,Robin Jia","id":"1ed29beb55b10de8553c926ce6da2625ec2c8776","summary":"This work proposes a method to create challenging benchmarks that require generalizing to the tail of the distribution by re-splitting existing datasets by creating ‘Likeli-hood splits’ where examples that are assigned lower likelihood by a pre-trained language model are placed in the test set, and more likely examples are in the training set.","score":3},{"url":"https://www.semanticscholar.org/paper/c6e4518dfd687a2a5bed4e78d5d9f999292a1746","title":"Counterfactual Recipe Generation: Exploring Compositional Generalization in a Realistic Scenario","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/20/2022","authors":"Xiao Liu,Yansong Feng,Jizhi Tang,Chengang Hu,Dongyan Zhao","id":"c6e4518dfd687a2a5bed4e78d5d9f999292a1746","summary":"This paper investigates whether pretrained language models can perform compositional generalization in a realistic setting: recipe generation, and designs the counterfactual recipe generation task, which asks models to modify a base recipe according to the change of an ingredient.","score":3},{"url":"https://www.semanticscholar.org/paper/59afb82c235f3d89996cefa71cd33ba7592e6b53","title":"Measuring Alignment Bias in Neural Seq2seq Semantic Parsers","venue":"STARSEM","year":2022,"referenceCount":39,"citationCount":1,"influentialCitationCount":0,"publicationDate":"05/17/2022","authors":"Davide Locatelli,A. Quattoni","id":"59afb82c235f3d89996cefa71cd33ba7592e6b53","summary":"This work augments the popular Geo semantic parsing dataset with alignment annotations and creates Geo-Aligned, and studies the performance of standard seq2seq models on the examples that can be aligned monotonically versus examples that require more complex alignments.","score":3},{"url":"https://www.semanticscholar.org/paper/8008348e87d3904842a2dd230c14b83112e8bf48","title":"Compositional Generalization in Dependency Parsing","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":18,"citationCount":3,"influentialCitationCount":0,"publicationDate":"10/13/2021","authors":"Emily Goodwin,Siva Reddy,T. O’Donnell,Dzmitry Bahdanau","id":"8008348e87d3904842a2dd230c14b83112e8bf48","summary":"This work introduces a gold-standard set of dependency parses for CFQ, and uses this to analyze the behaviour of a state-of-the art dependency parser on the CFQ dataset, finding that increasing compound divergence degrades dependency parsing performance, although not as dramatically as semantic parsing performance.","score":3},{"url":"https://www.semanticscholar.org/paper/2b060b89324c376892a096c84fd14664f7b71710","title":"Understanding Robust Generalization in Learning Regular Languages","venue":"International Conference on Machine Learning","year":2022,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":"02/20/2022","authors":"Soham Dan,O. Bastani,D. Roth","id":"2b060b89324c376892a096c84fd14664f7b71710","summary":"The empirical results support the hypothesis that auxiliary tasks can enable robust generalization, and theoretically prove that the compositional strategy generalizes significantly better than the end-to-end strategy.","score":3},{"url":"https://www.semanticscholar.org/paper/03eeff98d24383518ce0dacc0b3c4a38b6f1a514","title":"Recursive Decoding: A Situated Cognition Approach to Compositional Generation in Grounded Language Understanding","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/27/2022","authors":"Matthew Setzler,Scott Howland,Lauren A. Phillips","id":"03eeff98d24383518ce0dacc0b3c4a38b6f1a514","summary":"Recursive Decoding (RD) is presented, a novel procedure for training and using seq2seq models, targeted towards decode-side generalization, which yields dramatic improvement on two previously neglected generalization tasks in gSCAN.","score":3},{"url":"https://www.semanticscholar.org/paper/3d5699e7f7e085ad72102859b06fa4884d207e77","title":"Iterative Decoding for Compositional Generalization in Transformers","venue":"ArXiv","year":2021,"referenceCount":27,"citationCount":4,"influentialCitationCount":0,"publicationDate":"10/08/2021","authors":"Luana Ruiz,J. Ainslie,Santiago Ontan'on","id":"3d5699e7f7e085ad72102859b06fa4884d207e77","summary":"This paper introduces iterative decoding, an alternative toseq2seq that improves transformer compositional generalization in the PCFG and Cartesian product datasets and evidences that, in these datasets, seq2seq transformers do not learn iterations that are not unrolled.","score":3},{"url":"https://www.semanticscholar.org/paper/a77468f6bd4db7f8d761a0569d9cc29d5a8f0034","title":"L OGIC I NFERENCE : A N EW D ATASET FOR T EACHING L OGICAL I NFERENCE TO SEQ 2 SEQ M ODELS","venue":"","year":2022,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"Santiago Ontañón,J. Ainslie,V. Cvicek,Zachary Kenneth Fisher","id":"a77468f6bd4db7f8d761a0569d9cc29d5a8f0034","summary":"A new dataset to evaluate the ability of models to perform logical inference using propositional logic and a small subset of ﬁrst-order logic, represented both in semi-formal logical notation, as well as in natural language is presented.","score":3},{"url":"https://www.semanticscholar.org/paper/5021fd710fd17dee53bc7bc7bf334b148ef3d8b6","title":"LogicInference: A New Dataset for Teaching Logical Inference to seq2seq Models","venue":"ArXiv","year":2022,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/28/2022","authors":"Santiago Ontañón,J. Ainslie,V. Cvicek,Zachary Kenneth Fisher","id":"5021fd710fd17dee53bc7bc7bf334b148ef3d8b6","summary":"A new dataset to evaluate the ability of models to perform logical inference using propositional logic and a small subset of ﬁrst-order logic, represented both in semi-formal logical notation, as well as in natural language is presented.","score":3},{"url":"https://www.semanticscholar.org/paper/a5378175d31d3dd8fa004037df663aa00f236a0b","title":"Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/02/2022","authors":"Yuxuan Li,James L. McClelland","id":"a5378175d31d3dd8fa004037df663aa00f236a0b","summary":"It is shown that two-layer transformers learn reliable solutions to multi-level problems, develop signs of task decomposition, and encode input items in a way that encourages the exploitation of shared computation across related tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/97833e2aa0da5240e62436373b58af988a4ab6ab","title":"The Curious Case of Absolute Position Embeddings","venue":"ArXiv","year":2022,"referenceCount":56,"citationCount":2,"influentialCitationCount":0,"publicationDate":"10/23/2022","authors":"Koustuv Sinha,A. Kazemnejad,Siva Reddy,J. Pineau,D. Hupkes,Adina Williams","id":"97833e2aa0da5240e62436373b58af988a4ab6ab","summary":"This work observes that models trained with APE over-rely on positional information to the point that they break-down when subjected to sentences with shifted position information, and raises questions about theacy of APEs to model the relativity of position information.","score":3},{"url":"https://www.semanticscholar.org/paper/86589b6286ef3c55b8b4fccfb41a3b30b7afdf61","title":"Going Beyond Linear Transformers with Recurrent Fast Weight Programmers","venue":"Neural Information Processing Systems","year":2021,"referenceCount":80,"citationCount":24,"influentialCitationCount":1,"publicationDate":"06/11/2021","authors":"Kazuki Irie,Imanol Schlag,R'obert Csord'as,J. Schmidhuber","id":"86589b6286ef3c55b8b4fccfb41a3b30b7afdf61","summary":"The novel recurrent FWPs (RFWPs) are evaluated on two synthetic algorithmic tasks (code execution and sequential ListOps), Wikitext-103 language models, and on the Atari 2600 2D game environment, where the models exhibit properties of Transformers and RNNs.","score":2},{"url":"https://www.semanticscholar.org/paper/79cb080c84da314c2113692585b1e9ee29afa33a","title":"On learning an interpreted language with recurrent models","venue":"","year":2018,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/11/2018","authors":"Denis Paperno","id":"79cb080c84da314c2113692585b1e9ee29afa33a","summary":"This work constructs simplified datasets reflecting core properties of natural language as modeled in formal syntax and semantics: recursive syntactic structure and compositionality, and finds LSTM and GRU networks to generalise to compositional interpretation well, but only in the most favorable learning settings.","score":2},{"url":"https://www.semanticscholar.org/paper/4b58367375466e653751a0c258b2f50bd3551408","title":"Sequence-to-Sequence Networks Learn the Meaning of Reflexive Anaphora","venue":"CRAC","year":2020,"referenceCount":24,"citationCount":2,"influentialCitationCount":0,"publicationDate":"11/02/2020","authors":"R. Frank,Jackson Owen Petty","id":"4b58367375466e653751a0c258b2f50bd3551408","summary":"This paper considers sequence-to-sequence architectures with recurrent units and shows that such networks are capable of learning semantic interpretations for reflexive anaphora which generalize to novel antecedents.","score":2},{"url":"https://www.semanticscholar.org/paper/7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8","title":"AND does not mean OR: Using Formal Languages to Study Language Models’ Representations","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":17,"citationCount":7,"influentialCitationCount":0,"publicationDate":null,"authors":"Aaron Traylor,Roman Feiman,Elizabeth-Jane Pavlick","id":"7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8","summary":"None of the simulated training corpora result in models which definitively differentiate meaningfully different symbols (e.g., AND vs. OR), suggesting a limitation to the types of semantic signals that current models are able to exploit.","score":2},{"url":"https://www.semanticscholar.org/paper/0b1470014bdbaa80ba63da0491d9db6c7d4febcc","title":"Detecting Compositionally Out-of-Distribution Examples in Semantic Parsing","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":30,"citationCount":2,"influentialCitationCount":0,"publicationDate":null,"authors":"Denis Lukovnikov,Sina Däubener,Asja Fischer","id":"0b1470014bdbaa80ba63da0491d9db6c7d4febcc","summary":"This work investigates several strong yet simple methods for OOD detection based on predictive uncertainty and shows that these techniques perform well on the standard SCAN and CFQ datasets and can be improved by using a heterogeneous ensemble.","score":2},{"url":"https://www.semanticscholar.org/paper/acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":67,"citationCount":84,"influentialCitationCount":18,"publicationDate":"10/24/2020","authors":"Peter Shaw,Ming-Wei Chang,Panupong Pasupat,Kristina Toutanova","id":"acf8a1040034820bf99379a3422815f4e0859ec9","summary":"NQG-T5 is proposed, a hybrid model that combines a high-precision grammar-based approach with a pre-trained sequence-to-sequence model that outperforms existing approaches across several compositional generalization challenges on non-synthetic data, while also being competitive with the state of theart on standard evaluations.","score":2},{"url":"https://www.semanticscholar.org/paper/40848b41ed8c9c255ecd8a920006877691b52d03","title":"WILDS: A Benchmark of in-the-Wild Distribution Shifts","venue":"International Conference on Machine Learning","year":2020,"referenceCount":424,"citationCount":467,"influentialCitationCount":81,"publicationDate":"12/14/2020","authors":"P. W. Koh,Shiori Sagawa,H. Marklund,Sang Michael Xie,Marvin Zhang,Akshay Balsubramani,Weihua Hu,Michihiro Yasunaga,Richard L. Phillips,Sara Beery,J. Leskovec,A. Kundaje,E. Pierson,S. Levine,Chelsea Finn,Percy Liang","id":"40848b41ed8c9c255ecd8a920006877691b52d03","summary":"WILDS is presented, a benchmark of in-the-wild distribution shifts spanning diverse data modalities and applications, and is hoped to encourage the development of general-purpose methods that are anchored to real-world distribution shifts and that work well across different applications and problem settings.","score":2},{"url":"https://www.semanticscholar.org/paper/0d39d525f30609d0541330f933007025cd457a83","title":"Exploring Transitivity in Neural NLI Models through Veridicality","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":59,"citationCount":12,"influentialCitationCount":1,"publicationDate":"01/26/2021","authors":"Hitomi Yanaka,K. Mineshima,Kentaro Inui","id":"0d39d525f30609d0541330f933007025cd457a83","summary":"It is found that current NLI models do not perform consistently well on transitivity inference tasks, suggesting that they lack the generalization capacity for drawing composite inferences from provided training examples.","score":2},{"url":"https://www.semanticscholar.org/paper/5505d608a1d482fdc083796db812379ec1cb8723","title":"Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches","venue":"ArXiv","year":2021,"referenceCount":107,"citationCount":17,"influentialCitationCount":1,"publicationDate":null,"authors":"Nelson F. Liu,Tony Lee,Robin Jia,Percy Liang","id":"5505d608a1d482fdc083796db812379ec1cb8723","summary":"Small, targeted synthetic benchmarks are constructed that do not resemble natural language, yet have high concurrence with SQuAD, demonstrating that naturalness and size are not necessary for reflecting historical modeling improvements on SQuad.","score":2},{"url":"https://www.semanticscholar.org/paper/577d44a10b424a55165a6bf4839bafce2c695302","title":"SyGNS: A Systematic Generalization Testbed Based on Natural Language Semantics","venue":"Findings","year":2021,"referenceCount":60,"citationCount":4,"influentialCitationCount":0,"publicationDate":"06/02/2021","authors":"Hitomi Yanaka,K. Mineshima,Kentarou Inui","id":"577d44a10b424a55165a6bf4839bafce2c695302","summary":"This work proposes a Systematic Generalization testbed based on Natural language Semantics (SyGNS), whose challenge is to map natural language sentences to multiple forms of scoped meaning representations, designed to account for various semantic phenomena.","score":2},{"url":"https://www.semanticscholar.org/paper/83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","title":"Lexicon Learning for Few Shot Sequence Modeling","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":42,"citationCount":17,"influentialCitationCount":2,"publicationDate":"06/07/2021","authors":"Ekin Akyürek,Jacob Andreas","id":"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","summary":"This work augments neural decoders with a lexical translation mechanism that generalizes existing copy mechanisms to incorporate learned, decontextualized, token-level translation rules, and shows that it improves systematic generalization on a diverse set of sequence modeling tasks drawn from cognitive science, formal semantics, and machine translation.","score":2},{"url":"https://www.semanticscholar.org/paper/3962f108081b22c7e54b413f47ba6f2c16f2cc05","title":"Frequency Effects on Syntactic Rule Learning in Transformers","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":38,"citationCount":26,"influentialCitationCount":4,"publicationDate":"09/14/2021","authors":"Jason Wei,Dan Garrette,Tal Linzen,Ellie Pavlick","id":"3962f108081b22c7e54b413f47ba6f2c16f2cc05","summary":"It is shown that BERT often generalizes well to subject–verb pairs that never occurred in training, suggesting a degree of rule-governed behavior, and that performance is heavily influenced by word frequency.","score":2},{"url":"https://www.semanticscholar.org/paper/af749e5dbde38914ca6fa1463fca17eac8f69ecc","title":"ReaSCAN: Compositional Reasoning in Language Grounding","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":56,"citationCount":8,"influentialCitationCount":1,"publicationDate":"09/18/2021","authors":"Zhengxuan Wu,Elisa Kreiss,Desmond C. Ong,Christopher Potts","id":"af749e5dbde38914ca6fa1463fca17eac8f69ecc","summary":"This work proposes ReaSCAN, a benchmark dataset that builds off gSCAN but requires compositional language interpretation and reasoning about entities and relations, and assesses two models on Rea SCAN: a multi-modal baseline and a state-of-the-art graph convolutional neural model.","score":2},{"url":"https://www.semanticscholar.org/paper/06fbeaf4d16639f177973a06cd7c4f78cb5e38ed","title":"COVR: A Test-Bed for Visually Grounded Compositional Generalization with Real Images","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":29,"citationCount":11,"influentialCitationCount":3,"publicationDate":"09/22/2021","authors":"Ben Bogin,Shivanshu Gupta,Matt Gardner,Jonathan Berant","id":"06fbeaf4d16639f177973a06cd7c4f78cb5e38ed","summary":"This work proposes COVR, a new test-bed for visually-grounded compositional generalization with real images, and proposes an almost fully automatic procedure for generating question-answer pairs along with a set of context images.","score":2},{"url":"https://www.semanticscholar.org/paper/4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80","title":"How BPE Affects Memorization in Transformers","venue":"ArXiv","year":2021,"referenceCount":67,"citationCount":9,"influentialCitationCount":0,"publicationDate":"10/06/2021","authors":"E. Kharitonov,Marco Baroni,D. Hupkes","id":"4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80","summary":"It is demonstrated that the size of the subword vocabulary learned by Byte-Pair Encoding greatly affects both ability and tendency of standard Transformer models to memorize training data, even when the authors control for the number of learned parameters.","score":2},{"url":"https://www.semanticscholar.org/paper/04db9b694280134f09af5fa787a306907edba29d","title":"How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN","venue":"ArXiv","year":2021,"referenceCount":81,"citationCount":16,"influentialCitationCount":2,"publicationDate":"11/18/2021","authors":"R. Thomas McCoy,P. Smolensky,Tal Linzen,Jianfeng Gao,Asli Celikyilmaz","id":"04db9b694280134f09af5fa787a306907edba29d","summary":"AVEN, a suite of analyses for assessing the novelty of generated text, focusing on sequential structure (n-grams) and syntactic structure, is introduced, showing that GPT-2's novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues.","score":2},{"url":"https://www.semanticscholar.org/paper/a576512a7562597fd30719a834d5866d010ef6ab","title":"Compositional Generalization for Natural Language Interfaces to Web APIs","venue":"ArXiv","year":2021,"referenceCount":33,"citationCount":1,"influentialCitationCount":1,"publicationDate":"12/09/2021","authors":"Saghar Hosseini,A. Awadallah,Yu Su","id":"a576512a7562597fd30719a834d5866d010ef6ab","summary":"New compositional generalization tasks for NL2API are defined which explore the models’ ability to extrapolate from simple API calls in the training set to new and more complex API Calls in the inference phase.","score":2},{"url":"https://www.semanticscholar.org/paper/dc88d2bbcebd810d7c80ba281739908005b12235","title":"Neurocompositional computing in human and machine intelligence: A tutorial","venue":"","year":2022,"referenceCount":240,"citationCount":3,"influentialCitationCount":1,"publicationDate":null,"authors":"P. Smolensky,R. Thomas McCoy,Roland Fernandez,M. Goldrick,Jia-Hao Gao","id":"dc88d2bbcebd810d7c80ba281739908005b12235","summary":"It is shown that the new techniques now being deployed in second-generation neurocompositional computing create AI systems that are not only more robust and accurate than current systems, but also more comprehensible—making it possible to diagnose errors in, and exert human control over, artificial neural networks through interpretation of their internal states and direct intervention upon those states.","score":2},{"url":"https://www.semanticscholar.org/paper/b3f644a5ea1fdd8cec1c34ebed69125838a50de3","title":"The Paradox of the Compositionality of Natural Language: A Neural Machine Translation Case Study","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":51,"citationCount":22,"influentialCitationCount":1,"publicationDate":"08/12/2021","authors":"Verna Dankers,Elia Bruni,D. Hupkes","id":"b3f644a5ea1fdd8cec1c34ebed69125838a50de3","summary":"This work re-instantiate three compositionality tests from the literature and reformulate them for neural machine translation (NMT) and highlights that models are sometimes less compositional than expected, but sometimes more, exemplifying that different levels of compositionality are required.","score":2},{"url":"https://www.semanticscholar.org/paper/b8b813111c411ae61881ab9cd25707d9de6444ec","title":"Compositional Attention: Disentangling Search and Retrieval","venue":"International Conference on Learning Representations","year":2021,"referenceCount":45,"citationCount":8,"influentialCitationCount":1,"publicationDate":"10/18/2021","authors":"Sarthak Mittal,Sharath Chandra Raparthy,I. Rish,Yoshua Bengio,Guillaume Lajoie","id":"b8b813111c411ae61881ab9cd25707d9de6444ec","summary":"This work proposes a novel attention mechanism, called Compositional Attention, that replaces the standard head structure, and demonstrates that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings.","score":2},{"url":"https://www.semanticscholar.org/paper/40b4d98588719407fb72a014ab79e4145695654b","title":"Quantifying Adaptability in Pre-trained Language Models with 500 Tasks","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":60,"citationCount":3,"influentialCitationCount":1,"publicationDate":"12/06/2021","authors":"Belinda Z. Li,Jane A. Yu,Madian Khabsa,Luke Zettlemoyer,A. Halevy,Jacob Andreas","id":"40b4d98588719407fb72a014ab79e4145695654b","summary":"A large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence modeling tasks, shows that adaptability to new tasks, like generalization to new examples, can be systematically described and understood.","score":2},{"url":"https://www.semanticscholar.org/paper/ace2a00425f96e9d0dbbe2869023d56c6c91267f","title":"On Learning Interpreted Languages with Recurrent Models","venue":"Computational Linguistics","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/14/2022","authors":"Denis Paperno","id":"ace2a00425f96e9d0dbbe2869023d56c6c91267f","summary":"This work finds LSTM and GRU networks to generalize to compositional interpretation well, but only in the most favorable learning settings, with a well-paced curriculum, extensive training data, and left- to-right (but not right-to-left) composition.","score":2}]}