{"papers":[{"url":"https://www.semanticscholar.org/paper/dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models","venue":"","year":2022,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/24/2022","authors":"Yuekun Yao,Alexander Koller","id":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","summary":"","score":8},{"url":"https://www.semanticscholar.org/paper/6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing","venue":"ArXiv","year":2022,"referenceCount":83,"citationCount":3,"influentialCitationCount":1,"publicationDate":"05/24/2022","authors":"Linlu Qiu,Peter Shaw,Panupong Pasupat,Tianze Shi,Jonathan Herzig,Emily Pitler,Fei Sha,Kristina Toutanova","id":"6e10343767ab09dde83cf99ea3442907402a9810","summary":"Limits of current techniques for effectively leveraging model scale for compositional generalization are highlighted, while the analysis also suggests promising directions for future work.","score":7},{"url":"https://www.semanticscholar.org/paper/ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","title":"Compositional Generalisation with Structured Reordering and Fertility Layers","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/06/2022","authors":"Matthias Lindemann,Alexander Koller,Ivan Titov","id":"ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","summary":"This work presents a end-to-end differentiable neural model that composes two structural operations: a fertility step, which is introduced in this work, and a reordering step based on previous work, which outperforms seq2seq models by a wide margin on challenging compositional splits of realis-tic semantic parsing tasks.","score":7},{"url":"https://www.semanticscholar.org/paper/90c1a63aada7704eadc4324c16a66ec793d4b698","title":"Compositional generalization with a broad-coverage semantic parser","venue":"STARSEM","year":2022,"referenceCount":28,"citationCount":3,"influentialCitationCount":1,"publicationDate":null,"authors":"Pia Weissenhorn,L. Donatelli,Alexander Koller","id":"90c1a63aada7704eadc4324c16a66ec793d4b698","summary":"It is shown how the AM parser, a compositional semantic parser (Groschwitz et al., 2018) can solve compositional generalization on the COGS dataset and is the first semantic parser that achieves high accuracy on both naturally occurring language and the syntheticCOGS dataset.","score":6},{"url":"https://www.semanticscholar.org/paper/00050c15896e8ae6bb534f10d072351547993f72","title":"LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing","venue":"ArXiv","year":2021,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/14/2021","authors":"Dora Jambor,Dzmitry Bahdanau","id":"00050c15896e8ae6bb534f10d072351547993f72","summary":"This work shows that better systematic generalization can be achieved by producing the meaning representation directly as a graph and not as a sequence, and proposes LAGr (Label Aligned Graphs), a general framework to produce semantic parses by independently predicting node and edge labels for a complete multi-layer input-aligned graph.","score":6},{"url":"https://www.semanticscholar.org/paper/5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation","venue":"NAACL","year":2021,"referenceCount":77,"citationCount":12,"influentialCitationCount":1,"publicationDate":"12/14/2021","authors":"Linlu Qiu,Peter Shaw,Panupong Pasupat,Pawel Krzysztof Nowak,Tal Linzen,Fei Sha,Kristina Toutanova","id":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","summary":"This work presents a more powerful data recombination method using a model called Compositional Structure Learner (CSL), a generative model with a quasi-synchronous context-free grammar backbone, which results in a model even stronger than a T5-CSL ensemble on two real world compositional generalization tasks.","score":6},{"url":"https://www.semanticscholar.org/paper/a122909a31acf41cb2d9eb602c01b24b9b85a061","title":"LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing","venue":"ACL","year":2022,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":"05/19/2022","authors":"Dora Jambor,Dzmitry Bahdanau","id":"a122909a31acf41cb2d9eb602c01b24b9b85a061","summary":"This work shows that better systematic generalization can be achieved by producing the meaning representation directly as a graph and not as a sequence, and proposes LAGr (Label Aligned Graphs), a general framework to produce semantic parses by independently predicting node and edge labels for a complete multi-layer input-aligned graph.","score":6},{"url":"https://www.semanticscholar.org/paper/559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review","venue":"ArXiv","year":2022,"referenceCount":690,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/06/2022","authors":"D. Hupkes,Mario Giulianelli,Verna Dankers,Mikel Artetxe,Yanai Elazar,Tiago Pimentel,Christos Christodoulopoulos,Karim Lasri,Naomi Saphra,Arabella J. Sinclair,Dennis Ulmer,Florian Schottmann,Khuyagbaatar Batsuren,Kaiser Sun,Koustuv Sinha,Leila Khalatbari,Maria Ryskina,Rita Frieske,Ryan Cotterell,Zhijing Jin","id":"559bfba3bee31f6061a5d5c7061f22794de47e39","summary":"A taxonomy for characterising and understanding generalisation research in NLP is presented, a taxonomy is used to present a comprehensive map of published generalisation studies, and recommendations for which areas might deserve attention in the future are made.","score":6},{"url":"https://www.semanticscholar.org/paper/fcf25e1affc2f8ee5bb49d156f174e9769234deb","title":"Systematic Generalization with Edge Transformers","venue":"NeurIPS","year":2021,"referenceCount":46,"citationCount":3,"influentialCitationCount":0,"publicationDate":"12/01/2021","authors":"Leon Bergen,T. O’Donnell,Dzmitry Bahdanau","id":"fcf25e1affc2f8ee5bb49d156f174e9769234deb","summary":"The Edge Transformer is a new model that combines inspiration from Transformers and rulebased symbolic AI that outperforms Relation-aware, Universal and classical Transformer baselines on compositional generalization benchmarks in relational reasoning, semantic parsing, and dependency parsing.","score":5},{"url":"https://www.semanticscholar.org/paper/69078af65fc934f81fd340e9d1323d6c08194548","title":"Revisiting the Compositional Generalization Abilities of Neural Sequence Models","venue":"ACL","year":2022,"referenceCount":22,"citationCount":4,"influentialCitationCount":1,"publicationDate":"03/14/2022","authors":"Arkil Patel,S. Bhattamishra,P. Blunsom,Navin Goyal","id":"69078af65fc934f81fd340e9d1323d6c08194548","summary":"It is demonstrated that modifying the training distribution in simple and intuitive ways enables standard seq-to-seq models to achieve near-perfect generalization performance, thereby showing that their compositional generalization abilities were previously underestimated.","score":5},{"url":"https://www.semanticscholar.org/paper/6f0be1f9bda7530b1fa654cac84d595ca9d53740","title":"Revisit Systematic Generalization via Meaningful Learning","venue":"","year":2020,"referenceCount":81,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/14/2020","authors":"Ning Shi,Boxin Wang,Wei Wang,Xiangyu Liu,Zhouhan Lin","id":"6f0be1f9bda7530b1fa654cac84d595ca9d53740","summary":"It is suggested that models can successfully one-shot generalize to novel concepts and compositions through semantic linking, either inductively or deductively, and it is demonstrated that prior knowledge plays a key role as well.","score":4},{"url":"https://www.semanticscholar.org/paper/b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347","title":"Sequence-to-Sequence Learning with Latent Neural Grammars","venue":"NeurIPS","year":2021,"referenceCount":150,"citationCount":11,"influentialCitationCount":3,"publicationDate":"09/02/2021","authors":"Yoon Kim","id":"b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347","summary":"This work develops a neural parameterization of the grammar which enables parameter sharing over the combinatorial space of derivation rules without the need for manual feature engineering, and applies it to a diagnostic language navigation task and to small-scale machine translation.","score":4},{"url":"https://www.semanticscholar.org/paper/9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"C OMPOSITIONAL G ENERALIZATION AND D ECOMPOSITION IN N EURAL P ROGRAM S YNTHESIS","venue":"","year":null,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"","id":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","summary":"A suite of generalization tasks, which measure different types of compositional generalization that are desirable for program synthesis and are particularly difﬁcult for current sequence to sequence models, are proposed.","score":4},{"url":"https://www.semanticscholar.org/paper/4c430e6c3a72626bd4cb1893960c7c26dfec6c79","title":"Structurally Diverse Sampling Reduces Spurious Correlations in Semantic Parsing Datasets","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":2,"influentialCitationCount":0,"publicationDate":"03/16/2022","authors":"Shivanshu Gupta,Sameer Singh,Matt Gardner","id":"4c430e6c3a72626bd4cb1893960c7c26dfec6c79","summary":"This work proposes a novel algorithm for sampling a structurally diverse set of instances from a labeled instance pool with structured outputs that leads to better generalization and uses information theory to show that reduction in spurious correlations between substructures may be one reason why diverse training sets improve generalization.","score":4},{"url":"https://www.semanticscholar.org/paper/6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis","venue":"ArXiv","year":2022,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/07/2022","authors":"Kensen Shi,Joey Hong,M. Zaheer,Pengcheng Yin,Charles Sutton","id":"6a250b904965732840a75b6a13e35ac15f5cce4d","summary":"A suite of generalization tasks, which measure different types of compositional generalization that are desirable for program synthesis and are particularly difﬁcult for current sequence to sequence models, are proposed.","score":4},{"url":"https://www.semanticscholar.org/paper/c764ecba2bace12b9bfb9c2b0651a12ff6888ea7","title":"Learning to Generalize Compositionally by Transferring Across Semantic Parsing Tasks","venue":"ArXiv","year":2021,"referenceCount":51,"citationCount":3,"influentialCitationCount":0,"publicationDate":"11/09/2021","authors":"Wang Zhu,Peter Shaw,Tal Linzen,Fei Sha","id":"c764ecba2bace12b9bfb9c2b0651a12ff6888ea7","summary":"This work investigates learning representations that facilitate transfer learning from one compositional task to another: the representation and the task-specific layers of the models are strategically trained differently on a pre-finetuning task such that they generalize well on mismatched splits that require compositionality.","score":4},{"url":"https://www.semanticscholar.org/paper/04ca71d9089b2e86f9e4a874fd66ee8bd0baba8e","title":"When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks","venue":"","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/23/2022","authors":"Ankur Sikarwar,Arkil Patel,Navin Goyal","id":"04ca71d9089b2e86f9e4a874fd66ee8bd0baba8e","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/676fa805bd715591f99bb17e36d673a6a14e92fe","title":"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization","venue":"EMNLP","year":2021,"referenceCount":47,"citationCount":10,"influentialCitationCount":3,"publicationDate":"09/06/2021","authors":"I. Oren,Jonathan Herzig,Jonathan Berant","id":"676fa805bd715591f99bb17e36d673a6a14e92fe","summary":"This work investigates automatic generation of synthetic utterance-program pairs for improving compositional generalization in semantic parsing and selects a subset of synthetic examples that are structurally-diverse and uses them to improve compositionalgeneralization.","score":4},{"url":"https://www.semanticscholar.org/paper/bb0ab8591d6d57c7e2bd1ec35d806b3f25277752","title":"Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks","venue":"EMNLP","year":2021,"referenceCount":42,"citationCount":7,"influentialCitationCount":0,"publicationDate":"09/30/2021","authors":"Yichen Jiang,Mohit Bansal","id":"bb0ab8591d6d57c7e2bd1ec35d806b3f25277752","summary":"It is argued that compositionality can be induced in Transformers given minimal but proper guidance, and a better result is achieved using less contextualized vectors as the attention’s query, providing insights into architecture choices in achieving systematic compositionality.","score":4},{"url":"https://www.semanticscholar.org/paper/c735740b26ceaa4db9d77233116434c0e8b311d8","title":"Learning Adaptive Control Flow in Transformers for Improved Systematic Generalization","venue":"","year":2021,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"R. Csordás,Kazuki Irie,J. Schmidhuber","id":"c735740b26ceaa4db9d77233116434c0e8b311d8","summary":"The novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the compositional table lookup task, and its attention and gating patterns tend to be interpretable as an intuitive form of neural routing.","score":4},{"url":"https://www.semanticscholar.org/paper/61d56ece2d19f4bfeb322c92085fb28521e169da","title":"Neural-Symbolic Recursive Machine for Systematic Generalization","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/04/2022","authors":"Qing Li,Yixin Zhu,Yitao Liang,Y. Wu,Song-Chun Zhu,Siyuan Huang","id":"61d56ece2d19f4bfeb322c92085fb28521e169da","summary":"The proposed Neural-Symbolic Recursive Machine (NSR) demonstrates stronger generalization than pure neural networks due to its symbolic representation and inductive biases, and demonstrates better transferability than existing neural-symbolic approaches due to less domain-speciﬁc knowledge required.","score":4},{"url":"https://www.semanticscholar.org/paper/b49ebf36a29cf9734313066129ab0d7092d4041e","title":"Categorizing Semantic Representations for Neural Machine Translation","venue":"COLING","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/13/2022","authors":"Yongjing Yin,Yafu Li,Fandong Meng,Jie Zhou,Yue Zhang","id":"b49ebf36a29cf9734313066129ab0d7092d4041e","summary":"The main idea is to enhance generalization by reducing sparsity and overfitting, which is achieved by finding prototypes of token representations over the training set and integrating their embeddings into the source encoding.","score":4},{"url":"https://www.semanticscholar.org/paper/da09949d0c89aca711de0f00e84138c62df623e1","title":"FROM SCAN TO REAL DATA: SYSTEMATIC GENER-","venue":"","year":2021,"referenceCount":49,"citationCount":1,"influentialCitationCount":0,"publicationDate":null,"authors":"Ning Shi,Boxin Wang,Wei Wang,Xiangyu Liu,Rong Zhang,Hui Xue,Xinbing Wang,Zhouhan Lin","id":"da09949d0c89aca711de0f00e84138c62df623e1","summary":"This paper revisits systematic generalization from the perspective of meaningful learning, an exceptional capability of humans to learn new concepts by connecting them with other previously known knowledge, and proposes to augment a training dataset in either an inductive or deductive manner to build semantic links between new and old concepts.","score":3},{"url":"https://www.semanticscholar.org/paper/a406701b5fb05be55244d4f940db7be55fce85c6","title":"Semantic Systematicity in Connectionist Language Production","venue":"Inf.","year":2021,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"Jesús Calvillo,Harm Brouwer,M. Crocker","id":"a406701b5fb05be55244d4f940db7be55fce85c6","summary":"A novel connectionist model of sentence production that employs rich situation model representations originally proposed for modeling systematicity in comprehension, which provides a sufficient structure from which the neural network can interpret novel inputs.","score":3},{"url":"https://www.semanticscholar.org/paper/eaa88d697f92739f3569564329e9d037aabbe2d7","title":"A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics","venue":"","year":2021,"referenceCount":105,"citationCount":1,"influentialCitationCount":1,"publicationDate":"03/02/2021","authors":"Qing Li,Siyuan Huang,Yining Hong,Yixin Zhu,Y. Wu,Song-Chun Zhu","id":"eaa88d697f92739f3569564329e9d037aabbe2d7","summary":"Models show a gap toward human-level generalization when tested with new concepts in a few-shot setting, and the results suggest that current models still struggle in extrapolation to long-range syntactic dependency and semantics.","score":3},{"url":"https://www.semanticscholar.org/paper/76c9558b3fa10baf0e094386a650015b29a8a4bc","title":"Compositional generalization in semantic parsing with pretrained transformers","venue":"ArXiv","year":2021,"referenceCount":25,"citationCount":3,"influentialCitationCount":0,"publicationDate":"09/30/2021","authors":"A. Orhan","id":"76c9558b3fa10baf0e094386a650015b29a8a4bc","summary":"It is shown that language models pretrained exclusively with nonEnglish corpora, or even with programming language corporA, significantly improve out-of-distribution generalization in these benchmarks, compared with models trained from scratch, even though both benchmarks are English-based.","score":3},{"url":"https://www.semanticscholar.org/paper/a143cac1bc440135b612132c89e603f364b8a3b7","title":"Combine to Describe: Evaluating Compositional Generalization in Image Captioning","venue":"ACL","year":2022,"referenceCount":57,"citationCount":1,"influentialCitationCount":0,"publicationDate":null,"authors":"G. Pantazopoulos,Alessandro Suglia,Arash Eshghi","id":"a143cac1bc440135b612132c89e603f364b8a3b7","summary":"It is demonstrated that the models studied here do not compositionally generalize in terms of systematicity and productivity, however, they are robust to some degree to synonym substitutions.","score":3},{"url":"https://www.semanticscholar.org/paper/66f3f0e8ebc780e570770986f50bf9cb9cd53ec1","title":"WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series Tasks","venue":"ArXiv","year":2022,"referenceCount":128,"citationCount":2,"influentialCitationCount":1,"publicationDate":"03/18/2022","authors":"Jean-Christophe Gagnon-Audet,Kartik Ahuja,Mohammad Javad Darvishi Bayazi,G. Dumas,I. Rish","id":"66f3f0e8ebc780e570770986f50bf9cb9cd53ec1","summary":"WOODS: eight challenging open-source time series benchmarks covering a diverse range of data modalities, such as videos, brain recordings, and sensor signals is presented, underscoring the new challenges posed by time series tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/bc16284f517dd0011dcf64ea1c8fe6d6576494a4","title":"Is the Computation of Abstract Sameness Relations Human-Like in Neural Language Models?","venue":"ArXiv","year":2022,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/12/2022","authors":"Lukas Thoma,Benjamin Roth","id":"bc16284f517dd0011dcf64ea1c8fe6d6576494a4","summary":"This work explores one facet of the question whether state-of-the-art NLP models exhibit elementary mechanisms known from human cognition by de-signed experimental settings in which each element from the original studies was mapped to a component of language models.","score":3},{"url":"https://www.semanticscholar.org/paper/b1f33e956e36bf25e118c0d537dcc519cfe52e60","title":"CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations","venue":"ArXiv","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/12/2022","authors":"R'obert Csord'as,Kazuki Irie,J. Schmidhuber","id":"b1f33e956e36bf25e118c0d537dcc519cfe52e60","summary":"CTL++ is introduced, a new diagnostic dataset based on compositions of unary symbolic functions designed to test systematicity of NNs, that is, their capability to generalize to unseen compositions of known functions.","score":3},{"url":"https://www.semanticscholar.org/paper/1ed29beb55b10de8553c926ce6da2625ec2c8776","title":"Benchmarking Long-tail Generalization with Likelihood Splits","venue":"ArXiv","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/13/2022","authors":"Ameya Godbole,Robin Jia","id":"1ed29beb55b10de8553c926ce6da2625ec2c8776","summary":"This work proposes a method to create challenging benchmarks that require generalizing to the tail of the distribution by re-splitting existing datasets by creating ‘Likeli-hood splits’ where examples that are assigned lower likelihood by a pre-trained language model are placed in the test set, and more likely examples are in the training set.","score":3},{"url":"https://www.semanticscholar.org/paper/39f604fdd3ade5bd5a67d5284a6d9c12e535db85","title":"Compositionality as Lexical Symmetry","venue":"ArXiv","year":2022,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/30/2022","authors":"Ekin Akyürek,Jacob Andreas","id":"39f604fdd3ade5bd5a67d5284a6d9c12e535db85","summary":"This paper proves that for any task factorizable into a lex013 icon and a composition function, there exists a family of data transformation functions that are guaranteed to produce new, well-formed examples when applied to training data and shows that it is possible to identify these transformations even when the compositional function is unknown.","score":3},{"url":"https://www.semanticscholar.org/paper/1167b3864046b732cf057b8b05db311e726cadab","title":"Measuring Alignment Bias in Neural Seq2seq Semantic Parsers","venue":"STARSEM","year":2022,"referenceCount":39,"citationCount":1,"influentialCitationCount":0,"publicationDate":"05/17/2022","authors":"Davide Locatelli,A. Quattoni","id":"1167b3864046b732cf057b8b05db311e726cadab","summary":"This work augments the popular Geo semantic parsing dataset with alignment annotations and creates Geo-Aligned, and studies the performance of standard seq2seq models on the examples that can be aligned monotonically versus examples that require more complex alignments.","score":3},{"url":"https://www.semanticscholar.org/paper/1bd799cf462f926041dd2fc8fbe4af54bddbf5c5","title":"Translate First Reorder Later: Leveraging Monotonicity in Semantic Parsing","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Francesco Cazzaro,Davide Locatelli,A. Quattoni,X. Carreras","id":"1bd799cf462f926041dd2fc8fbe4af54bddbf5c5","summary":"By means of the monotonic translations, TP OL can learn reliable lexico-logical patterns from aligned data, improving compositional generalization both over conventional seq2seq models, as well as over a recently proposed approach that exploits gold alignments.","score":3},{"url":"https://www.semanticscholar.org/paper/45496cd0b256b75bfbe3bd95890b496069c7821c","title":"Multilingual Compositional Wikidata Questions","venue":"ArXiv","year":2021,"referenceCount":48,"citationCount":5,"influentialCitationCount":3,"publicationDate":null,"authors":"Ruixiang Cui,Rahul Aralikatte,Heather Christine Lent,Daniel Hershcovich","id":"45496cd0b256b75bfbe3bd95890b496069c7821c","summary":"This work proposes a method for creating a multilingual, parallel dataset of question-query pairs, grounded in Wikidata, and introduces such a dataset called CompositionalWikidata Questions (CWQ), and utilizes this data to train and evaluate semantic parsers for Hebrew, Kannada, Chinese and English, to better understand the current strengths and weaknesses of multilingual semantic parsing.","score":3},{"url":"https://www.semanticscholar.org/paper/ad331dce175b1d38d6516455013c1ec0e26e606b","title":"Compositional Generalization in Multilingual Semantic Parsing over Wikidata","venue":"Transactions of the Association for Computational Linguistics","year":2021,"referenceCount":82,"citationCount":1,"influentialCitationCount":0,"publicationDate":"08/07/2021","authors":"Ruixiang Cui,Rahul Aralikatte,Heather Christine Lent,Daniel Hershcovich","id":"ad331dce175b1d38d6516455013c1ec0e26e606b","summary":"A method is proposed for creating a multilingual, parallel dataset of question-query pairs, grounded in Wikidata, and it is used to analyze the compositional generalization of semantic parsers in Hebrew, Kannada, Chinese, and English.","score":3},{"url":"https://www.semanticscholar.org/paper/8008348e87d3904842a2dd230c14b83112e8bf48","title":"Compositional Generalization in Dependency Parsing","venue":"ACL","year":2021,"referenceCount":18,"citationCount":3,"influentialCitationCount":0,"publicationDate":"10/13/2021","authors":"Emily Goodwin,Siva Reddy,T. O’Donnell,Dzmitry Bahdanau","id":"8008348e87d3904842a2dd230c14b83112e8bf48","summary":"This work introduces a gold-standard set of dependency parses for CFQ, and uses this to analyze the behaviour of a state-of-the art dependency parser on the CFQ dataset, finding that increasing compound divergence degrades dependency parsing performance, although not as dramatically as semantic parsing performance.","score":3},{"url":"https://www.semanticscholar.org/paper/2b060b89324c376892a096c84fd14664f7b71710","title":"Understanding Robust Generalization in Learning Regular Languages","venue":"ICML","year":2022,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":"02/20/2022","authors":"Soham Dan,O. Bastani,D. Roth","id":"2b060b89324c376892a096c84fd14664f7b71710","summary":"The empirical results support the hypothesis that auxiliary tasks can enable robust generalization, and theoretically prove that the compositional strategy generalizes significantly better than the end-to-end strategy.","score":3},{"url":"https://www.semanticscholar.org/paper/03eeff98d24383518ce0dacc0b3c4a38b6f1a514","title":"Recursive Decoding: A Situated Cognition Approach to Compositional Generation in Grounded Language Understanding","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/27/2022","authors":"Matthew Setzler,Scott Howland,Lauren A. Phillips","id":"03eeff98d24383518ce0dacc0b3c4a38b6f1a514","summary":"Recursive Decoding (RD) is presented, a novel procedure for training and using seq2seq models, targeted towards decode-side generalization, which yields dramatic improvement on two previously neglected generalization tasks in gSCAN.","score":3},{"url":"https://www.semanticscholar.org/paper/3d5699e7f7e085ad72102859b06fa4884d207e77","title":"Iterative Decoding for Compositional Generalization in Transformers","venue":"ArXiv","year":2021,"referenceCount":27,"citationCount":3,"influentialCitationCount":0,"publicationDate":"10/08/2021","authors":"Luana Ruiz,J. Ainslie,Santiago Ontan'on","id":"3d5699e7f7e085ad72102859b06fa4884d207e77","summary":"This paper introduces iterative decoding, an alternative toseq2seq that improves transformer compositional generalization in the PCFG and Cartesian product datasets and evidences that, in these datasets, seq2seq transformers do not learn iterations that are not unrolled.","score":3},{"url":"https://www.semanticscholar.org/paper/a77468f6bd4db7f8d761a0569d9cc29d5a8f0034","title":"L OGIC I NFERENCE : A N EW D ATASET FOR T EACHING L OGICAL I NFERENCE TO SEQ 2 SEQ M ODELS","venue":"","year":2022,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"Santiago Ontañón,J. Ainslie,V. Cvicek,Zachary Kenneth Fisher","id":"a77468f6bd4db7f8d761a0569d9cc29d5a8f0034","summary":"A new dataset to evaluate the ability of models to perform logical inference using propositional logic and a small subset of ﬁrst-order logic, represented both in semi-formal logical notation, as well as in natural language is presented.","score":3},{"url":"https://www.semanticscholar.org/paper/e528466e2aff981511d4ca6e063211297c0b4175","title":"The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization","venue":"ICLR","year":2021,"referenceCount":60,"citationCount":7,"influentialCitationCount":2,"publicationDate":"10/14/2021","authors":"R. Csordás,Kazuki Irie,J. Schmidhuber","id":"e528466e2aff981511d4ca6e063211297c0b4175","summary":"The novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on a simple arithmetic task and a new variant of ListOps testing for generalization across computational depths.","score":3},{"url":"https://www.semanticscholar.org/paper/5021fd710fd17dee53bc7bc7bf334b148ef3d8b6","title":"LogicInference: A New Dataset for Teaching Logical Inference to seq2seq Models","venue":"ArXiv","year":2022,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/28/2022","authors":"Santiago Ontañón,J. Ainslie,V. Cvicek,Zachary Kenneth Fisher","id":"5021fd710fd17dee53bc7bc7bf334b148ef3d8b6","summary":"A new dataset to evaluate the ability of models to perform logical inference using propositional logic and a small subset of ﬁrst-order logic, represented both in semi-formal logical notation, as well as in natural language is presented.","score":3},{"url":"https://www.semanticscholar.org/paper/837cc9a366c873c84ceec7e84d5cb3d5753757d6","title":"Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/02/2022","authors":"Yuxuan Li,James L. McClelland","id":"837cc9a366c873c84ceec7e84d5cb3d5753757d6","summary":"This work shows that two-layer transformers learn generalizable solutions to multi-level problems and develop signs of systematic task decomposition, and provides key insights into how transformer models may be capable of decomposing complex decisions into reusable, multi- level policies in tasks requiring structured behavior.","score":3},{"url":"https://www.semanticscholar.org/paper/97833e2aa0da5240e62436373b58af988a4ab6ab","title":"The Curious Case of Absolute Position Embeddings","venue":"","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/23/2022","authors":"Koustuv Sinha,Amirhossein Kazemnejad,Siva Reddy,J. Pineau,D. Hupkes,Adina Williams","id":"97833e2aa0da5240e62436373b58af988a4ab6ab","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/79cb080c84da314c2113692585b1e9ee29afa33a","title":"On learning an interpreted language with recurrent models","venue":"","year":2018,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/11/2018","authors":"Denis Paperno","id":"79cb080c84da314c2113692585b1e9ee29afa33a","summary":"This work constructs simplified datasets reflecting core properties of natural language as modeled in formal syntax and semantics: recursive syntactic structure and compositionality, and finds LSTM and GRU networks to generalise to compositional interpretation well, but only in the most favorable learning settings.","score":2},{"url":"https://www.semanticscholar.org/paper/4b58367375466e653751a0c258b2f50bd3551408","title":"Sequence-to-Sequence Networks Learn the Meaning of Reflexive Anaphora","venue":"CRAC","year":2020,"referenceCount":24,"citationCount":2,"influentialCitationCount":0,"publicationDate":"11/02/2020","authors":"R. Frank,Jackson Petty","id":"4b58367375466e653751a0c258b2f50bd3551408","summary":"This paper considers sequence-to-sequence architectures with recurrent units and shows that such networks are capable of learning semantic interpretations for reflexive anaphora which generalize to novel antecedents.","score":2},{"url":"https://www.semanticscholar.org/paper/7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8","title":"AND does not mean OR: Using Formal Languages to Study Language Models’ Representations","venue":"ACL","year":2021,"referenceCount":17,"citationCount":7,"influentialCitationCount":0,"publicationDate":null,"authors":"Aaron Traylor,Roman Feiman,Elizabeth-Jane Pavlick","id":"7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8","summary":"None of the simulated training corpora result in models which definitively differentiate meaningfully different symbols (e.g., AND vs. OR), suggesting a limitation to the types of semantic signals that current models are able to exploit.","score":2},{"url":"https://www.semanticscholar.org/paper/0b1470014bdbaa80ba63da0491d9db6c7d4febcc","title":"Detecting Compositionally Out-of-Distribution Examples in Semantic Parsing","venue":"EMNLP","year":2021,"referenceCount":30,"citationCount":1,"influentialCitationCount":0,"publicationDate":null,"authors":"Denis Lukovnikov,Sina Däubener,Asja Fischer","id":"0b1470014bdbaa80ba63da0491d9db6c7d4febcc","summary":"This work investigates several strong yet simple methods for OOD detection based on predictive uncertainty and shows that these techniques perform well on the standard SCAN and CFQ datasets and can be improved by using a heterogeneous ensemble.","score":2},{"url":"https://www.semanticscholar.org/paper/acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?","venue":"ACL","year":2020,"referenceCount":67,"citationCount":72,"influentialCitationCount":17,"publicationDate":"10/24/2020","authors":"Peter Shaw,Ming-Wei Chang,Panupong Pasupat,Kristina Toutanova","id":"acf8a1040034820bf99379a3422815f4e0859ec9","summary":"NQG-T5 is proposed, a hybrid model that combines a high-precision grammar-based approach with a pre-trained sequence-to-sequence model that outperforms existing approaches across several compositional generalization challenges on non-synthetic data, while also being competitive with the state of theart on standard evaluations.","score":2}]}