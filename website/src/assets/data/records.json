{"papers":[{"url":"https://www.semanticscholar.org/paper/49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks","venue":"ACL","year":2021,"referenceCount":31,"citationCount":23,"influentialCitationCount":3,"publicationDate":"08/09/2021","authors":"Santiago Ontan'on,J. Ainslie,V. Cvicek,Zachary Kenneth Fisher","citations":[{"paperId":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models"},{"paperId":"97833e2aa0da5240e62436373b58af988a4ab6ab","title":"The Curious Case of Absolute Position Embeddings"},{"paperId":"b49ebf36a29cf9734313066129ab0d7092d4041e","title":"Categorizing Semantic Representations for Neural Machine Translation"},{"paperId":"ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","title":"Compositional Generalisation with Structured Reordering and Fertility Layers"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"61d56ece2d19f4bfeb322c92085fb28521e169da","title":"Neural-Symbolic Recursive Machine for Systematic Generalization"},{"paperId":"837cc9a366c873c84ceec7e84d5cb3d5753757d6","title":"Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks"},{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"a122909a31acf41cb2d9eb602c01b24b9b85a061","title":"LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing"},{"paperId":"aa8f3e081ad2869c9469e2726364bdae0d9bdc7f","title":"Fusing finetuned models for better pretraining"},{"paperId":"5021fd710fd17dee53bc7bc7bf334b148ef3d8b6","title":"LogicInference: A New Dataset for Teaching Logical Inference to seq2seq Models"},{"paperId":"69078af65fc934f81fd340e9d1323d6c08194548","title":"Revisiting the Compositional Generalization Abilities of Neural Sequence Models"},{"paperId":"16bf88a6d172699cb9a26a6936efb4941e3f3c13","title":"An Application of Pseudo-Log-Likelihoods to Natural Language Scoring"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"e528466e2aff981511d4ca6e063211297c0b4175","title":"The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization"},{"paperId":"c6dbd97f458c76925363a6b8f6c5e6198163e54e","title":"The Grammar-Learning Trajectories of Neural Language Models"},{"paperId":"a77468f6bd4db7f8d761a0569d9cc29d5a8f0034","title":"L OGIC I NFERENCE : A N EW D ATASET FOR T EACHING L OGICAL I NFERENCE TO SEQ 2 SEQ M ODELS"},{"paperId":"fcf25e1affc2f8ee5bb49d156f174e9769234deb","title":"Systematic Generalization with Edge Transformers"},{"paperId":"00050c15896e8ae6bb534f10d072351547993f72","title":"LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing"},{"paperId":"3d5699e7f7e085ad72102859b06fa4884d207e77","title":"Iterative Decoding for Compositional Generalization in Transformers"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"269de1d1e26559613fa4b02320aefc07bb2d556b","title":"Enhancing the Transformer Decoder with Transition-based Syntax"},{"paperId":"c735740b26ceaa4db9d77233116434c0e8b311d8","title":"Learning Adaptive Control Flow in Transformers for Improved Systematic Generalization"}],"references":[{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"523745e29f6cb1890f18352d449fd3597910c485","title":"Improving Compositional Generalization in Classification Tasks via Structure Annotations"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":null,"title":"Lexicon learning for few-shot neural sequence modeling"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"56676aef356ebb13cba77fc9e4d70760fbc151f5","title":"ETC: Encoding Long and Structured Inputs in Transformers"},{"paperId":"1b04936c2599e59b120f743fbb30df2eed3fd782","title":"Shortcut Learning in Deep Neural Networks"},{"paperId":"71b6394ad5654f5cd0fba763768ba4e523f7bbca","title":"Longformer: The Long-Document Transformer"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","title":"Reformer: The Efficient Transformer"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"f51497f463566581874c941353dd9d80069c5b77","title":"Compressive Transformers for Long-Range Sequence Modelling"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"815a3d56401483b635cfad9468852cddb46350ee","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"f4238bd2385a52413ccbacfd9e409a650235bd13","title":"Adaptive Attention Span in Transformers"},{"paperId":"203b543bfa1e564bb80ff4229b43174d7c71b0c0","title":"HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization"},{"paperId":"21da617a0f79aabf94272107184606cefe90ab75","title":"Generating Long Sequences with Sparse Transformers"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","title":"Self-Attention with Relative Position Representations"},{"paperId":"08fbb1b4cfdc83977d2c8f08bdfb663f13c0e60a","title":"Memorize or generalize? Searching for a compositional RNN in a haystack"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"595c45a6c4fe895e00742f8316710e1177896deb","title":"Diagnostic Classifiers Revealing how Neural Networks Process Hierarchical Structure"},{"paperId":"b71ac1e9fb49420d13e084ac67254a0bbd40f83f","title":"Understanding the difficulty of training deep feedforward neural networks"},{"paperId":"56010a55d49ac1f42355538f494427fd22402be1","title":"Exploring the Limits"},{"paperId":null,"title":"Add AddNeg Reverse Dup Cart Inters SCAN-l SCAN-aj PCFG-p PCFG-s COGS"}],"id":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","summary":"This paper explores the design space of Transformer models showing that the inductive biases given to the model by several design decisions significantly impact compositional generalization."},{"url":"https://www.semanticscholar.org/paper/ab72bccf6f3981537389510ecc609109e79595c3","title":"Disentangled Sequence to Sequence Learning for Compositional Generalization","venue":"ACL","year":2021,"referenceCount":50,"citationCount":8,"influentialCitationCount":0,"publicationDate":"10/09/2021","authors":"Hao Zheng,Mirella Lapata","citations":[{"paperId":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models"},{"paperId":"ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","title":"Compositional Generalisation with Structured Reordering and Fertility Layers"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"cdce13c7d3f344d0eed77a2437ac6ae635369262","title":"A Simple and Unified Tagging Model with Priming for Relational Structure Predictions"},{"paperId":"1d0ec47feac1a7b5c1ec56b7437e032bcd736a39","title":"Blackbird's language matrices (BLMs): a new benchmark to investigate disentangled generalisation in neural networks"},{"paperId":"557ebd17b7c7ac4e09bd167d7b8909b8d74d1153","title":"Compositional Generalization Requires Compositional Parsers"},{"paperId":"03eeff98d24383518ce0dacc0b3c4a38b6f1a514","title":"Recursive Decoding: A Situated Cognition Approach to Compositional Generation in Grounded Language Understanding"},{"paperId":"90c1a63aada7704eadc4324c16a66ec793d4b698","title":"Compositional generalization with a broad-coverage semantic parser"}],"references":[{"paperId":"95c20f35d352f23b19c378c0758b8dc1d7622872","title":"On Aspects of the Theory of Syntax"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","title":"Lexicon Learning for Few Shot Sequence Modeling"},{"paperId":"88dbde378e9ac5c25fc7d78f5da147223e8d34d4","title":"Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention"},{"paperId":"03ad126cfe495933f7bb769f27c03e5f31caedf8","title":"On Compositional Generalization of Neural Machine Translation"},{"paperId":"cc0fc60e9b9d036acf53899c259574c8ce2aedfc","title":"Compositional Generalization via Semantic Tagging"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"e69e5953905b9b9ded4c07f0505ed401ec39babf","title":"Universal Grammar"},{"paperId":"986cc3d0e3afb23f84564ea9588b7b8e9c3e1dd6","title":"Hierarchical Poset Decoding for Compositional Generalization in Language"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"37882abaec01eba1bf5bda8a36c904aaea0d5642","title":"Improving Compositional Generalization in Semantic Parsing"},{"paperId":"84476fdf6ead3553f4493dff8e02308439d6222b","title":"Improve Transformer Models with Better Relative Position Embeddings"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"0b40141779fafcedc28d83bd678807ddb5980df3","title":"The Pitfalls of Simplicity Bias in Neural Networks"},{"paperId":"9fe497d7c5a60806aae431d443aa155a4325661a","title":"Improving Disentangled Text Representation Learning with Information-Theoretic Guidance"},{"paperId":"5d0e2635a1ebe2c9347529975bc876d4286c9ab7","title":"Distributionally Robust Neural Networks"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"753b7a701adc1b6072378bd048cfa8567885d9c7","title":"Invariant Risk Minimization"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"9c5c794094fbf5da8c48df5c3242615dc0b1d245","title":"Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations"},{"paperId":"ea249d6793de488352858f11169bd718914731e1","title":"Disentangled Representation Learning for Non-Parallel Text Style Transfer"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"3e85fbde18cd4d9bb36aa7f227b84f78a2390cd2","title":"Towards Robust Neural Machine Translation"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","title":"Self-Attention with Relative Position Representations"},{"paperId":"2997b26ffb8c291ce478bd8a6e47979d5a55c466","title":"Annotation Artifacts in Natural Language Inference Data"},{"paperId":"04541599accc47d8174f63345ce9c987ef21685b","title":"Disentangling by Factorising"},{"paperId":"6fe99c4969c2f2d3dde8ada84e7388d74eaf0528","title":"Isolating Sources of Disentanglement in Variational Autoencoders"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"97394554eb5a74c3160c6bd743fcd3e4bd6cbe28","title":"LSDSem 2017 Shared Task: The Story Cloze Test"},{"paperId":"a90226c41b79f8b06007609f39f82757073641e2","title":"beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework"},{"paperId":"b7eac64a8410976759445cce235469163d23ee65","title":"Data Recombination for Neural Semantic Parsing"},{"paperId":"85b68477a6e031d88b963833e15a4b4fc6855264","title":"A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories"},{"paperId":"558ac446dc26bee9789d660a251b75728cb6eeb2","title":"Language to Logical Form with Neural Attention"},{"paperId":null,"title":"2016. A corpus"},{"paperId":null,"title":"2016. Data recombination"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":null,"title":"Autoencoding variational bayes"},{"paperId":"184ac0766262312ba76bbdece4e7ffad0aa8180b","title":"Representation Learning: A Review and New Perspectives"},{"paperId":"1c46943103bd7b7a2c7be86859995a4144d1938b","title":"Visualizing Data using t-SNE"},{"paperId":"a6383f155fa9d3e9b15092bfefbf613f982eb263","title":"The Algebraic Mind: Integrating Connectionism and Cognitive Science"},{"paperId":"07d2993d7b5cce0058c29010d5f85d4f2dc02067","title":"Lexical semantics and compositionality."},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":null,"title":"Die Grundlagen der Arithmetik (The Foundations of Arithmetic): eine logisch- mathematische Untersuchung ber den Begriff der Zahl"}],"id":"ab72bccf6f3981537389510ecc609109e79595c3","summary":"An extension to sequence-to-sequence models which encourage disentanglement by adaptively re-encoding (at each time step) the source input by condition the source representations on the newly decoded target context which makes it easier for the encoder to exploit specialized information for each prediction."},{"url":"https://www.semanticscholar.org/paper/d3edc20ed4a07195f3663abc0ead4220266fd75b","title":"*-CFQ: Analyzing the Scalability of Machine Learning on a Compositional Task","venue":"AAAI","year":2020,"referenceCount":58,"citationCount":10,"influentialCitationCount":1,"publicationDate":"12/15/2020","authors":"D. Tsarkov,Tibor Tihon,Nathan Scales,Nikola Momchev,Danila Sinopalnikov,Nathanael Scharli","citations":[{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"2b060b89324c376892a096c84fd14664f7b71710","title":"Understanding Robust Generalization in Learning Regular Languages"},{"paperId":"4b32ccab244cd8a8036c4d780cba81c9d60929ca","title":"Transformer Module Networks for Systematic Generalization in Visual Question Answering"},{"paperId":"8008348e87d3904842a2dd230c14b83112e8bf48","title":"Compositional Generalization in Dependency Parsing"},{"paperId":"ad331dce175b1d38d6516455013c1ec0e26e606b","title":"Compositional Generalization in Multilingual Semantic Parsing over Wikidata"},{"paperId":"fcf25e1affc2f8ee5bb49d156f174e9769234deb","title":"Systematic Generalization with Edge Transformers"},{"paperId":"bb0ab8591d6d57c7e2bd1ec35d806b3f25277752","title":"Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks"},{"paperId":"676fa805bd715591f99bb17e36d673a6a14e92fe","title":"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization"},{"paperId":"45496cd0b256b75bfbe3bd95890b496069c7821c","title":"Multilingual Compositional Wikidata Questions"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"}],"references":[{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"9b4c9f7714fa60d54feb9527c86ba4dfd9adfa51","title":"Building a Multi-domain Neural Machine Translation Model using Knowledge Distillation"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"d28c18a3c2a0afdc0a8634d18345af8d36e1f948","title":"A Constructive Prediction of the Generalization Error Across Scales"},{"paperId":"ae3501afbe8f3f5cf1c5270fa00d0b65fc1c9484","title":"Environmental drivers of systematicity and generalization in a situated agent"},{"paperId":"77d868cf7a5da2b030e8defba9a2b3804c5720b0","title":"Learning a Multi-Domain Curriculum for Neural Machine Translation"},{"paperId":"815a3d56401483b635cfad9468852cddb46350ee","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":"8195787260dfc6bc9abea3b1dac1ce15f747caa2","title":"A Survey of Unsupervised Deep Domain Adaptation"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"4beaabe0c4277ddb850a2f91a20b5fcec84f18af","title":"Emergent Systematic Generalization in a Situated Agent"},{"paperId":"4d031258a66076187001b4d6182345198624d872","title":"The compositionality of neural networks: integrating symbolism and connectionism"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"07fa8c8a703abd7496f4781e9dee53d5de9c8717","title":"Neural Shuffle-Exchange Networks - Sequence Processing in O(n log n) Time"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"636904d91d9dd1a641a595d9578ba7640f35aa74","title":"MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension"},{"paperId":"ec9984003962eb70a73bf0882ab49ef38cd6c239","title":"Curriculum Learning for Domain Adaptation in Neural Machine Translation"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"19b7769dab4e6092aa4b7eeb8aa078a7b725c9b4","title":"Relational inductive biases, deep learning, and graph networks"},{"paperId":"39af58c32e76e875d667804707ea110323207988","title":"A Survey of Domain Adaptation for Neural Machine Translation"},{"paperId":"1b362a75b40a0242bfd7996b02ccc0815edd18df","title":"Multi-Domain Neural Machine Translation"},{"paperId":"642c1b4a9da95ea4239708afc5929a5007a1870d","title":"Tensor2Tensor for Neural Machine Translation"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"a1c922be467d1c0c64b963e65dae41778b81b2a0","title":"Deep Learning Scaling is Predictable, Empirically"},{"paperId":"c80725ad0c0cd06416f3c01a78b7c419359d3fe2","title":"Instance Weighting for Neural Machine Translation Domain Adaptation"},{"paperId":"a5ae9a23a587dee02aedeb3ec271de0da95eabe4","title":"Effective Domain Mixing for Neural Machine Translation"},{"paperId":"8760bc7631c0cb04e7138254e9fd6451b7def8ca","title":"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"bc7fcefa3e333d50463d524406d107060c4a0cec","title":"Neural Semantic Parsing over Multiple Knowledge-bases"},{"paperId":"a21de9f6408b333d917f7a8b2585230ed8b6c57a","title":"How much data is needed to train a medical image deep learning system to achieve necessary high accuracy"},{"paperId":"b4482761879009635e04d170f9f9c70a74f4ba39","title":"A Unified Perspective on Multi-Domain and Multi-Task Learning"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"2826f9dccdcceb113b33ccf2841d488f1419bb30","title":"Stanford Neural Machine Translation Systems for Spoken Language Domains"},{"paperId":"2f31cfb36a8d0c9b2039eee1b5848f618d1a50dc","title":"Predicting sample size required for classification performance"},{"paperId":"6e785a402a60353e6e22d6883d3998940dcaea96","title":"Three Models for the Description of Language"},{"paperId":"7628b62d64d2e5c33a13a5a473bc41b2391c1ebc","title":"Scaling to Very Very Large Corpora for Natural Language Disambiguation"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":null,"title":"Was a movie whose art director and executive producer influenced and was influenced by a film distributor's employee and founder M1? 8. Who was the Dutch sibling and husband of M0?"},{"paperId":null,"title":"Was M2's Hindu spouse a English wife of M3?"},{"paperId":null,"title":"Who was a parent of a company's white Catholic Canadian founder?"},{"paperId":null,"title":"Was M2's child's friend, offspring, parent, wife, and sibling M0's child and parent?"},{"paperId":null,"title":"Was the woman's spouse, sibling, father, and child M2? 17. Who was the child, sibling, parent, and husband of M2's friend and sister?"},{"paperId":null,"title":"Was M2 a daughter, spouse, and sibling of the thriller movie's executive producer, costume designer, director, cinematographer, and star?"},{"paperId":null,"title":"Which Buddhist Mormon male Muslim parent of M6's star was M1's Hindu spouse?"},{"paperId":null,"title":"What female Irish American Indian child of a cinematographer did M2 and M3 employ?"},{"paperId":null,"title":"Was a Hindu costume designer's parent, spouse, and son a Mexican child of the creator and producer of M2?"},{"paperId":null,"title":"Was M3 a actor whose parent and wife distributed and produced M0 and M1?"},{"paperId":null,"title":"Was a Hindu Jewish Chinese Irish American brother of a drama film's producer's child M2?"},{"paperId":null,"title":"Was M2's producer's Buddhist sibling a Jewish TV director's daughter?"},{"paperId":null,"title":"Was M0's art director, costume designer, executive producer, producer, writer, editor, and star M2's mother? 19. Was the man the executive producer of M1, M2, M3, M4, and M5?"},{"paperId":null,"title":"Who was M2's Hindu costume designer's husband, sibling, friend, and parent?"}],"id":"d3edc20ed4a07195f3663abc0ead4220266fd75b","summary":"It is shown that compositional generalization remains a challenge at all training sizes, and that increasing the scope of natural language leads to consistently higher error rates, which are only partially offset by increased training data."},{"url":"https://www.semanticscholar.org/paper/9fe39e0f2cf3b6d5f70a379654f9c08ffa48ddee","title":"Unobserved Local Structures Make Compositional Generalization Hard","venue":"","year":2022,"referenceCount":30,"citationCount":9,"influentialCitationCount":1,"publicationDate":"01/15/2022","authors":"Ben Bogin,Shivanshu Gupta,Jonathan Berant","citations":[{"paperId":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models"},{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"}],"references":[{"paperId":"06fbeaf4d16639f177973a06cd7c4f78cb5e38ed","title":"COVR: A Test-Bed for Visually Grounded Compositional Generalization with Real Images"},{"paperId":"676fa805bd715591f99bb17e36d673a6a14e92fe","title":"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":"df66fcd3fd4f0d55bd96528cddb0e2f08ddb3385","title":"Revisiting Iterative Back-Translation from the Perspective of Compositional Generalization"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"b95184d5eb25b0fe66d8bd1ad1b7677a51c21702","title":"Latent Compositional Representations Improve Systematic Generalization in Grounded Question Answering"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"37882abaec01eba1bf5bda8a36c904aaea0d5642","title":"Improving Compositional Generalization in Semantic Parsing"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"4bc2bb6584774b0d8ad0b4f5215dc2075487c192","title":"A Benchmark for Systematic Generalization in Grounded Language Understanding"},{"paperId":"94f11f6cf04477fb38419f3ec4af097f57abc741","title":"Schema2QA: High-Quality and Low-Cost Q&A Agents for the Structured Web"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"80deaca65c2c155bd15718eeecff584841eb25b0","title":"CLOSURE: Assessing Systematic Generalization of CLEVR Models"},{"paperId":"f906264694759f1beda0cb07d02bf098b98c17bb","title":"Genie: a generator of natural language semantic parsers for virtual assistant commands"},{"paperId":"33ecb49e7b1eb1f44790fb6ceca6eed82cb0c7cd","title":"Jump to better conclusions: SCAN both left and right"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"ba30df190664193514d1d309cb673728ed48f449","title":"Incorporating Copying Mechanism in Sequence-to-Sequence Learning"},{"paperId":"25369f56a933e3bfb1d8e1588cdc6c50df93ecae","title":"Building a Semantic Parser Overnight"},{"paperId":"3ecd3e00bbbfd94446c3adc9c6878de27e250f7c","title":"Learning Dependency-Based Compositional Semantics"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"6dae782f8be8fe5c25dbf2d0d681b3f708bf1a32","title":"Expanding the Scope of the ATIS Task: The ATIS-3 Corpus"},{"paperId":"1d19708290ef3cc3f43c2c95b07acdd4f52f5cda","title":"The ATIS Spoken Language Systems Pilot Corpus"}],"id":"9fe39e0f2cf3b6d5f70a379654f9c08ffa48ddee","summary":"A criterion for the difficulty of an example is proposed: a test instance is hard if it contains a local structure that was not observed at training time and it predicts instance-level generalization well across 5 different semantic parsing datasets, substantially better than alternative decision rules."},{"url":"https://www.semanticscholar.org/paper/40047a74b707743157051d38f76061ba5ff9aab4","title":"Compositional Semantic Parsing with Large Language Models","venue":"ArXiv","year":2022,"referenceCount":62,"citationCount":3,"influentialCitationCount":0,"publicationDate":"09/29/2022","authors":"Andrew Drozdov,Nathanael Scharli,Ekin Akyuurek,Nathan Scales,Xinying Song,Xinyun Chen,O. Bousquet,Denny Zhou","citations":[{"paperId":"cca80d8d5edb0513b5ac744e6854744fa4aeb4b8","title":"Transcending Scaling Laws with 0.1% Extra Compute"},{"paperId":"663a41c866d49ce052801fbc88947d39764cad29","title":"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"},{"paperId":"95add249447a03653fc76921efc3691be1dcd204","title":"Large Language Models are few(1)-shot Table Reasoners"}],"references":[{"paperId":"d429373fb5190778856da933564d0d52092cc9bd","title":"Emergent Abilities of Large Language Models"},{"paperId":"4288d44c2b8e6a89607780caf1272061028f6f97","title":"On the Advance of Making Language Models Better Reasoners"},{"paperId":"1ee6a0b12264001c65b83536efcddac7f27e8571","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"f75a0ccb3c9b7e12dfb8b9c9dc3d35ca4518a643","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"51c2a0835fc565ad1fc7a58559ede9cbe8f6551e","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"716f9d0f6e96f437e127de90c87f7b2f7a6c8f12","title":"SeqZero: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models"},{"paperId":"a40693eefd351659cdeb3885917b1506ea01c38a","title":"Measuring and Improving Compositional Generalization in Text-to-SQL via Component Alignment"},{"paperId":"bcd4c46e4d75ddedb6138cfd77600c6d964a9aa8","title":"Natural Language to Code Translation with Execution"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"cb5e3f085caefd1f3d5e08637ab55d39e61234fc","title":"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"},{"paperId":"23dd78e424d32f6a48660dcd67ce994b8a7db8be","title":"STaR: Bootstrapping Reasoning With Reasoning"},{"paperId":"79b88230fabb59a1d368641bbc822af0f09bf262","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"5d0db797a45ce2453f821f7ded0b547d3fdab054","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a","title":"Few-Shot Semantic Parsing with Language Models Trained on Code"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"7cc74ffa1215321712d4a830bb9dee19d9f0fb47","title":"Grounded Graph Decoding Improves Compositional Generalization in Question Answering"},{"paperId":"b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347","title":"Sequence-to-Sequence Learning with Latent Neural Grammars"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","title":"Lexicon Learning for Few Shot Sequence Modeling"},{"paperId":"88dbde378e9ac5c25fc7d78f5da147223e8d34d4","title":"Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention"},{"paperId":"64a1dbdd7653eaca25c78e87335ee156b6f6959e","title":"Constrained Language Models Yield Few-Shot Semantic Parsers"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":"62d1a3137b01a69443bebf4d92c1990ec512a6a1","title":"Extracting Training Data from Large Language Models"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"e69e5953905b9b9ded4c07f0505ed401ec39babf","title":"Universal Grammar"},{"paperId":"986cc3d0e3afb23f84564ea9588b7b8e9c3e1dd6","title":"Hierarchical Poset Decoding for Compositional Generalization in Language"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"937fef6a786c4463a3bb19770c704945d1600b66","title":"Learning Compositional Rules via Neural Program Synthesis"},{"paperId":"4bc2bb6584774b0d8ad0b4f5215dc2075487c192","title":"A Benchmark for Systematic Generalization in Grounded Language Understanding"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"ac713aebdcc06f15f8ea61e1140bb360341fdf27","title":"Thieves on Sesame Street! Model Extraction of BERT-based APIs"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"80deaca65c2c155bd15718eeecff584841eb25b0","title":"CLOSURE: Assessing Systematic Generalization of CLEVR Models"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"32c9a0acee8d236c553395052c29a6d853d8ea2d","title":"Compositional Generalization in Image Captioning"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"210feb22ff541920caa4884e73eaff1c09644114","title":"Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"03eb382e04cca8cca743f7799070869954f1402a","title":"CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"},{"paperId":"7260c0692f8d265e11c4e9c4c8ef4c185bd587ad","title":"Building machines that learn and think like people"},{"paperId":"c2ecc66c0e5f976b0e0d95c64ed2d1e283a2625d","title":"Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus"},{"paperId":"6843890926bf0e5c887ffc78dcb1203135981bf1","title":"The compositionality papers"},{"paperId":"d65f4afadac11cd2ec6f0b14f244060c387926ce","title":"An introduction to unification-based approaches to grammar"},{"paperId":null,"title":"Syntactic structures. The Hague: Mouton"},{"paperId":null,"title":"?x0 produced M1 . ?x0 wrote M0 } Question: Did M2 's editor , director , star , art director , and cinematographer produce , direct , and edit M0 and M1 Answer: SELECT count(*) WHERE { ?x0 edited M0"},{"paperId":null,"title":"Static prompt context illustrating the composition of subclauses and prepositional phrases 2. Dynamically selected exemplars as additional context 3"},{"paperId":null,"title":"Adds sequential least-to-most prompting, including subproblems in the prompt that were used to select the exemplars"},{"paperId":null,"title":"Chain-of-thought (CoT) grounding. A constant prompt prefix with basic subproblems solved in a chain-of-thought-like way. This corresponds to the full implementation of dynamic least-to-most prompting"},{"paperId":null,"title":"3 COGS SOLUTION: DETAILS AND PROMPTS"},{"paperId":null,"title":"The * raisin) (was frozen [freeze]) PARSE: freeze ( theme = * raisin ) DONE"},{"paperId":null,"title":"Same as above, but adds brackets to sentences indicating compositional structure and hinting at relations between exemplars (see Appendix A for examples"},{"paperId":null,"title":"These parts are detailed throughout the rest of this section. input: Who was influenced by M1 , influenced by M4 's producer , cinematographer , and director"}],"id":"40047a74b707743157051d38f76061ba5ff9aab4","summary":"The best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse."},{"url":"https://www.semanticscholar.org/paper/6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization","venue":"FINDINGS","year":2021,"referenceCount":59,"citationCount":13,"influentialCitationCount":5,"publicationDate":"07/14/2021","authors":"Chenyao Liu,Shengnan An,Zeqi Lin,Qian Liu,Bei Chen,Jian-Guang Lou,L. Wen,Nanning Zheng,Dongmei Zhang","citations":[{"paperId":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models"},{"paperId":"04ca71d9089b2e86f9e4a874fd66ee8bd0baba8e","title":"When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks"},{"paperId":"1bd799cf462f926041dd2fc8fbe4af54bddbf5c5","title":"Translate First Reorder Later: Leveraging Monotonicity in Semantic Parsing"},{"paperId":"ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","title":"Compositional Generalisation with Structured Reordering and Fertility Layers"},{"paperId":"40047a74b707743157051d38f76061ba5ff9aab4","title":"Compositional Semantic Parsing with Large Language Models"},{"paperId":"a122909a31acf41cb2d9eb602c01b24b9b85a061","title":"LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing"},{"paperId":"1167b3864046b732cf057b8b05db311e726cadab","title":"Measuring Alignment Bias in Neural Seq2seq Semantic Parsers"},{"paperId":"557ebd17b7c7ac4e09bd167d7b8909b8d74d1153","title":"Compositional Generalization Requires Compositional Parsers"},{"paperId":"39f604fdd3ade5bd5a67d5284a6d9c12e535db85","title":"Compositionality as Lexical Symmetry"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"90c1a63aada7704eadc4324c16a66ec793d4b698","title":"Compositional generalization with a broad-coverage semantic parser"},{"paperId":"c764ecba2bace12b9bfb9c2b0651a12ff6888ea7","title":"Learning to Generalize Compositionally by Transferring Across Semantic Parsing Tasks"},{"paperId":"00050c15896e8ae6bb534f10d072351547993f72","title":"LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing"}],"references":[{"paperId":"d3edc20ed4a07195f3663abc0ead4220266fd75b","title":"*-CFQ: Analyzing the Scalability of Machine Learning on a Compositional Task"},{"paperId":"7344ed64d1717780422fd1d58fae85edc544d180","title":"Iterative Utterance Segmentation for Neural Semantic Parsing"},{"paperId":"df66fcd3fd4f0d55bd96528cddb0e2f08ddb3385","title":"Revisiting Iterative Back-Translation from the Perspective of Compositional Generalization"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"e69e5953905b9b9ded4c07f0505ed401ec39babf","title":"Universal Grammar"},{"paperId":"106fb432d2b62f3824a9d6f4a1b30e1f8b6ea9d7","title":"Sequence-level Mixed Sample Data Augmentation"},{"paperId":"4bc9d6596069c9277b57a7ee1e1127d231f28663","title":"Unsupervised Parsing with S-DIORA: Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders"},{"paperId":"986cc3d0e3afb23f84564ea9588b7b8e9c3e1dd6","title":"Hierarchical Poset Decoding for Compositional Generalization in Language"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"07fa8c8a703abd7496f4781e9dee53d5de9c8717","title":"Neural Shuffle-Exchange Networks - Sequence Processing in O(n log n) Time"},{"paperId":"7ea59779ffb392f099d5304680126b4299f43750","title":"Compound Probabilistic Context-Free Grammars for Grammar Induction"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"b0e2fe0fe9f4fc4ce05d5f637baff96a7e966c01","title":"Cooperative Learning of Disjoint Syntax and Semantics"},{"paperId":"16c844fd4d97f3c6eb38b0d6527c87d184efedc3","title":"The Evolved Transformer"},{"paperId":"15d6f3d815d0ff176fafb14a3f46e5723ebac723","title":"Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"027f9695189355d18ec6be8e48f3d23ea25db35d","title":"Learning to Compose Task-Specific Tree Structures"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"e1a25af38d1f8ba9b2b5c3256973d6cf4fe00559","title":"Grammar induction from (lots of) words alone"},{"paperId":"387f65f42b26479169bab9e58ee55f838cc31197","title":"Imitation Learning of Agenda-based Semantic Parsers"},{"paperId":"b41e95c8c97846d5ca4c11ef79d7814499cc9663","title":"Compositional Semantic Parsing on Semi-Structured Tables"},{"paperId":"32de44f01a96d4473d21099d15e25bc2b9f08e2f","title":"Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"b29447ba499507a259ae9d8f685d60cc1597d7d3","title":"Semantic Parsing on Freebase from Question-Answer Pairs"},{"paperId":"8729441d734782c3ed532a7d2d9611b438c0a09a","title":"ADADELTA: An Adaptive Learning Rate Method"},{"paperId":null,"title":"φa + γ ·E R(τ)∇ log πφa (ga|x, z, gl)"},{"paperId":"4004494592b288e135602146ab290148c8ff58d8","title":"Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models"},{"paperId":"3ecd3e00bbbfd94446c3adc9c6878de27e250f7c","title":"Learning Dependency-Based Compositional Semantics"},{"paperId":"8de174ab5419b9d3127695405efd079808e956e8","title":"Curriculum learning"},{"paperId":"42a9c575acb53fac332993087c1e1dbcc8161ccd","title":"An All-Subtrees Approach to Unsupervised Parsing"},{"paperId":"74fe7ec751cd50295b15cfd46389a8fefb37c414","title":"Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars"},{"paperId":"8dd9fd6a45afd266d48255c398429e01ea4fd6db","title":"Learning to Transform Natural to Formal Languages"},{"paperId":"4c915c1eecb217c123a36dc6d3ce52d12c742614","title":"Simple statistical gradient-following algorithms for connectionist reinforcement learning"},{"paperId":"622042fca4fe303178c87b934e6cc1e5459f33c4","title":"Context Dependence and Compositionality"},{"paperId":"de2df29b0a0312de7270c3f5a0af6af5645cf91a","title":"A Systematic Comparison of Various Statistical Alignment Models"},{"paperId":"97f95249ce73188a9488de160ddd0410d5c4fb5f","title":"MEANING POTENTIALS AND CONTEXT : SOME CONSEQUENCES FOR THE ANALYSIS OF VARIATION IN MEANING"},{"paperId":"6843890926bf0e5c887ffc78dcb1203135981bf1","title":"The compositionality papers"},{"paperId":"4d92df4a844c94fbb31b95157488e4b562b4f681","title":"The Optimal Reward Baseline for Gradient-Based Reinforcement Learning"},{"paperId":"5feb019135e705641380ec0dd9be8cc7bdcd973b","title":"Natural Language Grammar Induction Using a Constituent-Context Model"},{"paperId":"a6383f155fa9d3e9b15092bfefbf613f982eb263","title":"The Algebraic Mind: Integrating Connectionism and Cognitive Science"},{"paperId":"a20f0ce0616def7cc9a87446c228906cd5da093b","title":"Policy Gradient Methods for Reinforcement Learning with Function Approximation"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"b7c0e47f8b768258b7d536c21b218e6c46ab8791","title":"Learning to Parse Database Queries Using Inductive Logic Programming"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":null,"title":"Syntactic structures (the hague: Mouton, 1957)"},{"paperId":null,"title":"Joshua liked that Mason hoped that Amelia awarded the hedgehog beside the stage in the tent to a cat\". Figure 6: Examples of generated tree-structures and semantics in CFQ and COGS benchmarks"},{"paperId":null,"title":"An example of generated results in CFQ benchmark with the input \"Did M6' s star, costume designer, and director influence M0, M1, M2, and M3 and influence M4 and M5"}],"id":"6d00b1024298e5b64ee873028385f7bb4396b05d","summary":"This paper proposes LEAR, an end-toend neural model to learn algebraic recombination for compositional generalization, to model the semantic parsing task as a homomorphism between a latent syntactic algebra and a semantic algebra, thus encouraging algebraic rewriting."},{"url":"https://www.semanticscholar.org/paper/856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks","venue":"ICML","year":2017,"referenceCount":50,"citationCount":463,"influentialCitationCount":74,"publicationDate":"10/31/2017","authors":"B. Lake,Marco Baroni","citations":[{"paperId":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models"},{"paperId":"04ca71d9089b2e86f9e4a874fd66ee8bd0baba8e","title":"When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks"},{"paperId":"97833e2aa0da5240e62436373b58af988a4ab6ab","title":"The Curious Case of Absolute Position Embeddings"},{"paperId":"798abf86efae9e37b9b6a694ef87b6c1dbaab263","title":"Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models"},{"paperId":"c6e4518dfd687a2a5bed4e78d5d9f999292a1746","title":"Counterfactual Recipe Generation: Exploring Compositional Generalization in a Realistic Scenario"},{"paperId":"a4f1793b23a7f7ec3d65824d6ca3e6011567980d","title":"ELASTIC: Numerical Reasoning with Adaptive Symbolic Compiler"},{"paperId":"a638036c48cbd76ae3af30d6b273d49bb28a22e4","title":"Schrödinger's tree—On syntax and neural language models"},{"paperId":"6494c6149e5036c09ee92da9fd67cbecc998a52f","title":"PCFG-based Natural Language Interface Improves Generalization for Controlled Text Generation"},{"paperId":"a3cddf0b160b1d9b585e74bc114b403fc8590264","title":"Equi-Tuning: Group Equivariant Fine-Tuning of Pretrained Models"},{"paperId":"1ed29beb55b10de8553c926ce6da2625ec2c8776","title":"Benchmarking Long-tail Generalization with Likelihood Splits"},{"paperId":"b49ebf36a29cf9734313066129ab0d7092d4041e","title":"Categorizing Semantic Representations for Neural Machine Translation"},{"paperId":"b1f33e956e36bf25e118c0d537dcc519cfe52e60","title":"CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations"},{"paperId":"1bd799cf462f926041dd2fc8fbe4af54bddbf5c5","title":"Translate First Reorder Later: Leveraging Monotonicity in Semantic Parsing"},{"paperId":"eac5e5ef2bb1d158645ee8ae8f3e167767316b46","title":"Understanding and Improving Zero-shot Multi-hop Reasoning in Generative Question Answering"},{"paperId":"0828722a8317a556c8753cfe1a8cf3a3eec0004f","title":"Measuring and Narrowing the Compositionality Gap in Language Models"},{"paperId":"ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","title":"Compositional Generalisation with Structured Reordering and Fertility Layers"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"61d56ece2d19f4bfeb322c92085fb28521e169da","title":"Neural-Symbolic Recursive Machine for Systematic Generalization"},{"paperId":"efbfca5e8f922a9d804c388719ebf8fcf07bd0de","title":"Curriculum learning for human compositional generalization"},{"paperId":"837cc9a366c873c84ceec7e84d5cb3d5753757d6","title":"Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks"},{"paperId":"40047a74b707743157051d38f76061ba5ff9aab4","title":"Compositional Semantic Parsing with Large Language Models"},{"paperId":"ff4e8de2705773c2dd38365a28ffe05b32706efb","title":"Equivariant Transduction through Invariant Alignment"},{"paperId":"a265394d782bbdb869730775399be3dfe45fc1db","title":"Compositional generalization through abstract representations in human and artificial neural networks"},{"paperId":"60cd45dc61bc41456e00a5f48d8589674cc9fdf7","title":"Trust in Language Grounding: a new AI challenge for human-robot teams"},{"paperId":"60f208b19bb63d82fda5759897677f92d4e1e2fc","title":"Improving Compositional Generalization in Math Word Problem Solving"},{"paperId":"d6236d1ad7e5678c3537f8d74f63e678b7015250","title":"PU-GEN: Enhancing generative commonsense reasoning for language models with human-centered knowledge"},{"paperId":"c4899af5e101b84ada1acfed60ec90eb5113f4a5","title":"On a Built-in Conflict between Deep Learning and Systematic Generalization"},{"paperId":"01b2f7601ab3df0d2982a204e2fb309f6622646f","title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models"},{"paperId":"13babb4aa168b26d5330775dd87582a0e0efad4f","title":"Learning Transductions to Test Systematic Compositionality"},{"paperId":"a6be79aa22eaeef05caae66aab924bcb4e07cfa8","title":"Benchmarking Compositionality with Formal Languages"},{"paperId":"1a7a24c73521eecf0a2d555e921b27e2c4d8e3c3","title":"What Artificial Neural Networks Can Tell Us About Human Language Acquisition"},{"paperId":"74cc3d340039c67bdabaef090d1386fe2c5376ca","title":"CLEVR-Math: A Dataset for Compositional Language, Visual and Mathematical Reasoning"},{"paperId":"22f9a5fe8e446d215530fb90ea08b10499b36b0b","title":"Unit Testing for Concepts in Neural Networks"},{"paperId":"294b7f9927f10e944a3d33f548d9b92792f5df84","title":"Interpolation, extrapolation, and local generalization in common neural networks"},{"paperId":"d045f97124c9fb64f5d5f4d77da85eb4df502667","title":"Meta-Referential Games to Learn Compositional Learning Behaviours"},{"paperId":"9337d750993d8715c872db8d406480d58464555a","title":"How to Reuse and Compose Knowledge for a Lifetime of Tasks: A Survey on Continual Learning and Functional Composition"},{"paperId":"8e21576387f46f1b9090bdbff1ceadf187feeada","title":"CompoSuite: A Compositional Reinforcement Learning Benchmark"},{"paperId":"7821736570438c2d4db4e9cb4e0c04182b20b605","title":"Inferring the nature of linguistic computations in the brain"},{"paperId":"0b3ca2a700085a877c560e20558566536f18d5a2","title":"Modular Lifelong Reinforcement Learning via Neural Composition"},{"paperId":"196d1820f48e8fc6fc5275adb76d4c8730359917","title":"A Benchmark for Compositional Visual Reasoning"},{"paperId":"60584b10f794ed74fac7990b8fcbb7f4ff93d73e","title":"Defending Compositionality in Emergent Languages"},{"paperId":"e10ed48cceca216d8ac43113c0562cf340dbdce3","title":"Unveiling Transformers with LEGO: a synthetic reasoning task"},{"paperId":"ba6ff2c50862a6be18d1c610d1fadc751fd6d8e7","title":"Few-shot Subgoal Planning with Language Models"},{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"51c2a0835fc565ad1fc7a58559ede9cbe8f6551e","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"a122909a31acf41cb2d9eb602c01b24b9b85a061","title":"LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing"},{"paperId":"1167b3864046b732cf057b8b05db311e726cadab","title":"Measuring Alignment Bias in Neural Seq2seq Semantic Parsers"},{"paperId":"e184d1113060bbeec171b2faf0db0a4022515859","title":"TreeMix: Compositional Constituency-based Data Augmentation for Natural Language Understanding"},{"paperId":"bc16284f517dd0011dcf64ea1c8fe6d6576494a4","title":"Is the Computation of Abstract Sameness Relations Human-Like in Neural Language Models?"},{"paperId":"a40693eefd351659cdeb3885917b1506ea01c38a","title":"Measuring and Improving Compositional Generalization in Text-to-SQL via Component Alignment"},{"paperId":"b06c41432dc060c5591b91f8ed40ae15913a150d","title":"SUBS: Subtree Substitution for Compositional Semantic Parsing"},{"paperId":"f2611a09cf0942170785ee3025cb511de3bdec2e","title":"Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems"},{"paperId":"919cebaad8a2f98ec5d6fb540f7dd17bac9e9ef8","title":"Toward Compositional Generalization in Object-Oriented World Modeling"},{"paperId":"106b55f668eb9be1c2558095b22251c1be23618b","title":"Counterfactual Explanations for Natural Language Interfaces"},{"paperId":"cb16b85891172572cd856142880b503db0c2bc61","title":"What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment"},{"paperId":"997410e2bf80f25f73752dd6fd7122227385ed2d","title":"Measuring Compositional Consistency for Video Question Answering"},{"paperId":"6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis"},{"paperId":"0ea06fdde94a7d5e8a49a0b4f68799dd102d7a03","title":"Learning to Compose Soft Prompts for Compositional Zero-Shot Learning"},{"paperId":"0d156c24aa1adba4d04b0b4717aba547887a21af","title":"Disentangling Abstraction from Statistical Pattern Matching in Human and Machine Learning"},{"paperId":"376726c82ccc30f59d01a7c609ffbda6db42abeb","title":"Learning Reduplication with a Neural Network that Lacks Explicit Variables"},{"paperId":"5021fd710fd17dee53bc7bc7bf334b148ef3d8b6","title":"LogicInference: A New Dataset for Teaching Logical Inference to seq2seq Models"},{"paperId":"260a57327415c0a498f0b27da9e4311fa78902c6","title":"Compositional Temporal Grounding with Structured Variational Cross-Graph Correspondence Learning"},{"paperId":"59494dcb572cebb577a1bcb2d6f87dfca93d6591","title":"Differentiable Reasoning over Long Stories - Assessing Systematic Generalisation in Neural Models"},{"paperId":"66f3f0e8ebc780e570770986f50bf9cb9cd53ec1","title":"WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series Tasks"},{"paperId":"4c430e6c3a72626bd4cb1893960c7c26dfec6c79","title":"Structurally Diverse Sampling Reduces Spurious Correlations in Semantic Parsing Datasets"},{"paperId":"e0b63d7870fd2f6aeee4d06ae8c0e849846df59f","title":"Zipfian environments for Reinforcement Learning"},{"paperId":"69078af65fc934f81fd340e9d1323d6c08194548","title":"Revisiting the Compositional Generalization Abilities of Neural Sequence Models"},{"paperId":"1d41a0ddda57caa6c8d268dd1703e4c9b35db18b","title":"One-Shot Learning from a Demonstration with Hierarchical Latent Language"},{"paperId":"bd4cff8ae98125958e865531150c4a8ad66153d0","title":"Neuro-symbolic Natural Logic with Introspective Revision for Natural Language Inference"},{"paperId":"557ebd17b7c7ac4e09bd167d7b8909b8d74d1153","title":"Compositional Generalization Requires Compositional Parsers"},{"paperId":"69df5b68fbf492341336b39b4cc9fcc74fff4d5f","title":"Improving Systematic Generalization Through Modularity and Augmentation"},{"paperId":"2b060b89324c376892a096c84fd14664f7b71710","title":"Understanding Robust Generalization in Learning Regular Languages"},{"paperId":"e824da3543603c99da0066469d7fd413059a3c68","title":"Do Transformers use variable binding?"},{"paperId":"39f604fdd3ade5bd5a67d5284a6d9c12e535db85","title":"Compositionality as Lexical Symmetry"},{"paperId":"03eeff98d24383518ce0dacc0b3c4a38b6f1a514","title":"Recursive Decoding: A Situated Cognition Approach to Compositional Generation in Grounded Language Understanding"},{"paperId":"4b32ccab244cd8a8036c4d780cba81c9d60929ca","title":"Transformer Module Networks for Systematic Generalization in Visual Question Answering"},{"paperId":"91a25facc0829c320f274e0196b8abc58b72f035","title":"Learning Invariable Semantical Representation from Language for Extensible Policy Generalization"},{"paperId":"9fe39e0f2cf3b6d5f70a379654f9c08ffa48ddee","title":"Unobserved Local Structures Make Compositional Generalization Hard"},{"paperId":"ace2a00425f96e9d0dbbe2869023d56c6c91267f","title":"On Learning Interpreted Languages with Recurrent Models"},{"paperId":"790f75e70a42c661b60b5d929c6e48a7f405a88c","title":"Data-driven Model Generalizability in Crosslinguistic Low-resource Morphological Segmentation"},{"paperId":"3743a90fa56345d92a07b18400c0e513e2fdc3cd","title":"Does entity abstraction help generative Transformers reason?"},{"paperId":"c3e7f95e7d1e37003bbef8dca1aee6dcf05a5c16","title":"Learning Bounded Context-Free-Grammar via LSTM and the Transformer: Difference and Explanations"},{"paperId":"63f17017257063ee034c4082d93005dc4b25d42d","title":"Pushing the Limits of Rule Reasoning in Transformers through Natural Language Satisfiability"},{"paperId":"0b483b550b21ec42d693fc04a372dbb10dd07019","title":"Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"40b4d98588719407fb72a014ab79e4145695654b","title":"Quantifying Adaptability in Pre-trained Language Models with 500 Tasks"},{"paperId":"8a221ca3a48727ccaec03133c8973469ef66e92b","title":"Neurosymbolic Systems of Perception and Cognition: The Role of Attention"},{"paperId":"ba2c5e980733378cc4e63fc34e6e2b54331c3809","title":"Building human-like communicative intelligence: A grounded perspective"},{"paperId":"ccd04c27bf1237368b35eb456b3dd1c18ef9a9b9","title":"Inducing Causal Structure for Interpretable Neural Networks"},{"paperId":"37c8f0f4915b68f94669d7eeb51b4785b35c70de","title":"Dyna-bAbI: unlocking bAbI’s potential with dynamic synthetic benchmarking"},{"paperId":"ea3fa9fb842e055c3c5e231838b548c7aaa90fa2","title":"NeuroLISP: High-level symbolic programming with attractor neural networks"},{"paperId":"b8b813111c411ae61881ab9cd25707d9de6444ec","title":"Compositional Attention: Disentangling Search and Retrieval"},{"paperId":"daa5ca0a39ecec8ab5c534196eca526bafe41051","title":"Illiterate DALL-E Learns to Compose"},{"paperId":"dbeff5429ff0caa85f9e02621928e787e789ca2b","title":"Hey AI, Can You Solve Complex Tasks by Talking to Agents?"},{"paperId":"6a9394e5d49c1251c0fb6d7fb0c0813d26c6a907","title":"On The Ingredients of an Effective Zero-shot Semantic Parser"},{"paperId":"6bd91a3183ddb844641acb9f3fe9faec6a9ff617","title":"Meta-learning via Language Model In-context Tuning"},{"paperId":"e528466e2aff981511d4ca6e063211297c0b4175","title":"The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization"},{"paperId":"8008348e87d3904842a2dd230c14b83112e8bf48","title":"Compositional Generalization in Dependency Parsing"},{"paperId":"ab72bccf6f3981537389510ecc609109e79595c3","title":"Disentangled Sequence to Sequence Learning for Compositional Generalization"},{"paperId":"a38253162bdaab2c6d0ae7b3e295913b982c7c87","title":"Distinguishing rule- and exemplar-based generalization in learning systems"},{"paperId":"00ef52092ef3f109a09b66037707cd3227accb42","title":"Challenges in Generalization in Open Domain Question Answering"},{"paperId":"b3f644a5ea1fdd8cec1c34ebed69125838a50de3","title":"The Paradox of the Compositionality of Natural Language: A Neural Machine Translation Case Study"},{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"ad331dce175b1d38d6516455013c1ec0e26e606b","title":"Compositional Generalization in Multilingual Semantic Parsing over Wikidata"},{"paperId":"99425e899f9cfca016e87fe38e044df544215e53","title":"The Structure of Systematicity in the Brain"},{"paperId":"61b1171efcf0242eb011816de1aa415f4262c55a","title":"Visual Representation Learning Does Not Generalize Strongly Within the Same Domain"},{"paperId":"7e38476342ce1fcc8ef0dcd23686539395961769","title":"Inductive biases for deep learning of higher-level cognition"},{"paperId":"2aa1d4350e80613feed88d5a6337e79693f7aa57","title":"KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base"},{"paperId":"f8f00ac17b8facf855f1ecf851ddb93240469c28","title":"Hangul Fonts Dataset: a Hierarchical and Compositional Dataset for Interrogating Learned Representations"},{"paperId":"db0fedb5be998be95ef2f33d327e10ab16f79735","title":"A Dog Is Passing Over The Jet? A Text-Generation Dataset for Korean Commonsense Reasoning and Evaluation"},{"paperId":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"DECOMPOSITION IN NEURAL PROGRAM SYNTHESIS"},{"paperId":"90c1a63aada7704eadc4324c16a66ec793d4b698","title":"Compositional generalization with a broad-coverage semantic parser"},{"paperId":"3e60cba99b4e8a45f2e3ba3df462ac949a720833","title":"Towards Collaborative Neural-Symbolic Graph Semantic Parsing via Uncertainty"},{"paperId":"7732a48dc3f66b857d42ba5d2bb6309daf216c75","title":"Experiments in Learning Dyck-1 Languages with Recurrent Neural Networks"},{"paperId":"6ada57ec94cd8712cd140f0741c6df0bc01e7509","title":"Assessing Combinational Generalization of Language Models in Biased Scenarios"},{"paperId":"44772fe1c3fa422a3da7e25092db2544893d6bfb","title":"Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming"},{"paperId":"bbaee85fe38122e4691a78ac23d4a6a751c357e6","title":"Learning Proof Path Selection Policies in Neural Theorem Proving"},{"paperId":"bb494b7d150ef15a45de24a7f02560c7fae3751f","title":"Towards Understanding and Improving the Generalization Performance of Neural Networks"},{"paperId":"6299cdda42d6c2a960f890b92e10924e01a0d0ca","title":"Compositional Generalization for Kinship Prediction through Data Augmentation"},{"paperId":"d129841cb2e30e25000dcd9edb83c880fc4babc1","title":"Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention"},{"paperId":"ff6d561552db2e6d113df35504dbef51a5cd8e28","title":"COAT: Measuring Object Compositionality in Emergent Representations"},{"paperId":"3207b6820785e243fdc402ce43ff5e93e4b1a92d","title":"Un algorithme d’analyse sémantique fondée sur les graphes via le problème de l’arborescence généralisée couvrante (A graph-based semantic parsing algorithm via the generalized spanning arborescence problem)"},{"paperId":"c7572eb872d032a180925be77b9ebd402c9362de","title":"O BJECT - CENTRIC C OMPOSITIONAL I MAGINATION FOR V ISUAL A BSTRACT R EASONING"},{"paperId":"0fdf910a78b153b76b813408636744f49c31d2eb","title":"C ATEGORIAL G RAMMAR I NDUCTION AS A C OMPOSI TIONALITY M EASURE FOR E MERGENT L ANGUAGES IN S IGNALING G AMES"},{"paperId":"123434be58eb21d41525171130b14679a4cc0010","title":"A GENT , DO YOU SEE IT NOW ? SYSTEMATIC GENERALI SATION IN DEEP REINFORCEMENT LEARNING"},{"paperId":"a143cac1bc440135b612132c89e603f364b8a3b7","title":"Combine to Describe: Evaluating Compositional Generalization in Image Captioning"},{"paperId":"474c954231413f2a249f04272dbeda19cd8ff09b","title":"Bridging the Generalization Gap in Text-to-SQL Parsing with Schema Expansion"},{"paperId":"dc88d2bbcebd810d7c80ba281739908005b12235","title":"Neurocompositional computing in human and machine intelligence: A tutorial"},{"paperId":"a77468f6bd4db7f8d761a0569d9cc29d5a8f0034","title":"L OGIC I NFERENCE : A N EW D ATASET FOR T EACHING L OGICAL I NFERENCE TO SEQ 2 SEQ M ODELS"},{"paperId":"b581c4973f6106a4597270483518e9c6f25924b2","title":"Deep Learning for Object Detection and Segmentation in Videos: Toward an Integration With Domain Knowledge"},{"paperId":"2c749751960af3a920ff04beaff62596442baca7","title":"Grounding Spatio-Temporal Language with Transformers"},{"paperId":"2fea588313c899ac512bb90f77b7ce9d31db421d","title":"Grammar-Based Grounded Lexicon Learning"},{"paperId":"97a0df299077a95de002c353965a2b6157f9e8d2","title":"Discrete and continuous representations and processing in deep learning: Looking forward"},{"paperId":"67b283f83c8730101019d3f0ec690d43a2602443","title":"Towards Interactive Language Modeling"},{"paperId":"f565a79ea8b29fc537dc317e7f38c83c9d3c49d3","title":"A discriminative account of the learning, representation and processing of inflection systems"},{"paperId":"a576512a7562597fd30719a834d5866d010ef6ab","title":"Compositional Generalization for Natural Language Interfaces to Web APIs"},{"paperId":"fcf25e1affc2f8ee5bb49d156f174e9769234deb","title":"Systematic Generalization with Edge Transformers"},{"paperId":"03da1b6759f70c10e49b33a0ee914cb893d6f949","title":"Learning Algebraic Representation for Systematic Generalization in Abstract Reasoning"},{"paperId":"2db6c10f135d5701ae7aec45986124ce264c1344","title":"Learning Symbolic Rules for Reasoning in Quasi-Natural Language"},{"paperId":"04db9b694280134f09af5fa787a306907edba29d","title":"How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN"},{"paperId":"c764ecba2bace12b9bfb9c2b0651a12ff6888ea7","title":"Learning to Generalize Compositionally by Transferring Across Semantic Parsing Tasks"},{"paperId":"7cc74ffa1215321712d4a830bb9dee19d9f0fb47","title":"Grounded Graph Decoding Improves Compositional Generalization in Question Answering"},{"paperId":"582f977cc2b06700dea6faa183c64f5d11204cfe","title":"How Do Neural Sequence Models Generalize? Local and Global Context Cues for Out-of-Distribution Prediction"},{"paperId":"f0787aace6a6d892140d9da85b49e76cb06f6862","title":"Automatic Knowledge Augmentation for Generative Commonsense Reasoning"},{"paperId":"1da81224d2a781b88186d81872755535e82fce5c","title":"Understanding How Encoder-Decoder Architectures Attend"},{"paperId":"d91dae75e7d3a13aad9a6815d6cbdf9a42f897e2","title":"SQALER: Scaling Question Answering by Decoupling Multi-Hop and Logical Reasoning"},{"paperId":"382504c7013f4578d0f1829fdcd413fed529cde1","title":"Learning to Solve Complex Tasks by Talking to Agents"},{"paperId":"00050c15896e8ae6bb534f10d072351547993f72","title":"LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing"},{"paperId":"8ccaf0c0fbd5e4079f36fa720cf23890be10dd66","title":"Dynamic Inference with Neural Interpreters"},{"paperId":"b07efb22576761eb15f82f2be9b5923f4f71aa43","title":"Learning to Follow Language Instructions with Compositional Policies"},{"paperId":"3d5699e7f7e085ad72102859b06fa4884d207e77","title":"Iterative Decoding for Compositional Generalization in Transformers"},{"paperId":"4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80","title":"How BPE Affects Memorization in Transformers"},{"paperId":"76c9558b3fa10baf0e094386a650015b29a8a4bc","title":"Compositional generalization in semantic parsing with pretrained transformers"},{"paperId":"bb0ab8591d6d57c7e2bd1ec35d806b3f25277752","title":"Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks"},{"paperId":"cdb2a71f42c88be6d5c5e4fecb8288a9a315f094","title":"Visually Grounded Concept Composition"},{"paperId":"85ef684e8f761e31cc8da9fcd6fe37c553d093f3","title":"Hierarchy in language interpretation: evidence from behavioural experiments and computational modelling"},{"paperId":"6ed7f8d8673fbf380c45cefa30138ff2b77d1d1b","title":"Abstraction, Reasoning and Deep Learning: A Study of the \"Look and Say\" Sequence"},{"paperId":"d41417f22f898125c4d34f672938ebb2a3764961","title":"Systematic Generalization on gSCAN: What is Nearly Solved and What is Next?"},{"paperId":"42c9186a600555f55a65c61638c155046774f081","title":"Robust Generalization of Quadratic Neural Networks via Function Identification"},{"paperId":"06fbeaf4d16639f177973a06cd7c4f78cb5e38ed","title":"COVR: A Test-Bed for Visually Grounded Compositional Generalization with Real Images"},{"paperId":"af749e5dbde38914ca6fa1463fca17eac8f69ecc","title":"ReaSCAN: Compositional Reasoning in Language Grounding"},{"paperId":"3962f108081b22c7e54b413f47ba6f2c16f2cc05","title":"Frequency Effects on Syntactic Rule Learning in Transformers"},{"paperId":"676fa805bd715591f99bb17e36d673a6a14e92fe","title":"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization"},{"paperId":"26c60f8ffb0d66d8732d22af6f5b539e49f2a1e6","title":"Discover AI Knowledge to Preserve Cultural Heritage"},{"paperId":"b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347","title":"Sequence-to-Sequence Learning with Latent Neural Grammars"},{"paperId":"4bc9521e56bca8995ed5e18274301961b25499a9","title":"How children learn to communicate discriminatively"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"95c379fa77e05cd2adc9f65e0dbb8e8065e30c43","title":"Neural Symbolic Representation Learning for Image Captioning"},{"paperId":"a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99","title":"Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning"},{"paperId":"45496cd0b256b75bfbe3bd95890b496069c7821c","title":"Multilingual Compositional Wikidata Questions"},{"paperId":"b61de520bc1ae57abde895601b62b4f92d82c0b4","title":"Pointer Value Retrieval: A new benchmark for understanding the limits of neural network generalization"},{"paperId":"d9911a0996fe6a4af7b67f378a0fd1c01aaf5e11","title":"Language Models as Zero-shot Visual Semantic Learners"},{"paperId":"be793883b04967307b4c59764c0199d65bec5972","title":"Hierarchical clustering optimizes the tradeoff between compositionality and expressivity of task structures for flexible reinforcement learning"},{"paperId":"7af9eb636ebe9da4aac10314e43da234e31ad535","title":"Systematic Generalization in Neural Networks-based Multivariate Time Series Forecasting Models"},{"paperId":"d839256cf7445748c4cf7354d2c6a1ff94efb694","title":"Generalization in Multimodal Language Learning from Simulation"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"},{"paperId":"10b809531cdd20b05274adcffcd4aa927f1fe54c","title":"Zero-Shot Compositional Concept Learning"},{"paperId":"d4b61b49c3b0c16d8d5c103802b77135fb9bf650","title":"What underlies rapid learning and systematic generalization in humans"},{"paperId":"1bed382373aed687c045bb65bc7541b16fc7a6be","title":"Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN"},{"paperId":"523745e29f6cb1890f18352d449fd3597910c485","title":"Improving Compositional Generalization in Classification Tasks via Structure Annotations"},{"paperId":"013ef3de1cc144b3014216255e8161b6e013d7ee","title":"Grounding Spatio-Temporal Language with Transformers"},{"paperId":"2d9b578e74d9b82cd30c6a5bee50162e626c5f5b","title":"How Modular Should Neural Module Networks Be for Systematic Generalization?"},{"paperId":"946179bd263d46d70422cdff7e6657b97b81230c","title":"Learning to Combine Per-Example Solutions for Neural Program Synthesis"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"3b8c8bbf2c1a69f2bf60b2434f2d1996aea2e740","title":"One Semantic Parser to Parse Them All: Sequence to Sequence Multi-Task Learning on Semantic Parsing Datasets"},{"paperId":"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","title":"Lexicon Learning for Few Shot Sequence Modeling"},{"paperId":"c0e059c46aea358872b4760aed53c4da3beaaeee","title":"Structured Reordering for Modeling Latent Alignments in Sequence Transduction"},{"paperId":"af84a6ed24ff37a6c1147240584a5cdb31b38f06","title":"Using top-down modulation to optimally balance shared versus separated task representations"},{"paperId":"83ed3184cc7b2dcee3c2b91529870bc304513468","title":"Examining the Inductive Bias of Neural Language Models with Artificial Languages"},{"paperId":"577d44a10b424a55165a6bf4839bafce2c695302","title":"SyGNS: A Systematic Generalization Testbed Based on Natural Language Semantics"},{"paperId":"32feca141fce06c6588b4014d27953a3fc25f19b","title":"PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World"},{"paperId":"88dbde378e9ac5c25fc7d78f5da147223e8d34d4","title":"Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention"},{"paperId":"35c6bdab35e8fd4e982302b5270da3c8098c58b1","title":"Modular Networks for Compositional Instruction Following"},{"paperId":"b2d08f6e17882545b2cbb865fd8c907eb21c3aac","title":"Generalization in Instruction Following Systems"},{"paperId":"03ad126cfe495933f7bb769f27c03e5f31caedf8","title":"On Compositional Generalization of Neural Machine Translation"},{"paperId":"9bc7c64812fe4a14ac319e07a00926ce93b20b5a","title":"Modelling the development of counting with memory-augmented neural networks"},{"paperId":"f3879fedf036175aefdb750c5527d184f038b932","title":"Compositional Processing Emerges in Neural Networks Solving Math Problems"},{"paperId":"18bb3bbeffe2b00378342a876d3de4ed695c57b4","title":"Multi-Task Recurrent Modular Networks"},{"paperId":"2de6f237a86e733d335a023ff644e8fbe624b217","title":"Image interpretation by iterative bottom-up top-down processing"},{"paperId":"8bb292d04fcef84864d5a4ac9c170d4fc8003ae9","title":"gComm: An environment for investigating generalization in Grounded Language Acquisition"},{"paperId":"ca3535dcdda9849350ad7c991a60660b22844f2f","title":"Searchable Hidden Intermediates for End-to-End Models of Decomposable Sequence Tasks"},{"paperId":"fadbe4a7cac1abffd3b3f7b39588d01e7d14e919","title":"The Future of Computational Linguistics: On Beyond Alchemy"},{"paperId":"53bbc4213bad0ff9e0d56392dd2d66b5168ffbda","title":"Intuitive Physics Guided Exploration for Sample Efficient Sim2real Transfer"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":"c114db5f1c38cbe6797bc74ef98072cac71f6cc6","title":"ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser"},{"paperId":"d969066d1dff7203055a493daaa3af8c490bf58e","title":"Recognizing and Verifying Mathematical Equations using Multiplicative Differential Neural Units"},{"paperId":"0d73c5d6aa5d75fbde7b8fbffcb2bcf58268e650","title":"Embodying Pre-Trained Word Embeddings Through Robot Actions"},{"paperId":"cf2b945be7b695d01a9abc5b099c54d8cdabac2a","title":"AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning"},{"paperId":"316c5d697f7caf92b419213e929a6063afaf253c","title":"ACRE: Abstract Causal REasoning Beyond Covariation"},{"paperId":"2ab7473f9975d28ca40c6a5e4c39ba48c89a911d","title":"Relational Weight Priors in Neural Networks for Abstract Pattern Learning and Language Modelling"},{"paperId":"d3ef7abfa1dc2545e1842e98d09fb6473d600ce7","title":"Harnessing Geometric Constraints from Emotion Labels to improve Face Verification"},{"paperId":"eaa88d697f92739f3569564329e9d037aabbe2d7","title":"A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics"},{"paperId":"4d1b18ee2d6093fc977df09d62b2a0e40b62a698","title":"A HINT from Arithmetic: On Systematic Generalization of Perception, Syntax, and Semantics"},{"paperId":"c9873d7240df69d84fcad24b18d248466ec2e83d","title":"HALMA: Humanlike Abstraction Learning Meets Affordance in Rapid Problem Solving"},{"paperId":"c8fc3da2d6a0a7f05e716fa3c06fc7c813c9c049","title":"Can deep learning beat numerical weather prediction?"},{"paperId":"24a5ebb09502fcd29b7af851f40f858137d41818","title":"Compositional memory in attractor neural networks with one-step learning"},{"paperId":"02dac573bff1d0620bb7412ed68681d21aee7e12","title":"Systematic Generalization for Predictive Control in Multivariate Time Series"},{"paperId":"be09ed6cd73654a23f78416433a1b23ea623ea79","title":"Symbolic Behaviour in Artificial Intelligence"},{"paperId":"5505d608a1d482fdc083796db812379ec1cb8723","title":"Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches"},{"paperId":"10161db52bfa53bdab84ae97b47cef2f22119131","title":"The Role of Syntactic Planning in Compositional Image Captioning"},{"paperId":"0d39d525f30609d0541330f933007025cd457a83","title":"Exploring Transitivity in Neural NLI Models through Veridicality"},{"paperId":"64b8692b83b2b1d13790fc549c7b3f71348a9968","title":"Situation and Behavior Understanding by Trope Detection on Films"},{"paperId":"2c0a266f9cb88bb914c138ece0deaab8cf528f78","title":"Neural Sequence-to-grid Module for Learning Symbolic Rules"},{"paperId":"1a0cb72de90835245316562c6e56537f5ff0d0a4","title":"Can RNNs learn Recursive Nested Subject-Verb Agreements?"},{"paperId":"8eaa6a82e1f94c1552e5c284061e25fee36ec427","title":"Emergent Symbols through Binding in External Memory"},{"paperId":"d3edc20ed4a07195f3663abc0ead4220266fd75b","title":"*-CFQ: Analyzing the Scalability of Machine Learning on a Compositional Task"},{"paperId":"40848b41ed8c9c255ecd8a920006877691b52d03","title":"WILDS: A Benchmark of in-the-Wild Distribution Shifts"},{"paperId":"7344ed64d1717780422fd1d58fae85edc544d180","title":"Iterative Utterance Segmentation for Neural Semantic Parsing"},{"paperId":"df66fcd3fd4f0d55bd96528cddb0e2f08ddb3385","title":"Revisiting Iterative Back-Translation from the Perspective of Compositional Generalization"},{"paperId":"15ad6f868acb05e836e88774aa2b71b0e082e5aa","title":"Meta-Learning of Structured Task Distributions in Humans and Machines"},{"paperId":"b9c3e87bc09c4c6167a03a835c30b1c23bef7a40","title":"Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"856208bf9f17320dddb4bdd7127f08cae7d922e3","title":"A Differentiable Relaxation of Graph Segmentation and Alignment for AMR Parsing"},{"paperId":"cc0fc60e9b9d036acf53899c259574c8ce2aedfc","title":"Compositional Generalization via Semantic Tagging"},{"paperId":"5cc8ea815bd05be3b28519b489afe6de278a4209","title":"Learning to Recombine and Resample Data for Compositional Generalization"},{"paperId":"1ffed50c5a14012145ee4d87855ae23deee44be5","title":"CURI: A Benchmark for Productive Concept Learning Under Uncertainty"},{"paperId":"649c758b0e59ddedaae37a3757e8eabdba664e5a","title":"Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks"},{"paperId":"a29c55573839087490d7469fbbcfd06728566b44","title":"Paired Examples as Indirect Supervision in Latent Decision Models"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"b95184d5eb25b0fe66d8bd1ad1b7677a51c21702","title":"Latent Compositional Representations Improve Systematic Generalization in Grounded Question Answering"},{"paperId":"032399b7fc693a9fc12bb26d6be8c02d77dd397a","title":"What they do when in doubt: a study of inductive biases in seq2seq learners"},{"paperId":"27ad6a5a17a75d1879f47a21a6d07f56ce87cab9","title":"Learning Task-General Representations with Generative Neuro-Symbolic Modeling"},{"paperId":"320a260faa9a0533b29e062def6dc97ac1353d28","title":"Compositional Embeddings for Multi-Label One-Shot Learning"},{"paperId":"6001843fde0f61cf886cf3fc2d0ec9ac3f2a3197","title":"Challenges of Acquiring Compositional Inductive Biases via Meta-Learning"},{"paperId":"bcf2bc325e4a48b615efb9cad2da2ce3e2ecbec7","title":"Solving SCAN Tasks with Data Augmentation and Input Embeddings"},{"paperId":"bd652738efa000e4a74d27f5495577dce8ba34a0","title":"Few-Shot Novel Concept Learning for Semantic Parsing"},{"paperId":"8d5f5c693b803ca58db748d97af905d5b5a0130b","title":"Categorial Grammar Induction as a Compositionality Measure for Understanding the Structure of Emergent Languages"},{"paperId":"581f52c4a045b2fd2d906b8d9f31165b832d0049","title":"Supplementary Material for Grammar-Based Grounded Lexicon Learning"},{"paperId":"c735740b26ceaa4db9d77233116434c0e8b311d8","title":"Learning Adaptive Control Flow in Transformers for Improved Systematic Generalization"},{"paperId":"30c9c73ed04a2dad44c20312653b4c65a27a6b7b","title":"KommonGen: A Dataset for Korean Generative Commonsense Reasoning Evaluation"},{"paperId":"15e2520e7f3bd96ba9d9d6e967771b34e448a9f1","title":"Compositional Generalization and Neuro-Symbolic Architectures"},{"paperId":"6fedcd81fb5aa0e3a30d1c32895759456b8b3cee","title":"Neural Structure Mapping For Learning Abstract Visual Analogies"},{"paperId":"751f55beac45cec14b0aff6174ed0139afb54b08","title":"Modelling Symbolic Knowledge Using Neural Representations"},{"paperId":"aa04c312bad4ab8ed192a33c355dbfbab65d8098","title":"LEARNING STRUCTURE FROM THE GROUND UP: HIERARCHICAL REPRESENTATION LEARNING BY CHUNKING"},{"paperId":"b156ddf444b997cbbc3168ee96ff76a85f0a5d62","title":"Robust Visual Reasoning via Language Guided Neural Module Networks"},{"paperId":"0b1470014bdbaa80ba63da0491d9db6c7d4febcc","title":"Detecting Compositionally Out-of-Distribution Examples in Semantic Parsing"},{"paperId":"6d466de7180776023a539371fad3d521eb5ff791","title":"Retrieval, Analogy, and Composition: A framework for Compositional Generalization in Image Captioning"},{"paperId":"1a95eed753096bcf219fa5394623141899115851","title":"How Do Neural Sequence Models Generalize? Local and Global Cues for Out-of-Distribution Prediction"},{"paperId":"d68255e8210843118d641175105e69686ad5b40f","title":"Mind the Context: The Impact of Contextualization in Neural Module Networks for Grounding Visual Referring Expressions"},{"paperId":"6d8943bd6edf34b022862be286cfd11fff16c89d","title":"Who’s on First?: Probing the Learning and Representation Capabilities of Language Models on Deterministic Closed Domains"},{"paperId":"af4147ed4d8c4dd81fdfb21eb4658e2ef3b825df","title":"Controlled tasks for model analysis: Retrieving discrete information from sequences"},{"paperId":"87e0b67e02a90e7e714336be5941190907cfd33f","title":"C L ] 1 7 M ay 2 01 8 Extrapolation in NLP"},{"paperId":"a406701b5fb05be55244d4f940db7be55fce85c6","title":"Semantic Systematicity in Connectionist Language Production"},{"paperId":"dff59ec1f1d3c01c3c7046517aa7b0612655764c","title":"Guiding Multi-Step Rearrangement Tasks with Natural Language Instructions"},{"paperId":"10a269a4f37a3d5f3b1c7e585b81ac704a3036a3","title":"Learning to Represent State with Perceptual Schemata"},{"paperId":"95f8cf3dd2cd1d050d6b155e3c056f0e14f496b7","title":"Better Chinese Sentence Segmentation with Reinforcement Learning"},{"paperId":"7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8","title":"AND does not mean OR: Using Formal Languages to Study Language Models’ Representations"},{"paperId":"41e9b2470849ceeeabcead25255519bf80311f8d","title":"Contribution d’informations syntaxiques aux capacités de généralisation compositionelle des modèles seq2seq convolutifs (Assessing the Contribution of Syntactic Information for Compositional Generalization of seq2seq Convolutional Networks)"},{"paperId":"240467c5c29ebf4926e8df392cc75e2a4fcffd3a","title":"Structure-(in)dependent Interpretation of Phrases in Humans and LSTMs"},{"paperId":"bab0efc6870d848ab4fe89f47d0fc947769cee7f","title":"Systemic Oversimplification Limits the Potential for Human-AI Partnership"},{"paperId":"fed5ad5d8f4a745c04eefc6fbff4bee4b0631323","title":"Episodes Meta Sequence S 2 Fast Update Slow Update Fast Update Slow Update"},{"paperId":"7f9b6708572d1950e40dae89fd5e556198c105e7","title":"Modular Networks for Compositional Instruction Following"},{"paperId":"94997502ffeafe48545a23a7f6cf6b1de79d4979","title":"A computational framework for learning and transforming task representations"},{"paperId":"eadd4de61cdc3ddd8c6e237187ea5cede78d484f","title":"USING MULTIPLICATIVE DIFFERENTIAL NEURAL UNITS"},{"paperId":"1f0e1657063ea38cf225eaf1c1187ae7b2e4a0e0","title":"Increasing Robustness to Spurious Correlations using Forgettable Examples"},{"paperId":"359c56a068e4a84a9f3b78f43b53fe3b333c0ba0","title":"Systematic generalisation with group invariant predictions"},{"paperId":"fd57ee5b9aa17c263281a8901ac36f83bb540a0d","title":"Harnessing Geometric Constraints from Auxiliary Labels to Improve Embedding Functions for One-Shot Learning"},{"paperId":"01295091d318dca8212431590cce2c7a1081469c","title":"Compositional Generalization via Parsing Tree Annotation"},{"paperId":"b82b89bf6405bc98cb39fc477e0c5a85a563e7fb","title":"GENERATIVE NEURO-SYMBOLIC MODELING"},{"paperId":"9dc7fba6551c8e142b95ee6baf0d25ccb1aed988","title":"On (Emergent) Systematic Generalisation and Compositionality in Visual Referential Games with Straight-Through Gumbel-Softmax Estimator"},{"paperId":"373bc164d7b552f8782988e7da6b0d00092a20b0","title":"Continual Lifelong Learning in Natural Language Processing: A Survey"},{"paperId":"d642868ce4325ebf3026c0aa0c497a079f112a8d","title":"On the Binding Problem in Artificial Neural Networks"},{"paperId":"690e5b12b80b33eb8da4efd5621e56def1217db1","title":"Infinite use of finite means: Zero-Shot Generalization using Compositional Emergent Protocols"},{"paperId":"f162b64756f01cdf04bc59c7592a77e4c8981656","title":"Question Answering over Knowledge Bases by Leveraging Semantic Parsing and Neuro-Symbolic Reasoning"},{"paperId":"d8ee993d54b83f81f7f65f2d33be04ae540d5590","title":"Invertible Tree Embeddings using a Cryptographic Role Embedding Scheme"},{"paperId":"ac5ed37f35375ea423c8474a944f27d3a0ab8774","title":"Learning Canonical Transformations"},{"paperId":"4b58367375466e653751a0c258b2f50bd3551408","title":"Sequence-to-Sequence Networks Learn the Meaning of Reflexive Anaphora"},{"paperId":"f4dbae9454eaf73a5dd7e2b7a087b52946d286dd","title":"Distilling Structured Knowledge for Text-Based Relational Reasoning"},{"paperId":"106fb432d2b62f3824a9d6f4a1b30e1f8b6ea9d7","title":"Sequence-level Mixed Sample Data Augmentation"},{"paperId":"e0acae87ae6d1d14bb2852aad7d645fceee87eb2","title":"The MAGICAL Benchmark for Robust Imitation"},{"paperId":"83d2d970db0eeb645d087a7f37bb05adb780706e","title":"Modularity Improves Out-of-Domain Instruction Following"},{"paperId":"259cf65eeae13861031f44cf906d43b155192b10","title":"Explicitly Modeling Syntax in Language Model improves Generalization"},{"paperId":"5262a59603b816eb3a339da937170e2b134a139f","title":"Compositional Generalization with Tree Stack Memory Units."},{"paperId":"42d376cdf2437769b9619aa38db64921772920ca","title":"Linguistically-Informed Transformations (LIT): A Method for Automatically Generating Contrast Sets"},{"paperId":"e16820ec1445cf6ec55d6e8ec27afa5c02812a11","title":"Inferring symmetry in natural language"},{"paperId":"986cc3d0e3afb23f84564ea9588b7b8e9c3e1dd6","title":"Hierarchical Poset Decoding for Compositional Generalization in Language"},{"paperId":"227fe850a72fab24998c7e08d75db214715dc74e","title":"The EOS Decision and Length Extrapolation"},{"paperId":"37882abaec01eba1bf5bda8a36c904aaea0d5642","title":"Improving Compositional Generalization in Semantic Parsing"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"055fac05cd424e7b1bdcd359ff7980ca8d938ef3","title":"Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually)"},{"paperId":"3fd45fc420a882ab2fba3166ef08f376cc758ad0","title":"On Long-Tailed Phenomena in Neural Machine Translation"},{"paperId":"f65f90c41aafd40449edc8e2c2c80a63bc767e6d","title":"Interpretable Neural Computation for Real-World Compositional Visual Question Answering"},{"paperId":"389596027e577fb28ea5e1cc313b4f3d610cffd3","title":"Recursive Top-Down Production for Sentence Generation with Latent Trees"},{"paperId":"e28ce15bc1e5108dbed621a1e72af6906a772d42","title":"Meta-Learning of Compositional Task Distributions in Humans and Machines"},{"paperId":"31a5556d5d311a17a20bc9cfb6c7fed7c4efe66d","title":"Unseen Filler Generalization In Attention-based Natural Language Reasoning Models"},{"paperId":"89cb62dc83c1b1895267bd28639fbf5bb7ed21a4","title":"Measuring Systematic Generalization in Neural Proof Generation with Transformers"},{"paperId":"ddf1b38b6ba885326e7f44721a135a7b2d7f415a","title":"Think before you act: A simple baseline for compositional generalization"},{"paperId":"00b1e962182c42949822821dc1a929bd132ec082","title":"Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models"},{"paperId":"1f95641726dc6875a7f6a89c1d8b0ee414bd4b52","title":"Recurrent Inference in Text Editing"},{"paperId":"3ecc8f61418f6afdbb600d9f6fbb286143e56026","title":"Systematic Generalization on gSCAN with Language Conditioned Embedding"},{"paperId":"5b7547aa20140b29cd6d8426e4110d4ef97717ed","title":"ReLEx: Regularisation for Linear Extrapolation in Neural Networks with Rectified Linear Units"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"8320ea909c38a616f9daccff4e5a49cfce4d9735","title":"Analogical Reasoning for Visually Grounded Language Acquisition"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"2e8c84fd61c91e067dddef52ced76b824beb7013","title":"Learning Reasoning Strategies in End-to-End Differentiable Proving"},{"paperId":"189d489d7dabf51949124f55fb0104326d8c84a4","title":"Learning Representations that Support Extrapolation"},{"paperId":"c3375b9b26c8f32e5c6a16443e80e9ec45055e0e","title":"The Scattering Compositional Learner: Discovering Objects, Attributes, Relationships in Analogical Reasoning"},{"paperId":"79d7648aeacc5d5e90b296acbf156ed1131eae37","title":"Compositionality Decomposed: How do Neural Networks Generalise? (Extended Abstract)"},{"paperId":"61a412f7c351a2a582f65853522ad7a632b0186d","title":"Evaluating Compositionality of Sentence Representation Models"},{"paperId":"23835438889899885d9f33de2fb2356da10bbc0c","title":"Compositional Generalization by Factorizing Alignment and Translation"},{"paperId":"aecb95605083c460feb289ec40901e328805fae5","title":"Deep Reinforcement Learning and Its Neuroscientific Implications"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"c103f1903e6cb9904729e65fb167a18dbfe3b129","title":"A Study of Compositional Generalization in Neural Models"},{"paperId":"61c49e6399e76a96ebc82dbe486131db6c07b772","title":"Relational reasoning and generalization using non-symbolic neural networks"},{"paperId":"5abfd3c004f79402a3a321564e8963797f60c6b9","title":"Systematic Generalisation through Task Temporal Logic and Deep Reinforcement Learning"},{"paperId":"432a572956526239103c6d8658bdd46c29104aa1","title":"Closed Loop Neural-Symbolic Learning via Integrating Neural Perception, Grammar Parsing, and Symbolic Reasoning"},{"paperId":"2eb710b446570f48377b25eb279295648d05f65d","title":"On sample efficiency and systematic generalization of grounded language understanding with deep learning"},{"paperId":"0bfd4ed399054eae26c3cdaabc0aed80ca95e125","title":"Neural Power Units"},{"paperId":"b0434df9b6ed85aa79859335905f549eeac3e817","title":"Constitutional Rights in the Machine Learning State"},{"paperId":"347b926e82fa8e635050a5c7781598642c115596","title":"Transforming task representations to perform novel tasks"},{"paperId":"5ea7d562df6aac215630df8abf4a1321a9e47e6e","title":"Transforming task representations to allow deep learning models to perform novel tasks"},{"paperId":"4dc005ea288c50d57222122903edf87f21689781","title":"Probing Linguistic Systematicity"},{"paperId":"f7f20e163cba2ec5ca74fd1c4352a987d67ef7b7","title":"Emergence of Syntax Needs Minimal Supervision"},{"paperId":"4f91d16d7d9c21e2d9fffeff6abc78619be4d133","title":"Visually Grounded Continual Learning of Compositional Phrases"},{"paperId":"ca48348e0d00b5c3766b9a1ed8864ffce9285d96","title":"Visually Grounded Continual Learning of Compositional Semantics"},{"paperId":"ec3d5bdfda2c5c841c2481f5da123b2c086e6f5c","title":"Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking"},{"paperId":"c30b457fdfb0623b87379de79ffaa570a7f3bb48","title":"Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation"},{"paperId":"7e69f2255d3685c6aae2790d0807b5df5fd031f5","title":"Compositional Continual Language Learning"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"4599f96dd1a2584e00d342953fc7e1361ffd6e1f","title":"Neural Status Registers"},{"paperId":"db03f8de1944ae6001f860219f6e483bd6280030","title":"Abstract Rule Based Pattern Learning with Neural Networks"},{"paperId":"387b5988331f8fe779c323f8a88df23daa715a8a","title":"Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?"},{"paperId":"eb7d630c0fc1fc5601775be72265230707382107","title":"What Limits Our Capacity to Process Nested Long-Range Dependencies in Sentence Comprehension?"},{"paperId":"d70af4990cba2574c41b1235030f7a5b702e2d70","title":"Compositionality and Generalization In Emergent Languages"},{"paperId":"39e3c1c4ea9ca492123941c0963146e1898a5a10","title":"Generating new concepts with hybrid neuro-symbolic models"},{"paperId":"022dcaa4cc51ecd038970683146b2ad19f45df61","title":"Synonymous Generalization in Sequence-to-Sequence Recurrent Networks"},{"paperId":"0dc5dd7c64ee016bdc33a5f32dc25747be5ca702","title":"From SCAN to Real Data: Systematic Generalization via Meaningful Learning"},{"paperId":"6f0be1f9bda7530b1fa654cac84d595ca9d53740","title":"Revisit Systematic Generalization via Meaningful Learning"},{"paperId":"7e9af8a6081dc00187bd4a6727751d1721bd7816","title":"Evaluating Logical Generalization in Graph Neural Networks"},{"paperId":"937fef6a786c4463a3bb19770c704945d1600b66","title":"Learning Compositional Rules via Neural Program Synthesis"},{"paperId":"4bc2bb6584774b0d8ad0b4f5215dc2075487c192","title":"A Benchmark for Systematic Generalization in Grounded Language Understanding"},{"paperId":"e4b0ec63559b6792bb129385e3c67d541afb53a6","title":"A Puzzle concerning Compositionality in Machines"},{"paperId":"c04262cb3f76ff769af32afad05263bd47ebef18","title":"Evaluating visually grounded language capabilities using microworlds"},{"paperId":"0119a57cf88ef16e6dc291252fae340bb6b3953c","title":"CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning"},{"paperId":"8bae31b144575426f14131a3f04f1e7cd2cc02f5","title":"Compositional properties of emergent languages in deep learning"},{"paperId":"4aea918d9bd66440ce0c00abbfbb57b212d76158","title":"Neural Arithmetic Units"},{"paperId":"4e91c1fde4030c9518932c7a0b46358bb93ba91a","title":"Exploiting Language Instructions for Interpretable and Compositional Reinforcement Learning"},{"paperId":"b0ea633e0c22fbd8cbc531c7326376725d16ce25","title":"Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"3ee9a301c4f269f5bd20390d28d2a50d0935339b","title":"Hierarchical Character Embeddings: Learning Phonological and Semantic Representations in Languages of Logographic Origin Using Recursive Neural Networks"},{"paperId":"fe2f525349d31f854472dc39b63612322bf8a445","title":"Weight Priors for Learning Identity Relations"},{"paperId":"d715b4a9282562b9d84fb66e04ee70e66b12e86d","title":"Location Attention for Extrapolation to Longer Sequences"},{"paperId":"cf96b99f4d35b452d4679b664b74aa255e000b8c","title":"Capacity, Bandwidth, and Compositionality in Emergent Language Learning"},{"paperId":"80993cc44eef146116bf682fd89f5f256211620f","title":"Discovering the Compositional Structure of Vector Representations with Role Learning Networks"},{"paperId":"ae3501afbe8f3f5cf1c5270fa00d0b65fc1c9484","title":"Environmental drivers of systematicity and generalization in a situated agent"},{"paperId":"681fbcd98acf20df3355eff3585994bd1f9008b7","title":"Probing Natural Language Inference Models through Semantic Fragments"},{"paperId":"d4fc020db15584ea040162a1afb6abc81aa6c7e4","title":"Analyzing machine-learned representations: A natural language case study"},{"paperId":"097c71968b5eaacec908f4d1ce1137a53dca8bdc","title":"Learning First-Order Symbolic Representations for Planning from the Structure of the State Space"},{"paperId":"815a3d56401483b635cfad9468852cddb46350ee","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":"4cfd8f903506865e7ccf28b0a07ee3c551487e92","title":"Detecting semantic anomalies"},{"paperId":"ba310a9b9f7b812478b2a08dbd9917ea937a4e36","title":"Mutual exclusivity as a challenge for deep neural networks"},{"paperId":"57bb2a385675f8b2ab24e7b9b397553fdaafb849","title":"Sequential Mastery of Multiple Visual Tasks: Networks Naturally Learn to Learn and Forget to Forget"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"389b9d9f88c262fa65800318cf029aca1b37eb1f","title":"Systematicity in a Recurrent Neural Network by Factorizing Syntax and Semantics"},{"paperId":"ab9e5d6804c5051e4f7fa07e467e766ca0126500","title":"Are Neural Nets Modular? Inspecting Their Functionality Through Differentiable Weight Masks"},{"paperId":"7b85c3dd9120ff2cab03f36d778520cd7fae903c","title":"THE SCATTERING COMPOSITIONAL LEARNER: DISCOVERING OBJECTS, ATTRIBUTES, RELATION-"},{"paperId":"3f51de90ef9c875b4aa16d5dd3a90c9689bbf249","title":"MODELS WITH VARIABILITY IN DATA"},{"paperId":"d2b39c91d0f49557ff01844ee22411fcf9933a36","title":"E MERGENT S YMBOLS THROUGH B INDING IN E XTERNAL M EMORY"},{"paperId":"55c9b62b69b7f644e326969f8f712e6487cd4b9d","title":"Constitutional Rights in the Machine Learning State"},{"paperId":"2afad1b280396594911012ae58bfc427d9bb3873","title":"LATENT DECISION MODELS"},{"paperId":"39a7e79063cf9e967e65599fe89ed45efe2f63fb","title":"A Simple and General Strategy for Refer- ential Problem in Low-Resource Neural Ma- chine Translation"},{"paperId":"502a1995c0a56af4487d364f56e1ce8abe78f23a","title":"The DeepMind Chinese–English Document Translation System at WMT2020"},{"paperId":"6027dd5d9491a04b3845c5e62fc6df8bcd320e66","title":"Artificial Intelligence XXXVII: 40th SGAI International Conference on Artificial Intelligence, AI 2020, Cambridge, UK, December 15–17, 2020, Proceedings"},{"paperId":"871d5f31e23f427ba229023ea10ec6873ec36a0d","title":"Multimodal Graph Networks for Compositional Generalization in Visual Question Answering"},{"paperId":"dc8020f05b18ed434009b5d56398c57456c7dcde","title":"Disentangling neural network representations for improved generalization"},{"paperId":"80deaca65c2c155bd15718eeecff584841eb25b0","title":"CLOSURE: Assessing Systematic Generalization of CLEVR Models"},{"paperId":"6e8bd7310e2603776ec4f51b6abc2a485f9ca7ce","title":"Extending Machine Language Models toward Human-Level Language Understanding"},{"paperId":"6422d5e83caec99487936035cfbb2b0d18f2a76d","title":"Biology and Compositionality: Empirical Considerations for Emergent-Communication Protocols"},{"paperId":"83989d1dcf1bb533d37db6a6ff9af478feeb4aae","title":"Composing and Embedding the Words-as-Classifiers Model of Grounded Semantics"},{"paperId":"72e0d7e374937374897cab79306079f6f436a9d4","title":"On Compositionality in Neural Machine Translation"},{"paperId":"9dc75988a53b86c1e36539daa0d8ac003b234502","title":"Big Generalizations with Small Data: Exploring the Role of Training Samples in Learning Adjectives of Size"},{"paperId":"287e85aca777d6d3d73e1484ba9c0f09d40f578a","title":"Ordered Memory"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"112ac68ddb0f021517dd465e89918fa52755cc35","title":"Measuring Arithmetic Extrapolation Performance"},{"paperId":"4beaabe0c4277ddb850a2f91a20b5fcec84f18af","title":"Emergent Systematic Generalization in a Situated Agent"},{"paperId":"d88f31a0091eee02c5a2aa2013914818cdef114e","title":"Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving"},{"paperId":"48e43bb9d4843e9c9e0599d85dde3121989de01a","title":"Learning First-Order Symbolic Planning Representations from Plain Graphs"},{"paperId":"32c9a0acee8d236c553395052c29a6d853d8ea2d","title":"Compositional Generalization in Image Captioning"},{"paperId":"a1a662301818cdc77dd015280da285847223765b","title":"How to study the neural mechanisms of multiple tasks"},{"paperId":"4d031258a66076187001b4d6182345198624d872","title":"The compositionality of neural networks: integrating symbolism and connectionism"},{"paperId":"5e35895fc4731858f0b286cb5a1613a819cc2367","title":"CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text"},{"paperId":"363a24a96423227ed6b03fc71a169ad0f80e6fac","title":"Grammatical Sequence Prediction for Real-Time Neural Semantic Parsing"},{"paperId":"9d671f60388f1da0ec1605b42260f7271f35a3de","title":"Modeling question asking using neural program generation"},{"paperId":"136c05cb8dd359fb8e0dc7947172a9ecb74ccbec","title":"Learning by Abstraction: The Neural State Machine"},{"paperId":"c7abc9d3d3b540ec9e3b423df706f750c8dc36f9","title":"The teaching size: computable teachers and learners for universal languages"},{"paperId":"2030b07a36a77821b978ff8a20a14d88c1c60db1","title":"Mutual exclusivity as a challenge for neural networks"},{"paperId":"3a04b6098a965781f5ed06ea7f260a66421f61bf","title":"Neural Theorem Provers Do Not Learn Rules Without Exploration"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"32c0e0b181eff3e9dd1c1dacec4138770008125e","title":"Assessing Incrementality in Sequence-to-Sequence Models"},{"paperId":"b39efed2e73357db4691f66935cf62e7b51f30e1","title":"Transcoding Compositionally: Using Attention to Find More Generalizable Solutions"},{"paperId":"5ee4bd410e633f07953d88827ee9dce4688b436e","title":"Scaling characteristics of sequential multitask learning: Networks naturally learn to learn"},{"paperId":"82dac30bc25eb1470e07ff5bd1ec000f28f4c6d8","title":"Siamese recurrent networks learn first-order logic reasoning and exhibit zero-shot compositional generalization"},{"paperId":"6a9cab267526de228a51f70ae56f1e07ecfc7bf8","title":"Grammar-based Neural Text-to-SQL Generation"},{"paperId":"3323d0956b2db141f3727aacbf7c111e9d39f17d","title":"Planning with State Abstractions for Non-Markovian Task Specifications"},{"paperId":"ed2f6d491c3796afa3e03460ede800cf13191231","title":"Sequential mastery of multiple tasks: Networks naturally learn to learn"},{"paperId":"2621323502fc779c79bca7ba112bc4d0c1db1d3f","title":"CNNs found to jump around more skillfully than RNNs: Compositional Generalization in Seq2seq Convolutional Networks"},{"paperId":"46940a78ad8ba3292666738fcfd92d2f5ec12ba9","title":"The relational processing limits of classic and contemporary neural network models of language processing"},{"paperId":"e24e44515b15e1326dd25ab092a152a067c63fc1","title":"Word-order Biases in Deep-agent Emergent Communication"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"b42c1eb6a8eadc8d734acd4c1ff1ff0f6b164500","title":"Emergence of Compositional Language with Deep Generational Transmission"},{"paperId":"e2bee23ac912cf36f0479b830ff78d4bb6cc73c3","title":"Analyzing and interpreting neural networks for NLP: A report on the first BlackboxNLP workshop"},{"paperId":"0dc092d33f7c71bc9d8d42b53ebb1fad101db4c8","title":"Linguistic generalization and compositionality in modern artificial neural networks"},{"paperId":"dc30dd7d209f518a2af01fc20b49ee55bd2c4957","title":"Training neural networks to encode symbols enables combinatorial generalization"},{"paperId":"66a388a200bd36aff05ec8acb6fbb092b3e1d777","title":"Data for free: Fewer-shot algorithm learning with parametricity data augmentation"},{"paperId":"32948ae25dbd35f2d94a59c27f6bee935bd602b8","title":"Studying the Inductive Biases of RNNs with Synthetic Variations of Natural Languages"},{"paperId":"d74037476b85dc02eb74369b12e0a5848617ecbc","title":"Deep Learning for Cognitive Neuroscience"},{"paperId":"1dd5bcc9c55416bacab30fd96818bc1e6e02e02e","title":"No integration without structured representations: Response to Pater"},{"paperId":"cef77310f326bd30b172459dbecaedf228fc7b23","title":"Probabilistic Neural-symbolic Models for Interpretable Visual Question Answering"},{"paperId":"0d16b0fbe1e2bf6ef02aea2e058f2e13c3a83fa2","title":"Learning to Make Analogies by Contrasting Abstract Relational Structure"},{"paperId":"7a8f8109e65ed9a6048859681a825eb5655e5dd2","title":"No Training Required: Exploring Random Encoders for Sentence Classification"},{"paperId":"9d9b4cc02fc0ac6fe7eac649599db1a47cf99d89","title":"Human few-shot learning of compositional instructions"},{"paperId":"648c33300ae7597f24e73600353aef31df745086","title":"Task representations in neural networks trained to perform many cognitive tasks"},{"paperId":"668f42a4d4094f0a66d402a16087e14269b31a1f","title":"Analysis Methods in Neural Language Processing: A Survey"},{"paperId":"a686c3becc42588517a4ca061badeb12c3c5a333","title":"Modelling Identity Rules with Neural Networks"},{"paperId":"6c7494a47cc5421a7b636c244e13586dc2dab007","title":"Systematic Generalization: What Is Required and Can It Be Learned?"},{"paperId":"56326d969591d3c30e2f48027b926be20a3e75f7","title":"Automatically Composing Representation Transformations as a Means for Generalization"},{"paperId":"ae307bfefb75495331e66be955e7f64a0aba1430","title":"Natural Language Statistical Features of LSTM-Generated Texts"},{"paperId":"9fcd9536c03870255d13935a4fd6379052ac15cc","title":"SYSTEMATIC GENERALIZATION: WHAT IS REQUIRED AND CAN IT BE LEARNED?"},{"paperId":"7d7487351353cd2708106e995d6296c2c84c295d","title":"Systematic Compositionality in Recurrent Neural Networks (RNNs)"},{"paperId":"8f8216c871b52812f4122e55d39dbfa759d1fdda","title":"Compositional Generalization in Image Captioning"},{"paperId":"96a7232eb547fed9e7fd2a2b1d6a100feee3a2ed","title":"Compositionality as Directional Consistency in Sequential Neural Networks"},{"paperId":"8225f6f8a037188f2701e9596241da519df8e8ef","title":"M UTUAL EXCLUSIVITY AS A CHALLENGE FOR DEEP NEURAL NETWORKS"},{"paperId":"c356394d9f1296103d3cb8c3b4cf56ab1e3305e5","title":"Compositionality as Directional Consistency in Sequential Neural Networks"},{"paperId":"d4eb596212d25bb52aae4c43ef65236439357af6","title":"Structured learning and inference with neural networks and generative models by"},{"paperId":"486f4a078560dd258ca5af63cb025759cc5638be","title":"Learning Reduplication with a Neural Network without Explicit Variables"},{"paperId":"09fb53961e9a4e3c172c3e3b726ebf94961528e1","title":"Zero-Shot Transfer VQA Dataset"},{"paperId":"33ecb49e7b1eb1f44790fb6ceca6eed82cb0c7cd","title":"Jump to better conclusions: SCAN both left and right"},{"paperId":"1519e58ca9f6a14c7b08fd644508977caae9477d","title":"Limitations in learning an interpreted language with recurrent models"},{"paperId":"79cb080c84da314c2113692585b1e9ee29afa33a","title":"On learning an interpreted language with recurrent models"},{"paperId":"31f48073366d5fad7b1c9e60af315690475b52f6","title":"RNNs as psycholinguistic subjects: Syntactic state and grammatical dependency"},{"paperId":"5fc548f3f3112de7eddad3744717dc2f9d22ca38","title":"Neural Arithmetic Logic Units"},{"paperId":"843c6b0a35b02e2c3d74bb545e74bc655e16e992","title":"Assessing Composition in Sentence Vector Representations"},{"paperId":"210feb22ff541920caa4884e73eaff1c09644114","title":"Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks"},{"paperId":"395f0f8676a0b41360fbfe3a001b9f8c8dfcd24c","title":"Extrapolation in NLP"},{"paperId":"c7a46543829b59a9423a067ca798c63b872eb613","title":"Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing"},{"paperId":"08fbb1b4cfdc83977d2c8f08bdfb663f13c0e60a","title":"Memorize or generalize? Searching for a compositional RNN in a haystack"},{"paperId":"3c092128a2c98e5e3be5f8872cf05c635430cd60","title":"Evaluating Compositionality in Sentence Embeddings"},{"paperId":"fc05801280853ff6f6a15c55d9b76d8c58182f39","title":"Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction"},{"paperId":"744c22e95eaceabb031589ba808bf2e9f24b04c7","title":"From Sequence to Attention Search for a Compositional Bias in Sequence-to-Sequence Models by Kristian Korrel"},{"paperId":"0d45e411a7b588b70b82f22b04c7ccfaef0e3fd7","title":"SYSTEMATIC GENERALIZATION: WHAT IS REQUIRED"},{"paperId":"0c43dfe8a834fce0467ba6a74b2daeebb5bb8b53","title":"On internal language representations in deep learning: an analysis of machine translation and speech recognition"},{"paperId":"acf13c52c86a3b38642ba0c6cbcd1b771778965c","title":"NAACL HLT 2018 Generalization in the Age of Deep Learning Proceedings of the Workshop"},{"paperId":"0fff5c49c05c27c22ac7685130197146491f0b36","title":"The Consciousness Prior"}],"references":[{"paperId":"c6c171d2a9be192d60af7b434e4ba2fcbbad7f48","title":"Memory-augmented Neural Machine Translation"},{"paperId":"2e17cf6a339fd071ad222062f868e882ef4120a4","title":"Inferring and Executing Programs for Visual Reasoning"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"a396a6febdacb84340d139096455e67049ac1e22","title":"Learning to Reason: End-to-End Module Networks for Visual Question Answering"},{"paperId":"c889d6f98e6d79b89c3a6adf8a921f88fa6ba518","title":"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"},{"paperId":"bc7fcefa3e333d50463d524406d107060c4a0cec","title":"Neural Semantic Parsing over Multiple Knowledge-bases"},{"paperId":"53bb7789be36a58f865f2ec84f6d8f816ddaae6a","title":"Learning to Learn"},{"paperId":"784ee73d5363c711118f784428d1ab89f019daa5","title":"Hybrid computing using a neural network with dynamic external memory"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"1a327709cc53ff9e52454e50a643abf4a0ac92af","title":"Findings of the 2016 Conference on Machine Translation"},{"paperId":"b7eac64a8410976759445cce235469163d23ee65","title":"Data Recombination for Neural Semantic Parsing"},{"paperId":"9e7ad19160313552175f7dc3e5acf94a430f66ac","title":"The Architecture of Cognition: Rethinking Fodor and Pylyshyn’s Systematicity Challenge"},{"paperId":"7260c0692f8d265e11c4e9c4c8ef4c185bd587ad","title":"Building machines that learn and think like people"},{"paperId":"558ac446dc26bee9789d660a251b75728cb6eeb2","title":"Language to Logical Form with Neural Attention"},{"paperId":"3457ddb9b9f614aad52052d680f9b11c08b3a4cf","title":"A Roadmap Towards Machine Intelligence"},{"paperId":"b59d91e0699d4e1896a15bae13fd180bdaf77ea5","title":"Neural Programmer-Interpreters"},{"paperId":null,"title":"TreeStructured Composition in Neural Networks without TreeStructured Architectures"},{"paperId":null,"title":"Neural programmerinterpreters"},{"paperId":"04d1a26c2516dc14a765112a63ec60dc3cb3de72","title":"Tree-Structured Composition in Neural Networks without Tree-Structured Architectures"},{"paperId":"d38e8631bba0720becdaf7b89f79d9f9dca45d82","title":"Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"adfcf065e15fd3bc9badf6145034c84dfb08f204","title":"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"a2f09447e8f1ceda391101e5ae7f863a8f2b2836","title":"Efficient Gradient-Based Inference through Transformations between Bayes Nets and Neural Nets"},{"paperId":"b4ff85584c7b5f487c735a131240c12d1baed9c2","title":"Getting real about systematicity"},{"paperId":"9ec7bdd58594c5da03b967c9f91dc31c5bd394c0","title":"A fundamental limitation of the conjunctive codes learned in PDP models of cognition: comment on Botvinick and Plaut (2006)."},{"paperId":"a83699a15ce3364aeb0ccd049615adc90d04f50b","title":"Empirical and computational support for context-dependent representations of serial order: reply to Bowers, Damian, and Davis (2009)."},{"paperId":"2b68712e29750f7ad530edd13e4eb1e3a67794cc","title":"How novelty search escapes the deceptive trap of learning to learn"},{"paperId":"4731f53d939c190653f99f434886d358f196732b","title":"37. Distributions in text"},{"paperId":"15c460439979d1ddf617ad343308359532f316d2","title":"Connectionist semantic systematicity"},{"paperId":"06c10fcd7cfce4614f8460298820a65b5b8e1818","title":"Strong systematicity in sentence processing by simple recurrent networks"},{"paperId":null,"title":"Distributions in text"},{"paperId":"3ccaa9d20e1f16f6c818853a970755ce888df792","title":"Generalisation towards Combinatorial Productivity in Language Acquisition by Simple Recurrent Networks"},{"paperId":"08f2d51806af94338c4cc35024d553b362da797b","title":"Short-term memory for serial order: a recurrent neural network model."},{"paperId":"dfc79017e52efb270155ce8b93337467804cb697","title":"Constructions at Work: The Nature of Generalization in Language"},{"paperId":"a56ee505e5651d90210d7f19adb77b1a6a424d3a","title":"Lack of combinatorial productivity in language processing with simple recurrent networks"},{"paperId":"6843890926bf0e5c887ffc78dcb1203135981bf1","title":"The compositionality papers"},{"paperId":"2ced1a9aa4fd00b8da18b7b9c8f2274bd12ff608","title":"Symbolically speaking: a connectionist model of sentence production"},{"paperId":"a6383f155fa9d3e9b15092bfefbf613f982eb263","title":"The Algebraic Mind: Integrating Connectionism and Cognitive Science"},{"paperId":"08dc7b19e679539f0f93db0192a8e8d11538b3dd","title":"Rethinking Eliminative Connectionism"},{"paperId":"e78dd4aa91817bbeb2fd87c82ae4db91c23b0997","title":"Are Feedforward and Recurrent Networks Systematic? Analysis and Implications for a Connectionist Cognitive Architecture"},{"paperId":null,"title":"Rethinking Eliminative Connectionism. Cognitive Psychology"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":null,"title":"Long short - term mem"},{"paperId":"ae1f906de7d0136acb8ba7b42665e9bdad245619","title":"Generalization and connectionist language learning"},{"paperId":"668087f0ae7ce1de6e0bd0965dbb480c08103260","title":"Finding Structure in Time"},{"paperId":"ce9a21b93ba29d4145a8ef6bf401e77f261848de","title":"A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":null,"title":"Syntactic Structures. Mouton, Berlin, Germany"},{"paperId":null,"title":"Syntactic Structures"}],"id":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","summary":"This paper introduces the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences, and tests the zero-shot generalization capabilities of a variety of recurrent neural networks trained on SCAN with sequence-to-sequence methods."},{"url":"https://www.semanticscholar.org/paper/b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation","venue":"EMNLP","year":2020,"referenceCount":58,"citationCount":93,"influentialCitationCount":16,"publicationDate":"10/12/2020","authors":"Najoung Kim,Tal Linzen","citations":[{"paperId":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models"},{"paperId":"04ca71d9089b2e86f9e4a874fd66ee8bd0baba8e","title":"When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks"},{"paperId":"c6e4518dfd687a2a5bed4e78d5d9f999292a1746","title":"Counterfactual Recipe Generation: Exploring Compositional Generalization in a Realistic Scenario"},{"paperId":"a638036c48cbd76ae3af30d6b273d49bb28a22e4","title":"Schrödinger's tree—On syntax and neural language models"},{"paperId":"76e2b3b6e1da49764d342ea922290410162125ca","title":"Measures of Information Reﬂect Memorization Patterns"},{"paperId":"4129fb99f5c8785452d60ddaf29d178525e29d0a","title":"Measures of Information Reflect Memorization Patterns"},{"paperId":"711d5e8ddbb840ad31a9ffa3d38590603ba69a92","title":"Prompting GPT-3 To Be Reliable"},{"paperId":"1ed29beb55b10de8553c926ce6da2625ec2c8776","title":"Benchmarking Long-tail Generalization with Likelihood Splits"},{"paperId":"b1f33e956e36bf25e118c0d537dcc519cfe52e60","title":"CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations"},{"paperId":"1bd799cf462f926041dd2fc8fbe4af54bddbf5c5","title":"Translate First Reorder Later: Leveraging Monotonicity in Semantic Parsing"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","title":"Compositional Generalisation with Structured Reordering and Fertility Layers"},{"paperId":"61d56ece2d19f4bfeb322c92085fb28521e169da","title":"Neural-Symbolic Recursive Machine for Systematic Generalization"},{"paperId":"40047a74b707743157051d38f76061ba5ff9aab4","title":"Compositional Semantic Parsing with Large Language Models"},{"paperId":"60f208b19bb63d82fda5759897677f92d4e1e2fc","title":"Improving Compositional Generalization in Math Word Problem Solving"},{"paperId":"1a7a24c73521eecf0a2d555e921b27e2c4d8e3c3","title":"What Artificial Neural Networks Can Tell Us About Human Language Acquisition"},{"paperId":"22f9a5fe8e446d215530fb90ea08b10499b36b0b","title":"Unit Testing for Concepts in Neural Networks"},{"paperId":"d2018d1b0f69ca6805aa18a22be95cab9b5c44c7","title":"On Neural Architecture Inductive Biases for Relational Tasks"},{"paperId":"96486c71106f0a61ebe02061c32f9fb80c227011","title":"Linear Connectivity Reveals Generalization Strategies"},{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"a122909a31acf41cb2d9eb602c01b24b9b85a061","title":"LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing"},{"paperId":"1167b3864046b732cf057b8b05db311e726cadab","title":"Measuring Alignment Bias in Neural Seq2seq Semantic Parsers"},{"paperId":"bc16284f517dd0011dcf64ea1c8fe6d6576494a4","title":"Is the Computation of Abstract Sameness Relations Human-Like in Neural Language Models?"},{"paperId":"f2611a09cf0942170785ee3025cb511de3bdec2e","title":"Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems"},{"paperId":"6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis"},{"paperId":"66f3f0e8ebc780e570770986f50bf9cb9cd53ec1","title":"WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series Tasks"},{"paperId":"4c430e6c3a72626bd4cb1893960c7c26dfec6c79","title":"Structurally Diverse Sampling Reduces Spurious Correlations in Semantic Parsing Datasets"},{"paperId":"69078af65fc934f81fd340e9d1323d6c08194548","title":"Revisiting the Compositional Generalization Abilities of Neural Sequence Models"},{"paperId":"557ebd17b7c7ac4e09bd167d7b8909b8d74d1153","title":"Compositional Generalization Requires Compositional Parsers"},{"paperId":"69df5b68fbf492341336b39b4cc9fcc74fff4d5f","title":"Improving Systematic Generalization Through Modularity and Augmentation"},{"paperId":"2b060b89324c376892a096c84fd14664f7b71710","title":"Understanding Robust Generalization in Learning Regular Languages"},{"paperId":"39f604fdd3ade5bd5a67d5284a6d9c12e535db85","title":"Compositionality as Lexical Symmetry"},{"paperId":"03eeff98d24383518ce0dacc0b3c4a38b6f1a514","title":"Recursive Decoding: A Situated Cognition Approach to Compositional Generation in Grounded Language Understanding"},{"paperId":"9fe39e0f2cf3b6d5f70a379654f9c08ffa48ddee","title":"Unobserved Local Structures Make Compositional Generalization Hard"},{"paperId":"ace2a00425f96e9d0dbbe2869023d56c6c91267f","title":"On Learning Interpreted Languages with Recurrent Models"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"40b4d98588719407fb72a014ab79e4145695654b","title":"Quantifying Adaptability in Pre-trained Language Models with 500 Tasks"},{"paperId":"e968a3c9590481cd13f2f86a7ac8839e3cf3455f","title":"Improving Compositional Generalization with Self-Training for Data-to-Text Generation"},{"paperId":"8008348e87d3904842a2dd230c14b83112e8bf48","title":"Compositional Generalization in Dependency Parsing"},{"paperId":"ab72bccf6f3981537389510ecc609109e79595c3","title":"Disentangled Sequence to Sequence Learning for Compositional Generalization"},{"paperId":"aead4418733b998792deb9cbf198a834449e00d2","title":"Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics"},{"paperId":"b3f644a5ea1fdd8cec1c34ebed69125838a50de3","title":"The Paradox of the Compositionality of Natural Language: A Neural Machine Translation Case Study"},{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"ad331dce175b1d38d6516455013c1ec0e26e606b","title":"Compositional Generalization in Multilingual Semantic Parsing over Wikidata"},{"paperId":"04833b92c9002f241b8f8b956d018759eebc85b3","title":"Tailor: Generating and Perturbing Text with Semantic Controls"},{"paperId":"10e824b010c2125b13decef30cf1dceec6196817","title":"ANLIzing the Adversarial Natural Language Inference Dataset"},{"paperId":"90c1a63aada7704eadc4324c16a66ec793d4b698","title":"Compositional generalization with a broad-coverage semantic parser"},{"paperId":"d129841cb2e30e25000dcd9edb83c880fc4babc1","title":"Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention"},{"paperId":"45291baa6a05ef8319e07442de5f6a5c85c16611","title":"SemEval-2022 Task 9: R2VQ – Competence-based Multimodal Question Answering"},{"paperId":"a143cac1bc440135b612132c89e603f364b8a3b7","title":"Combine to Describe: Evaluating Compositional Generalization in Image Captioning"},{"paperId":"dc88d2bbcebd810d7c80ba281739908005b12235","title":"Neurocompositional computing in human and machine intelligence: A tutorial"},{"paperId":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"DECOMPOSITION IN NEURAL PROGRAM SYNTHESIS"},{"paperId":"a576512a7562597fd30719a834d5866d010ef6ab","title":"Compositional Generalization for Natural Language Interfaces to Web APIs"},{"paperId":"fcf25e1affc2f8ee5bb49d156f174e9769234deb","title":"Systematic Generalization with Edge Transformers"},{"paperId":"cbf98ebe967e0f3f3236e7932f37013b98244e94","title":"ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning"},{"paperId":"04db9b694280134f09af5fa787a306907edba29d","title":"How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN"},{"paperId":"c764ecba2bace12b9bfb9c2b0651a12ff6888ea7","title":"Learning to Generalize Compositionally by Transferring Across Semantic Parsing Tasks"},{"paperId":"00050c15896e8ae6bb534f10d072351547993f72","title":"LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing"},{"paperId":"4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80","title":"How BPE Affects Memorization in Transformers"},{"paperId":"bb0ab8591d6d57c7e2bd1ec35d806b3f25277752","title":"Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks"},{"paperId":"76c9558b3fa10baf0e094386a650015b29a8a4bc","title":"Compositional generalization in semantic parsing with pretrained transformers"},{"paperId":"c2dda1f632072a331a50398d613079b71d76ee95","title":"Transformers Generalize Linearly"},{"paperId":"06fbeaf4d16639f177973a06cd7c4f78cb5e38ed","title":"COVR: A Test-Bed for Visually Grounded Compositional Generalization with Real Images"},{"paperId":"3962f108081b22c7e54b413f47ba6f2c16f2cc05","title":"Frequency Effects on Syntactic Rule Learning in Transformers"},{"paperId":"676fa805bd715591f99bb17e36d673a6a14e92fe","title":"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization"},{"paperId":"b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347","title":"Sequence-to-Sequence Learning with Latent Neural Grammars"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"45496cd0b256b75bfbe3bd95890b496069c7821c","title":"Multilingual Compositional Wikidata Questions"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","title":"Lexicon Learning for Few Shot Sequence Modeling"},{"paperId":"2a6ea3d8ad7ef4fb1ca0b1568748c284c156eb42","title":"It's the Meaning That Counts: The State of the Art in NLP and Semantics"},{"paperId":"577d44a10b424a55165a6bf4839bafce2c695302","title":"SyGNS: A Systematic Generalization Testbed Based on Natural Language Semantics"},{"paperId":"52b1cf563d1368f72e82b91b0349a7012a746f4f","title":"On the Interplay Between Fine-tuning and Composition in Transformers"},{"paperId":"ef06ddd84acea3c6dddd5a86eb500f11f828e07a","title":"Designing Multimodal Datasets for NLP Challenges"},{"paperId":"2365410a710b421b2295cdca0074946cb50bb1d4","title":"Are Pre-trained Convolutions Better than Pre-trained Transformers?"},{"paperId":"40c3327a6ddb0603b6892344509c7f428ab43d81","title":"Documenting the English Colossal Clean Crawled Corpus"},{"paperId":"77a096d80eb4dd4ccd103d1660c5a5498f7d026b","title":"Dynabench: Rethinking Benchmarking in NLP"},{"paperId":"eaa88d697f92739f3569564329e9d037aabbe2d7","title":"A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics"},{"paperId":"5505d608a1d482fdc083796db812379ec1cb8723","title":"Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches"},{"paperId":"0d39d525f30609d0541330f933007025cd457a83","title":"Exploring Transitivity in Neural NLI Models through Veridicality"},{"paperId":"40848b41ed8c9c255ecd8a920006877691b52d03","title":"WILDS: A Benchmark of in-the-Wild Distribution Shifts"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"642492003112a47b0bf86d60fac5507bc3b35a49","title":"Are Pretrained Convolutions Better than Pretrained Transformers?"},{"paperId":"0b1470014bdbaa80ba63da0491d9db6c7d4febcc","title":"Detecting Compositionally Out-of-Distribution Examples in Semantic Parsing"},{"paperId":"a406701b5fb05be55244d4f940db7be55fce85c6","title":"Semantic Systematicity in Connectionist Language Production"},{"paperId":"7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8","title":"AND does not mean OR: Using Formal Languages to Study Language Models’ Representations"},{"paperId":"21210677494669cc83a7878fbd3a6360ce02f7f3","title":"Testing for Grammatical Category Abstraction in Neural Language Models"},{"paperId":"4b58367375466e653751a0c258b2f50bd3551408","title":"Sequence-to-Sequence Networks Learn the Meaning of Reflexive Anaphora"},{"paperId":"6d0cc01e4bdf18b86bb74d1c6d9a41b5a4890c58","title":"Priorless Recurrent Networks Learn Curiously"},{"paperId":"0dc5dd7c64ee016bdc33a5f32dc25747be5ca702","title":"From SCAN to Real Data: Systematic Generalization via Meaningful Learning"},{"paperId":"6f0be1f9bda7530b1fa654cac84d595ca9d53740","title":"Revisit Systematic Generalization via Meaningful Learning"},{"paperId":"79cb080c84da314c2113692585b1e9ee29afa33a","title":"On learning an interpreted language with recurrent models"}],"references":[{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"815a3d56401483b635cfad9468852cddb46350ee","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":"ba310a9b9f7b812478b2a08dbd9917ea937a4e36","title":"Mutual exclusivity as a challenge for deep neural networks"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"356645552f8f40adf5a99b4e3a69f47699399010","title":"Quantity doesn’t buy quality syntax with neural language models"},{"paperId":"2030b07a36a77821b978ff8a20a14d88c1c60db1","title":"Mutual exclusivity as a challenge for neural networks"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"4043a936960de8e149dc208178fe1bcb157c7fa4","title":"Recent Advances in Natural Language Inference: A Survey of Benchmarks, Resources, and Approaches"},{"paperId":"42ed4a9994e6121a9f325f5b901c5b3d7ce104f5","title":"Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"},{"paperId":"bc488b437a53849c54c93e1a4c0eb87a942ddd2c","title":"Demonstratives"},{"paperId":"4d00097433a538002b36cfd7a621daddde3e4c0d","title":"Targeted Syntactic Evaluation of Language Models"},{"paperId":"3d42ddf7c5ce59ae04d1d27085be9f736d1be04b","title":"Colorless Green Recurrent Networks Dream Hierarchically"},{"paperId":"bdea8b6ceabaeb86bd23c2d2585da1ff3858d968","title":"Can Neural Networks Understand Logical Entailment?"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":null,"title":"A sequence-tosequence model for semantic role labeling"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"faee0c81a1170402b149500f1b91c51ccaf24027","title":"Universal Semantic Parsing"},{"paperId":"aab5002a22b9b4244a8329b140bd0a86021aa2d1","title":"OpenNMT: Open-Source Toolkit for Neural Machine Translation"},{"paperId":"03eb382e04cca8cca743f7799070869954f1402a","title":"CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"},{"paperId":null,"title":"2017) as closely as possible. The training time for each model was around 1 to 2 hours on a single NVIDIA K80"},{"paperId":null,"title":"Mark Steedman, and Mirella Lapata"},{"paperId":"558ac446dc26bee9789d660a251b75728cb6eeb2","title":"Language to Logical Form with Neural Attention"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"7c05a4ffee7e159e34b2efea7e44d994333ec628","title":"Recursive Neural Networks Can Learn Logical Semantics"},{"paperId":"15e8358e7be027a7bdfa5ad7a5f794a41456c83b","title":"Syntactic generalization with novel intransitive verbs."},{"paperId":"9ee949dc55a12849afb60c2d883e0bbc11d781df","title":"Zipf’s word frequency law in natural language: A critical review and future directions"},{"paperId":"187b06d8a18cbf7c5d9df4e232bd6649fc3f5a27","title":"MacArthur‐Bates Communicative Development Inventories"},{"paperId":"571a82875226d4e2f6189e25b346bb48b70809b2","title":"Once is Enough: N400 Indexes Semantic Integration of Novel Word Meanings from a Single Exposure in Context"},{"paperId":"84e3829aaa9d60cd42b2bc45ad1b67b63b23e14e","title":"The learnability of abstract syntactic principles"},{"paperId":null,"title":"PP modification on the subject of a declarative sentence occurred only 13 times whereas PP modification on the object occurred over 100 times"},{"paperId":"0831500f97b619e75a15160e0fe84729726d95f4","title":"Learning to use words: Event-related potentials index single-shot contextual word learning"},{"paperId":"4ed5b385d3c56f021833651f5ccbd37f3bf57c85","title":"Predicting syntax: Processing dative constructions in American and Australian varieties of English"},{"paperId":"c73dc3e30849e72f5465a681ad2bdb3878d0fcf5","title":"Syntactic recursion and iteration"},{"paperId":"5de1e28a1c538142b7c450194cc0a36b4934a45b","title":"Wide-Coverage Semantic Analysis with Boxer"},{"paperId":"e73da7791d71552a39e26d1ddbe725163ed7e51e","title":"Frequency of Basic English Grammatical Structures: A Corpus Analysis."},{"paperId":"9779c706c7e9e4b3ed14f65da70896ad59577887","title":"Early syntactic productivity: Evidence from dative shift"},{"paperId":"3adeb68c3a172de451433ea097576a1f32e331c6","title":"Situation variables and licensing by modification in opaque demonstratives"},{"paperId":"16179fb9a36e1eafb4d2d78329210397fda0d56f","title":"Individual differences in preschoolers ' ability to generalize unaccusative intransitive constructions in novel verb experiments : Evidence from their familiar verb usage in naturalistic play contexts"},{"paperId":"14f0c5bf45ad0dc55c80c8a772484d7bd3d6ee20","title":"MacArthur-Bates Communicative Development Inventories"},{"paperId":"a5dd6c4943f8cf774f9da441f9db73a1217f19e9","title":"Young children ’ s use of unaccusative intransitives in novel verb experiments"},{"paperId":"99d2dcdcf4cf05facaa101a48c7e31d140b4736d","title":"The Proposition Bank: An Annotated Corpus of Semantic Roles"},{"paperId":"bb6898d6041e97c4946661b3a3df0f82286a43b5","title":"Verbnet: a broad-coverage, comprehensive verb lexicon"},{"paperId":"beaca3493aed271bdfc42490fd22dd11cb40ce0e","title":"The faculty of language: what is it, who has it, and how did it evolve?"},{"paperId":"da75b3cbeeb22c75c5b2b66af9446718c6c1565d","title":"The (Non)Necessity of Recursion in Natural Language Processing"},{"paperId":"003969511c37448306fb951dba378c6c8561f4ce","title":"Word Frequencies in Written and Spoken English: based on the British National Corpus"},{"paperId":"45313a157dcaa1103eaa53f099204c66c7dd9cd9","title":"Toward a connectionist model of recursion in human linguistic performance"},{"paperId":"83fc20d0f34e794433079fe38da914d41cf84a87","title":"Young children learn to produce passives with nonce verbs."},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"f3c17a95ea5a5db6c8bcfe07d497b6c469ffa732","title":"Severing the External Argument from its Verb"},{"paperId":"0736a262b557c86e8d14cc7577bb94de5067d65e","title":"Systematicity in Connectionist Language Learning"},{"paperId":"59b39a93dd135b93c2d109692e72624fb8a4a9a4","title":"Twenty-Three-Month-Old Children Have a Grammatical Category of Noun."},{"paperId":"6cbc1eb25f4ab29a613418b3b0740e74141a0f17","title":"English Verb Classes and Alternations: A Preliminary Investigation"},{"paperId":null,"title":"Events in the Semantics of English, volume 334"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":null,"title":"Syntactic theory and the projection problem"},{"paperId":"1b5ad278a01a2c91f35c4c86fbbffc09d6fe2d72","title":"ENGLISH AS A FORMAL LANGUAGE"},{"paperId":null,"title":"Short-term memory limitations on decoding selfembedded sentences"}],"id":"b20ddcbd239f3fa9acc603736ac2e4416302d074","summary":"In experiments with Transformers and LSTMs, it is found that in-distribution accuracy on the COGS test set was near-perfect, but generalization accuracy was substantially lower, and the dataset showed high sensitivity to random seed."},{"url":"https://www.semanticscholar.org/paper/70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize","venue":"ACL","year":2021,"referenceCount":46,"citationCount":29,"influentialCitationCount":5,"publicationDate":"06/08/2021","authors":"Henry Conklin,Bailin Wang,Kenny Smith,Ivan Titov","citations":[{"paperId":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models"},{"paperId":"b49ebf36a29cf9734313066129ab0d7092d4041e","title":"Categorizing Semantic Representations for Neural Machine Translation"},{"paperId":"ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","title":"Compositional Generalisation with Structured Reordering and Fertility Layers"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"40047a74b707743157051d38f76061ba5ff9aab4","title":"Compositional Semantic Parsing with Large Language Models"},{"paperId":"e7b025f8bcd7e7ca0db65f666a99524d738a4717","title":"Exploring diversity in back translation for low-resource machine translation"},{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"a122909a31acf41cb2d9eb602c01b24b9b85a061","title":"LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing"},{"paperId":"bc16284f517dd0011dcf64ea1c8fe6d6576494a4","title":"Is the Computation of Abstract Sameness Relations Human-Like in Neural Language Models?"},{"paperId":"a40693eefd351659cdeb3885917b1506ea01c38a","title":"Measuring and Improving Compositional Generalization in Text-to-SQL via Component Alignment"},{"paperId":"43b7437ed33a29d3d90239ad66f325a465ff7e91","title":"Meta Learning for Natural Language Processing: A Survey"},{"paperId":"6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis"},{"paperId":"4c430e6c3a72626bd4cb1893960c7c26dfec6c79","title":"Structurally Diverse Sampling Reduces Spurious Correlations in Semantic Parsing Datasets"},{"paperId":"69078af65fc934f81fd340e9d1323d6c08194548","title":"Revisiting the Compositional Generalization Abilities of Neural Sequence Models"},{"paperId":"1d41a0ddda57caa6c8d268dd1703e4c9b35db18b","title":"One-Shot Learning from a Demonstration with Hierarchical Latent Language"},{"paperId":"557ebd17b7c7ac4e09bd167d7b8909b8d74d1153","title":"Compositional Generalization Requires Compositional Parsers"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"ab72bccf6f3981537389510ecc609109e79595c3","title":"Disentangled Sequence to Sequence Learning for Compositional Generalization"},{"paperId":"90c1a63aada7704eadc4324c16a66ec793d4b698","title":"Compositional generalization with a broad-coverage semantic parser"},{"paperId":"a143cac1bc440135b612132c89e603f364b8a3b7","title":"Combine to Describe: Evaluating Compositional Generalization in Image Captioning"},{"paperId":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"DECOMPOSITION IN NEURAL PROGRAM SYNTHESIS"},{"paperId":"c764ecba2bace12b9bfb9c2b0651a12ff6888ea7","title":"Learning to Generalize Compositionally by Transferring Across Semantic Parsing Tasks"},{"paperId":"00050c15896e8ae6bb534f10d072351547993f72","title":"LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing"},{"paperId":"bb0ab8591d6d57c7e2bd1ec35d806b3f25277752","title":"Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks"},{"paperId":"676fa805bd715591f99bb17e36d673a6a14e92fe","title":"Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization"},{"paperId":"b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347","title":"Sequence-to-Sequence Learning with Latent Neural Grammars"},{"paperId":"c735740b26ceaa4db9d77233116434c0e8b311d8","title":"Learning Adaptive Control Flow in Transformers for Improved Systematic Generalization"},{"paperId":"a406701b5fb05be55244d4f940db7be55fce85c6","title":"Semantic Systematicity in Connectionist Language Production"},{"paperId":"6f0be1f9bda7530b1fa654cac84d595ca9d53740","title":"Revisit Systematic Generalization via Meaningful Learning"}],"references":[{"paperId":"95c20f35d352f23b19c378c0758b8dc1d7622872","title":"On Aspects of the Theory of Syntax"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"19bd467b1c8de94b9bdaef1499788467937f594e","title":"Meta-Learning for Domain Generalization in Semantic Parsing"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"68d5ffe0637e704ab2ff25e93e631f76790c5707","title":"Characterizing Structural Regularities of Labeled Data in Overparameterized Models"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"bb82a8d24d8a564cbbbe04e6752451e8260a966a","title":"Understanding Human Intelligence through Human Limitations"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"d70af4990cba2574c41b1235030f7a5b702e2d70","title":"Compositionality and Generalization In Emergent Languages"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"0c5bc409e62e65f86838968a2a7cdae5fa0b288b","title":"RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":null,"title":"Underspecification Presents Challenges for Credibility"},{"paperId":null,"title":"Learning to recombine and resam"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"4d031258a66076187001b4d6182345198624d872","title":"The compositionality of neural networks: integrating symbolism and connectionism"},{"paperId":"e962c301df1d33bc12d8115f4c82093103c94eeb","title":"Model-Agnostic Meta-Learning for Relation Classification with Limited Supervision"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"efef34c1caef102ad5cc052642d75beaaf5adcaf","title":"Deep RNNs Encode Soft Hierarchical Syntax"},{"paperId":"97856a4c31fec7b189446a130aab4cbfa8d6a3e8","title":"Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":null,"title":"Learning to generalize: Metalearning for domain generalization"},{"paperId":"dcb028149bb3cf934fbd2e4cbb773ffbb9b0e49d","title":"Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2caa021d85d4878d3369000e0068f617576d6cca","title":"Natural Language Does Not Emerge ‘Naturally’ in Multi-Agent Dialog"},{"paperId":"c889d6f98e6d79b89c3a6adf8a921f88fa6ba518","title":"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"},{"paperId":"29c887794eed2ca9462638ff853e6fe1ab91d5d8","title":"Optimization as a Model for Few-Shot Learning"},{"paperId":"be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6","title":"Matching Networks for One Shot Learning"},{"paperId":"b7eac64a8410976759445cce235469163d23ee65","title":"Data Recombination for Neural Semantic Parsing"},{"paperId":"558ac446dc26bee9789d660a251b75728cb6eeb2","title":"Language to Logical Form with Neural Attention"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"0cddfeeab92d9abbd9b42065e916fe1995a7c2e6","title":"Formal Semantics : an Introduction"},{"paperId":"c393c48a2d634acb1daee3567eaaea733a1224fb","title":"Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees"},{"paperId":"e6c7adc28e20d361d5c35aa9808094b10f6a34d1","title":"Convolution Kernels for Natural Language"},{"paperId":"f330f1f472f860212b980bb9be81eff884f7f0e1","title":"Text Classification using String Kernels"},{"paperId":"5ee0d8aeb2cb01ef4d8a858d234e72a7400c03ac","title":"Convolution kernels on discrete structures"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"277ff0c74cc72663d0aabbeae25a3e97b245457c","title":"Simple Fast Algorithms for the Editing Distance Between Trees and Related Problems"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":"de28c165623adabcdba0fdb18b65eba685aaf31d","title":"On Estimation of a Probability Density Function and Mode"}],"id":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","summary":"A meta-learning augmented version of supervised learning whose objective directly optimizes for out-of-distribution generalization is implemented, and Experimental results on the COGS and SCAN datasets show that this similarity-driven meta- learning can improve generalization performance."},{"url":"https://www.semanticscholar.org/paper/ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers","venue":"EMNLP","year":2021,"referenceCount":55,"citationCount":46,"influentialCitationCount":10,"publicationDate":"08/26/2021","authors":"R. Csordás,Kazuki Irie,J. Schmidhuber","citations":[{"paperId":"dbe286676d094ca588312cbfc8f699a9a2ca1cc9","title":"Structural generalization is hard for sequence-to-sequence models"},{"paperId":"92394181881a9ff4063d9aedb3e4fd4ada466edb","title":"Functional Indirection Neural Estimator for Better Out-of-distribution Generalization"},{"paperId":"04ca71d9089b2e86f9e4a874fd66ee8bd0baba8e","title":"When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks"},{"paperId":"97833e2aa0da5240e62436373b58af988a4ab6ab","title":"The Curious Case of Absolute Position Embeddings"},{"paperId":"1ed29beb55b10de8553c926ce6da2625ec2c8776","title":"Benchmarking Long-tail Generalization with Likelihood Splits"},{"paperId":"b49ebf36a29cf9734313066129ab0d7092d4041e","title":"Categorizing Semantic Representations for Neural Machine Translation"},{"paperId":"b1f33e956e36bf25e118c0d537dcc519cfe52e60","title":"CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations"},{"paperId":"ef2522f15cafab8bafbabcd02ea8bf0fad6913b2","title":"Compositional Generalisation with Structured Reordering and Fertility Layers"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"61d56ece2d19f4bfeb322c92085fb28521e169da","title":"Neural-Symbolic Recursive Machine for Systematic Generalization"},{"paperId":"837cc9a366c873c84ceec7e84d5cb3d5753757d6","title":"Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks"},{"paperId":"0ac7966e8146799d49c9d5212bbd20bc85054bd8","title":"Training on the Test Set: Mapping the System-Problem Space in AI"},{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"a122909a31acf41cb2d9eb602c01b24b9b85a061","title":"LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing"},{"paperId":"6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis"},{"paperId":"aa8f3e081ad2869c9469e2726364bdae0d9bdc7f","title":"Fusing finetuned models for better pretraining"},{"paperId":"5021fd710fd17dee53bc7bc7bf334b148ef3d8b6","title":"LogicInference: A New Dataset for Teaching Logical Inference to seq2seq Models"},{"paperId":"66f3f0e8ebc780e570770986f50bf9cb9cd53ec1","title":"WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series Tasks"},{"paperId":"4c430e6c3a72626bd4cb1893960c7c26dfec6c79","title":"Structurally Diverse Sampling Reduces Spurious Correlations in Semantic Parsing Datasets"},{"paperId":"69078af65fc934f81fd340e9d1323d6c08194548","title":"Revisiting the Compositional Generalization Abilities of Neural Sequence Models"},{"paperId":"557ebd17b7c7ac4e09bd167d7b8909b8d74d1153","title":"Compositional Generalization Requires Compositional Parsers"},{"paperId":"9fe39e0f2cf3b6d5f70a379654f9c08ffa48ddee","title":"Unobserved Local Structures Make Compositional Generalization Hard"},{"paperId":"5626e1db3d4fa8f8de79b604ce9fb8eb96a75883","title":"Improving Compositional Generalization with Latent Structure and Data Augmentation"},{"paperId":"b8b813111c411ae61881ab9cd25707d9de6444ec","title":"Compositional Attention: Disentangling Search and Retrieval"},{"paperId":"e528466e2aff981511d4ca6e063211297c0b4175","title":"The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization"},{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"90c1a63aada7704eadc4324c16a66ec793d4b698","title":"Compositional generalization with a broad-coverage semantic parser"},{"paperId":"20867dd60552674beac1edc7fa029d2d1b8fd03a","title":"MAQA: A Multimodal QA Benchmark for Negation Anonymous Author(s)"},{"paperId":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"DECOMPOSITION IN NEURAL PROGRAM SYNTHESIS"},{"paperId":"a77468f6bd4db7f8d761a0569d9cc29d5a8f0034","title":"L OGIC I NFERENCE : A N EW D ATASET FOR T EACHING L OGICAL I NFERENCE TO SEQ 2 SEQ M ODELS"},{"paperId":"42ab2e42221a7fbd66ba368cf90b5e63b5270010","title":"Improving Baselines in the Wild"},{"paperId":"2c33f2aed89a7b04c2509b897e5fcccccdb2b7b6","title":"Assistive Tele-op: Leveraging Transformers to Collect Robotic Task Demonstrations"},{"paperId":"fcf25e1affc2f8ee5bb49d156f174e9769234deb","title":"Systematic Generalization with Edge Transformers"},{"paperId":"bfd1752963697520ceb484a8b8c65b9dba99ca96","title":"TraVLR: Now You See It, Now You Don't! Evaluating Cross-Modal Transfer of Visio-Linguistic Reasoning"},{"paperId":"00050c15896e8ae6bb534f10d072351547993f72","title":"LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing"},{"paperId":"3d5699e7f7e085ad72102859b06fa4884d207e77","title":"Iterative Decoding for Compositional Generalization in Transformers"},{"paperId":"76c9558b3fa10baf0e094386a650015b29a8a4bc","title":"Compositional generalization in semantic parsing with pretrained transformers"},{"paperId":"af749e5dbde38914ca6fa1463fca17eac8f69ecc","title":"ReaSCAN: Compositional Reasoning in Language Grounding"},{"paperId":"b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347","title":"Sequence-to-Sequence Learning with Latent Neural Grammars"},{"paperId":"86589b6286ef3c55b8b4fccfb41a3b30b7afdf61","title":"Going Beyond Linear Transformers with Recurrent Fast Weight Programmers"},{"paperId":"72f207c777e4a17180cc54ccc6a743d5f43227af","title":"Choose a Transformer: Fourier or Galerkin"},{"paperId":"eaa88d697f92739f3569564329e9d037aabbe2d7","title":"A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics"},{"paperId":"269de1d1e26559613fa4b02320aefc07bb2d556b","title":"Enhancing the Transformer Decoder with Transition-based Syntax"},{"paperId":"c735740b26ceaa4db9d77233116434c0e8b311d8","title":"Learning Adaptive Control Flow in Transformers for Improved Systematic Generalization"},{"paperId":"6f0be1f9bda7530b1fa654cac84d595ca9d53740","title":"Revisit Systematic Generalization via Meaningful Learning"},{"paperId":"0dc5dd7c64ee016bdc33a5f32dc25747be5ca702","title":"From SCAN to Real Data: Systematic Generalization via Meaningful Learning"}],"references":[{"paperId":"49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2","title":"Making Transformers Solve Compositional Tasks"},{"paperId":"86589b6286ef3c55b8b4fccfb41a3b30b7afdf61","title":"Going Beyond Linear Transformers with Recurrent Fast Weight Programmers"},{"paperId":"2040baf092ba73dfdffd97ae467e38ac0470520d","title":"Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations"},{"paperId":"1a703f08da01cf737cce3fb9064259b3f4b44e9c","title":"Linear Transformers Are Secretly Fast Weight Programmers"},{"paperId":"51f46cb42668cfe3745ecf029d032bf30253574f","title":"GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"649c758b0e59ddedaae37a3757e8eabdba664e5a","title":"Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"d9610589189e0821500516994dcee543a558b70c","title":"Learning advanced mathematical computations from examples"},{"paperId":null,"title":"Lexicon learning for few-shot neural sequence modeling"},{"paperId":"d642868ce4325ebf3026c0aa0c497a079f112a8d","title":"On the Binding Problem in Artificial Neural Networks"},{"paperId":"986cc3d0e3afb23f84564ea9588b7b8e9c3e1dd6","title":"Hierarchical Poset Decoding for Compositional Generalization in Language"},{"paperId":"227fe850a72fab24998c7e08d75db214715dc74e","title":"The EOS Decision and Length Extrapolation"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"b35b0a19425129432eefc21c3a9a1825f328c4b1","title":"Compositional Generalization via Neural-Symbolic Stack Machines"},{"paperId":"21f74e2617d8d8f5fc117ff2ad6e58a540541f6d","title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"},{"paperId":"e00484961fb2f30d2d48a5f9853fa3ebab140cac","title":"Improving Transformer Optimization Through Better Initialization"},{"paperId":"336ee50043b916c9e932338c02fd1abc87a6e849","title":"Compositional Generalization by Learning Analytical Expressions"},{"paperId":"3249dec80e963cbc86d941b819c549a325613f8c","title":"Permutation Equivariant Models for Compositional Generalization in Language"},{"paperId":"937fef6a786c4463a3bb19770c704945d1600b66","title":"Learning Compositional Rules via Neural Program Synthesis"},{"paperId":"5cdab78acc4f3aab429a0dd41c3ec7e605d42e7b","title":"Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"},{"paperId":"ea415809bf87ef4b99966c6c50de6cb996a02a97","title":"Deep double descent: where bigger models and more data hurt"},{"paperId":"815a3d56401483b635cfad9468852cddb46350ee","title":"Compositionality Decomposed: How do Neural Networks Generalise?"},{"paperId":"d4ae9dff186553d98eef4a275762b4cb15e1e41d","title":"Good-Enough Compositional Data Augmentation"},{"paperId":"80deaca65c2c155bd15718eeecff584841eb25b0","title":"CLOSURE: Assessing Systematic Generalization of CLEVR Models"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"2785e7e7f625630eeeedbc45124acf7931ba878d","title":"Compositional Generalization for Primitive Substitutions"},{"paperId":"d88f31a0091eee02c5a2aa2013914818cdef114e","title":"Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving"},{"paperId":"4cf963e5fd88825ac62ad6cce364447e5d2dfb2b","title":"Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention"},{"paperId":"07fa8c8a703abd7496f4781e9dee53d5de9c8717","title":"Neural Shuffle-Exchange Networks - Sequence Processing in O(n log n) Time"},{"paperId":"3eb44cc190093ba35e5cb6c54d107cd9220d58f5","title":"Compositional generalization through meta sequence-to-sequence learning"},{"paperId":"b39efed2e73357db4691f66935cf62e7b51f30e1","title":"Transcoding Compositionally: Using Attention to Find More Generalizable Solutions"},{"paperId":"d5535d4da15a7a8dfbeb34f61cddb4874bbc56e0","title":"Improving Differentiable Neural Computers Through Memory Masking, De-allocation, and Link Distribution Sharpness Control"},{"paperId":"f9318ec295ce285d613240e8e7df9bf0410d291a","title":"Compositional generalization in a deep seq2seq model by separating syntax and semantics"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"c4744a7c2bb298e4a52289a1e085c71cc3d37bc6","title":"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"bb6d3644fa5675351a4a05fe8b925416dc091c3c","title":"Measuring Generalization and Overfitting in Machine Learning"},{"paperId":null,"title":"Here, we use a decomposed attention matrix of the following form: A"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","title":"Self-Attention with Relative Position Representations"},{"paperId":"08fbb1b4cfdc83977d2c8f08bdfb663f13c0e60a","title":"Memorize or generalize? Searching for a compositional RNN in a haystack"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":null,"title":"We never combine absolute with relative positional embedding. In case of a relative positional variant of any Transformer model, we do not add absolute positional encoding to the word"},{"paperId":"784ee73d5363c711118f784428d1ab89f019daa5","title":"Hybrid computing using a neural network with dynamic external memory"},{"paperId":"04cca8e341a5da42b29b0bc831cb25a0f784fa01","title":"Adaptive Computation Time for Recurrent Neural Networks"},{"paperId":"5e4eb58d5b47ac1c73f4cf189497170e75ae6237","title":"Neural GPUs Learn Algorithms"},{"paperId":"d6f2f611da110b5b5061731be3fc4c7f45d8ee23","title":"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f","title":"Sequence Transduction with Recurrent Neural Networks"},{"paperId":"152d82025f02916019e4cfcc943dceecc159cda4","title":"Self-Delimiting Neural Networks"},{"paperId":"b71ac1e9fb49420d13e084ac67254a0bbd40f83f","title":"Understanding the difficulty of training deep feedforward neural networks"},{"paperId":null,"title":"Learning to control fastweight memories: An alternative to recurrent nets"},{"paperId":"2b83ce7567bad29afe4753eea7e72521dffdb075","title":"Connectionism and the problem of systematicity: Why Smolensky's solution doesn't work"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"}],"id":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","summary":"By revisiting model configurations as basic as scaling of embeddings, early stopping, relative positional embedding, and Universal Transformer variants, this work can drastically improve the performance of Transformers on systematic generalization."},{"url":"https://www.semanticscholar.org/paper/557ebd17b7c7ac4e09bd167d7b8909b8d74d1153","title":"Compositional Generalization Requires Compositional Parsers","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/24/2022","authors":"Pia Weissenhorn,Yuekun Yao,L. Donatelli,Alexander Koller","citations":[],"references":[{"paperId":"ab72bccf6f3981537389510ecc609109e79595c3","title":"Disentangled Sequence to Sequence Learning for Compositional Generalization"},{"paperId":"76c9558b3fa10baf0e094386a650015b29a8a4bc","title":"Compositional generalization in semantic parsing with pretrained transformers"},{"paperId":"ed535e93d5b5a8b689e861e9c6083a806d1535c2","title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"},{"paperId":"6d00b1024298e5b64ee873028385f7bb4396b05d","title":"Learning Algebraic Recombination for Compositional Generalization"},{"paperId":"523745e29f6cb1890f18352d449fd3597910c485","title":"Improving Compositional Generalization in Classification Tasks via Structure Annotations"},{"paperId":"70a136547d81290b9f4dbc1fac49d31bc010bd3c","title":"Meta-Learning to Compositionally Generalize"},{"paperId":"d34cac6a7101068f6ba6b9e08d169340b2589595","title":"Learning compositional structures for semantic graph parsing"},{"paperId":"83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34","title":"Lexicon Learning for Few Shot Sequence Modeling"},{"paperId":"25e7c9dcc294d77d184c4c1122c8304cdb58c69d","title":"One SPRING to Rule Them Both: Symmetric AMR Semantic Parsing and Generation without a Complex Pipeline"},{"paperId":"acf8a1040034820bf99379a3422815f4e0859ec9","title":"Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"},{"paperId":"307ec233777755b3d89b2096f4b54c83d9cd80ba","title":"Span-based Semantic Parsing for Compositional Generalization"},{"paperId":"642492003112a47b0bf86d60fac5507bc3b35a49","title":"Are Pretrained Convolutions Better than Pretrained Transformers?"},{"paperId":null,"title":"Making transformers solve"},{"paperId":null,"title":"Unlike them we didn’t use the fixed-tree decoder (described in Groschwitz et al. 2018), but opted for the projective A* decoder (Lindemann"},{"paperId":"36d1b02b4cd504ae8641556457a673ba1044b6a9","title":"Compositionality"},{"paperId":"b20ddcbd239f3fa9acc603736ac2e4416302d074","title":"COGS: A Compositional Generalization Challenge Based on Semantic Interpretation"},{"paperId":"60d99e00f7f96efea3099b9491a93bb8060ff502","title":"Fast Semantic Parsing with Well-typedness Guarantees"},{"paperId":"b0ea633e0c22fbd8cbc531c7326376725d16ce25","title":"Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks"},{"paperId":null,"title":"BART: Denoising sequence-to-sequence pretraining for natural language"},{"paperId":"671a05535da65f9fc22800b5aa94795fc670ac45","title":"Compositional Semantic Parsing across Graphbanks"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"0f4a3b7f835737c7e004073f4a873e356c0063b4","title":"Incorporating Source Syntax into Transformer-Based Neural Machine Translation"},{"paperId":"3abc5ffb1757ec3f35cb7b4100410570b0b51e09","title":"LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better"},{"paperId":"25109699b098c786832c906e4b36fa76fb2b66a0","title":"AMR dependency parsing with a typed semantic algebra"},{"paperId":"efef34c1caef102ad5cc052642d75beaaf5adcaf","title":"Deep RNNs Encode Soft Hierarchical Syntax"},{"paperId":"928f9dccb806a3278d20d82cc53781c5f44e2bb1","title":"Constituency Parsing with a Self-Attentive Encoder"},{"paperId":"856fe866bcce5e7a540655bea6ecc7406bdcfcba","title":"Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"ebb222fff7b71b82d1a5971e198982858abcd03d","title":"Modeling Source Syntax for Neural Machine Translation"},{"paperId":"3aa52436575cf6768a0a1a476601825f6a62e58f","title":"Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies"},{"paperId":"eec3a236ecd185712ce65fb336141f8656eea13d","title":"Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations"},{"paperId":"bf0f141bae83bd6d5ca0c37839d53f0d06059b34","title":"Controlling Politeness in Neural Machine Translation via Side Constraints"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"e72e5ee5de14fd463ab58ce830474157258e3578","title":"Abstract Meaning Representation for Sembanking"},{"paperId":"6843890926bf0e5c887ffc78dcb1203135981bf1","title":"The compositionality papers"},{"paperId":"56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7","title":"Connectionism and cognitive architecture: A critical analysis"},{"paperId":null,"title":"Syntactic Structures"},{"paperId":null,"title":"Agent NP to Unacc Subj 96"},{"paperId":null,"title":"Prim to Obj (proper noun) 10"}],"id":"557ebd17b7c7ac4e09bd167d7b8909b8d74d1153","summary":"The accuracy of different 005 parsers on the recent COGS corpus is analyzed and the role of syntactic generalization in compo- 016 sitional generalization is analyzed."}]}